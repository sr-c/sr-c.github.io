[{"categories":[],"contents":"在Android上使用双拼输入法，那么trime可能是你最佳的选择。\n近年来，许多大厂的输入法都存在这样或那样的隐私数据收集记录。6月11日，搜狗输入法、讯飞输入法、QQ输入法因未完全满足5月1日国家网信办关于个人信息收集违规问题通报的整改要求，被多家应用商店下架。因此，如果注重自生的隐私数据，输入法这部分，应当注重。\n安装与配置 直接在github上下载安装包，或者从F-Driod, Play Store安装。\n与Rime相同，Trime的配置也是通过配置文件完成。推荐在PC上编辑这些文件，之后覆盖至手机的配置目录(默认路径为/sdcard/rime)。\nRime的主要配置文件如下\n default.yaml —— 各方案共享的全局配置 default.custom.yaml —— （可选）对 default.yaml 的「补丁」 XXX.schema.yaml —— XXX 输入方案的配置 XXX.custom.yaml —— （可选）对 XXX.schema.yaml 的「补丁」 XXX.dict.yaml —— XXX 输入方案的词库（字典）  可直接移植使用自然码双拼方案。该方案能够使用辅码，以及拆字反查，并附带了许多词库。\n键盘布局 不同于PC端，手机端输入法的键盘布局尤为重要，直接影响输入体验。而Trime的键盘布局UI相关的配置文件为trime.yaml, 用户自定义的相应文件为trime.custom.yaml。Trime项目中有详尽的说明文档，默认的配置文件中的注释也很丰富。\n此处采用wzyboy自定义的类Google Pinyin界面，相应的配置文件在此。\n参考来源 https://wzyboy.im/post/1251.html\nhttps://github.com/osfans/trime\n","permalink":"https://sr-c.github.io/2021/07/01/trime-zrm-shuangpin/","tags":["rime","Android"],"title":"同文输入法(Trime)在Android上使用自然码双拼"},{"categories":[],"contents":"自定义Zotero的参考文献style\n常规的Zotero Style是.csl文件，使用xml语法。不同于Endnote, 这一格式开源，通用于Zotero, Mendeley等工具。Zotero提供自身的Style仓库，包括了国家标准GB/T 7714-2015 或GB/T 7714-2005 .\n中科院学位论文的参考文献要求同样衍生自国标GB/T 7714-2015 的作者-日期格式。实际上，在使用时还是有一些特殊的要求需要自行修改.csl文件。\n引文/citation 同时生成et al. 与等 在Style仓库中提供的国家标准GB/T 7714-2015对于既引用了中文文献，又有英文文献引用的情况，在文章存在多个作者需要省略时会统一使用“等”。这可能是由于笔者的系统语言设置为了中文。但是，这在引用英文文献时就不理想了。\nJurism提供了一个解决方案。它是一个在法律文献，多语言方面提供了增强的Zotero修改版。在其Style仓库中提供了一个JM Chinese Std GB/T 7714-2005 (numeric, Chinese)，可以根据文献的Language字段判断使用\u0026quot;et al.\u0026ldquo;还是“等”。默认的方式是英文文献需要在其Language/语言字段声明为en，否则默认视作中文文献使用“等”。\n参考文献/bibliography 参考文献中著录全部作者 找到定义参考文献的\u0026lt;bibliography\u0026gt;词段（通常位于最后的位置），将其中的et-al-min定义设置为0即可。\u0026lt;bibliography\u0026gt;词段的设置如下\n1  \u0026lt;bibliography et-al-min=\u0026#34;0\u0026#34; et-al-use-first=\u0026#34;3\u0026#34; hanging-indent=\u0026#34;true\u0026#34; line-spacing=\u0026#34;1.5\u0026#34; entry-spacing=\u0026#34;0\u0026#34;\u0026gt;   代表触发et-al的最小作者数是0（即不生效），使用前3个作者用于et-al之前，悬挂缩进（没找到设置缩进值的字段），1.5倍行距，entry-spacing是否指代段前段后间距不明。\n去掉引文最后的DOI编号 在\u0026lt;bibliography\u0026gt;词段内，删去如下使用URL和DOI的部分。\n1 2 3 4  \u0026lt;group delimiter=\u0026#34;. \u0026#34; prefix=\u0026#34;. \u0026#34;\u0026gt; \u0026lt;text variable=\u0026#34;URL\u0026#34;/\u0026gt; \u0026lt;text variable=\u0026#34;DOI\u0026#34; prefix=\u0026#34;DOI:\u0026#34;/\u0026gt; \u0026lt;/group\u0026gt;   按照作者姓名-出版年排序参考文献 在\u0026lt;bibliography\u0026gt;词段内，保证插入如下部分。\n1 2 3 4  \u0026lt;sort\u0026gt; \u0026lt;key macro=\u0026#34;author-intext\u0026#34;/\u0026gt; \u0026lt;key macro=\u0026#34;issue-date-year\u0026#34; sort=\u0026#34;ascending\u0026#34;/\u0026gt; \u0026lt;/sort\u0026gt;   后记  自行修改的style文件导入Zotero时可能会提示为不符合CLS 1.0.1 style的文件，询问是否继续，点击OK即可。\n 热心的johnmy提供了修改后的版本，详情可见其github.\n参考来源 https://zhuanlan.zhihu.com/p/320253145\nhttp://blog.sciencenet.cn/home.php?mod=space\u0026amp;uid=331295\u0026amp;do=blog\u0026amp;id=1265232\nhttps://github.com/redleafnew/Chinese-std-GB-T-7714-related-csl\n","permalink":"https://sr-c.github.io/2021/02/24/zotero-style-for-ucas-dissertation/","tags":["Zotero","style"],"title":"自定义Zotero Style"},{"categories":[],"contents":"在Manjaro桌面环境在安装reshape2或加载tidyverse之中。\n在安装reshape2时出现如下报错\n1 2 3  Error in dyn.load(file, DLLpath = DLLpath, ...) : unable to load shared object \u0026#39;/home/chance/R/x86_64-pc-linux-gnu-library/4.0/stringi/libs/stringi.so\u0026#39;: libicui18n.so.67: cannot open shared object file: No such file or directory   第一方案是使用locate查找libicui18n.so.67这个库。\n1 2 3 4 5  locate libicui18n.so.67 ## /usr/lib/libicui18n.so.67 ## /usr/lib/libicui18n.so.67.1 ## /usr/lib32/libicui18n.so.67 ## /usr/lib32/libicui18n.so.67.1   结果发现实际上libicui18n.so.67是在当前环境中是存在的，但stringi却索引不到。\n尝试自定义环境变量LD_LIBRARY_PATH在其中包括/usr/lib/:/usr/lib32/仍然不能解决问题。\n 实际上，/usr/lib/与/usr/lib32/均为系统默认的库索引路径。\n 解决方案 这是由于对于Manjaro这样的滚动发行版，stringi依赖的库是会持续更新变化的。因此，我们需要基于更新的系统库重新编译stringi。\n1  rm -rf /home/chance/R/x86_64-pc-linux-gnu-library/4.0/stringi/   在R中以普通用户重新安装stringi即可。\n1  install.packages(\u0026#34;stringi\u0026#34;)   参考来源 https://archived.forum.manjaro.org/t/manjaro-update-now-r-cant-load-libraries/115419/4\n","permalink":"https://sr-c.github.io/2021/01/20/unable-to-load-shared-object-stringi.so/","tags":["R","stringi","Manjaro","debug"],"title":"【debug】Unable to Load Shared Object Stringi"},{"categories":["Genomics","Visualization"],"contents":"Bacsnp是用来分析杆状病毒毒株之间遗传多样性（判断SNP的种群特异性）的下游工具。\n安装 新建R环境 环境需求R 3.6.3, 严格限制在此版本，其他版本均存在依赖限制的问题。\n因此，参照anaconda官方推荐，新建一个R 3.6.3的环境。\n 目前，官方conda源中的R似乎还没有更新到4.0.2, 最近的版本正是3.6.3. conda-forge或R的源中已更新至4.0.2\n 1 2 3 4 5  conda create -n R-3.6.3 r-essentials r-base conda activate R-3.6.3 ##如果你不小心将事实为R-3.6.3的环境命名成了R-4.0.2, 那么如下命令可以帮你将环境改回合适的名称 ##conda create --name R-3.6.3 --copy --clone R-4.0.2 ##conda remove --name R-4.0.2 --all # or its alias: `conda env remove --name R-4.0.2`   配置R软件源（可选）  目前官方源的范围速度还不错，因此也可直接使用官方源。\n 新建~/.Rprofile, 在其中添加如下语句。\n1 2 3 4  options(repos=structure(c(CRAN=“https://mirrors.tuna.tsinghua.edu.cn/CRAN/”))) source(“http://bioconductor.org/biocLite.R”) options(BioC_mirror=“http://mirrors.ustc.edu.cn/bioc/”)   安装依赖 1 2 3 4 5 6 7 8 9  install.packages(\u0026#34;devtools\u0026#34;) install.packages(\u0026#34;ggplot2\u0026#34;) if (!requireNamespace(\u0026#34;BiocManager\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;BiocManager\u0026#34;) BiocManager::install(\u0026#34;VariantAnnotation\u0026#34;) ## 确认这3个包成功安装后 library(devtools)   但在编译vignettes过程中报错：\n1 2 3 4 5 6  Error: Failed to install \u0026#39;unknown package\u0026#39; from GitHub: System command \u0026#39;Rcmd.exe\u0026#39; failed, exit status: 1, stdout + stderr (last 10 lines): E\u0026gt; Quitting from lines 107-108 (bacsnp.Rmd) E\u0026gt; Error: processing vignette \u0026#39;bacsnp.Rmd\u0026#39; failed with diagnostics: E\u0026gt; more cluster centers than distinct data points. E\u0026gt; --- failed re-building \u0026#39;bacsnp.Rmd\u0026#39;   需要修改源文件，在bacsnp.Rmd中的数据导入之前加入一行\n1  options(stringsAsFactors = FALSE)   防止读入数据转化为data.frame时将字符串自动转化为因子。\n然后，下载源码后手动本地安装\n1  install_local(\u0026#34;bacsnp-master.zip\u0026#34;, build_vignettes = TRUE)    其实，不编译vignettes也是解决方案，install_github(\u0026quot;wennj/bacsnp\u0026quot;, build_vignettes = FALSE)即可。\n 数据预处理 bacsnp推荐的数据预处理模式如下。\n根据其vignettes中的bacsnp.Rmd文件中的详细说明和提供的bac.vcf示例，其详细要求如下：\n 只接受Illunima数据。（长读长的测序数据通过合适的比对工具也能够用于后续分析 ） 测序数据应使用BWA比对至参考基因组，每个数据组要具有相应的标记。（经测试，bowtie2等其他工具也能够完成） 每个样品的数据都应当比对至同一个参考基因组序列 使用samtool/bcftool mpileup来分析BAM文件（不进行indel分析） bcftools只用来获取变异位点 bcftool的输出文件为非压缩的VCF格式  观察示例的bac.vcf文件，发现其Galaxy服务端的环境配置如下。\n1 2 3 4 5 6  ##samtoolsVersion=1.8+htslib-1.8 ##samtoolsCommand=samtools mpileup -f /CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540916.dat -C 0 -d 328 -q 0 -Q 13 --VCF --uncompressed --output-tags DP,DPR -I --output /CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540952.dat /CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540935.dat /CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540936.dat /CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540937.dat /CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540949.dat /CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540950.dat /CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540951.dat ##reference=file:///CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540916.dat ...... ##bcftools_callVersion=1.2+htslib-1.2.1 ##bcftools_callCommand=call -O v -A -v -f GQ -m -o /CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540976.dat /CLUSTER-FS/galaxy-dist/database/files/000/540/dataset_540952.dat   即使用samtool 1.8与bcftool 1.2的组合。作者强调samtool mpileup的输出文件需要有DP,DPR两个tag。但是，samtool在1.9之后的版本用AD替换了DPR，所以除非手动修改结果VCF文件，那么还是按照作者的环境配置来进行更省事。\nbacsnp分析 将bac.rmd的内容摘录如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  library(VariantAnnotation) library(IRanges) library(bacsnp) ## Loading VCF file data bacdata \u0026lt;- readVcf(\u0026#34;bac.vcf\u0026#34;) ## Transforming VCF file data bac.df \u0026lt;- bacsnp.transformation(bacdata) head(bac.df) ## Accessing basic SNP information unique(bac.df$ISO) length(unique(bac.df$POS)) unique(bac.df$POS) ## Filtering SNP data frame bac.f \u0026lt;- bacsnp.filter(bac.df, min.abs.cov = 100, min.abs.alt = 10, min.rel.alt = 0.05) ## Determination of SNP specificity bac.spec \u0026lt;- bacsnp.specificity(bac.f, c(\u0026#34;iIso1\u0026#34;, \u0026#34;iIso2\u0026#34;), which.rel = \u0026#34;REL.ALT1\u0026#34;) bac.spec$combinations head(bac.spec$data[, c(\u0026#34;POS\u0026#34;, \u0026#34;REF\u0026#34;, \u0026#34;ALT\u0026#34;,\u0026#34;REL.ALT1\u0026#34;, \u0026#34;SPEC\u0026#34;, \u0026#34;GROUP.ID\u0026#34;, \u0026#34;NT.SPEC\u0026#34;)]) ## Visualization of SNP frequencies with bacsnp.plot i1 \u0026lt;- bac.spec$data[bac.spec$data$ISO == \u0026#34;iIso1\u0026#34;,] i2 \u0026lt;- bac.spec$data[bac.spec$data$ISO == \u0026#34;iIso2\u0026#34;,] bacsnp.plot(i1, col = \u0026#34;SPEC\u0026#34;, genome.length = 123193, mark.repeats = FALSE)#, mark.lowGQ = 120, mark.lowQUAL = 400) bacsnp.plot(i2, col = \u0026#34;SPEC\u0026#34;, genome.length = 123193, mark.repeats = FALSE)#, mark.lowGQ = 120, mark.lowQUAL = 400)   结果 这个工具以多个毒株（种群）的二代重测序数据作为输入，能够分析得到种群特有的（或多个种群共有的）SNP位点，并进行可视化。\n可视化过程实际是从第一个SNP位点（基于基因组位置）开始。如果样本总体的SNP位点不多，在基因组起始5 kb的位置内又没有SNP位点，那么结果图中就不是从0 bp开始展示整个基因组的情况。作者在bacsnp.plot()中加入了genome.length参数输入基因组总长度，保证能够绘制到基因组的末尾，却忘了不一定会有开头。\n参考来源 Wennmann, Jörg T.; Fan, Jiangbin; Jehle, Johannes A. 2020. \u0026ldquo;Bacsnp: Using Single Nucleotide Polymorphism (SNP) Specificities and Frequencies to Identify Genotype Composition in Baculoviruses.\u0026quot; Viruses 12, no. 6: 625.\nhttp://samtools.github.io/bcftools/howtos/variant-calling.html\nhttps://github.com/wennj/bacsnp/blob/master/vignettes/bacsnp.Rmd\n","permalink":"https://sr-c.github.io/2020/09/11/bacsnp/","tags":["R","github"],"title":"使用Bacsnp可视化病毒基因组中的SNP位点"},{"categories":["Genomics","Assembly"],"contents":"GenomeScope是基因组评估的有效工具，但是之前只能够预测二倍体基因组，现在使用GenomeScope2则可以预测多倍体（至多六倍体）及其倍性。\n安装 1 2 3  git clone https://github.com/tbenavi1/genomescope2.0.git cd genomescope2.0/ Rscript install.R   注意，在安装前需要确认当前用户对目前环境中R的libraries安装路径存在写入权限。\n1 2 3 4 5  local_lib_path = \u0026#34;~/R_libs/\u0026#34; install.packages(\u0026#39;minpack.lm\u0026#39;, lib=local_lib_path) install.packages(\u0026#39;argparse\u0026#39;, lib=local_lib_path) install.packages(\u0026#39;.\u0026#39;, repos=NULL, type=\u0026#34;source\u0026#34;, lib=local_lib_path)   如此，则应该修改其中的local_lib_path为当前可用的R_LIBS路径。\n对于大多数用户，如果没有写入权限，那么可以直接新建目录~/R_libs，并在Renviron中声明\n1 2  mkdir ~/R_libs echo \u0026#34;R_LIBS=~/R_libs/\u0026#34; \u0026gt;\u0026gt; ~/.Renviron   使用 首先，使用KMC或jellyfish获取histogram_file\n获取k-mer的频率直方图 KMC 1 2 3 4 5 6  mkdir tmp ls *.fastq \u0026gt; FILES ##-k设定k-mer长度，-t设定使用线程数，-m设定使用内存大小，-ci设定使用k-mer count的下界，-cs设定使用k-mer count的上界。 kmc -k21 -t10 -m64 -ci1 -cs10000 @FILES reads tmp/ kmc_tools transform reads histogram reads.histo -cx10000   jellyfish 1 2  jellyfish count -C -m 21 -s 1000000000 -t 10 *.fastq -o reads.jf jellyfish histo -t 10 reads.jf \u0026gt; reads.histo   命令行运行GenomeScope 将genomescope2.0加入环境变量后，即可直接运行\n1  genomescope.R -i histogram_file -o output_dir -k k-mer_length   网页运行GenomeScope 将histogram_file上传至在线服务，即可。\n参考来源 https://bioinformaticsworkbook.org/dataAnalysis/GenomeAssembly/genomescope.html\n","permalink":"https://sr-c.github.io/2020/09/11/GenomeScope2/","tags":[],"title":"使用GenomeScope2评估基因组"},{"categories":[],"contents":"批量搜索并杀死目标进程\n1 2  ## 搜索当前进程 | 关键词过滤 | 获取pid | 批量运行kill -9 ps -ef | grep \u0026#34;key words\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | xargs kill -9   批量杀死同一个会话下的进程 1  pkill -s \u0026lt;SID\u0026gt;   杀死父进程与子进程 以SIGHUP信号杀死父进程后，子进程也会被杀死。\n参考来源 https://leokongwq.github.io/2017/08/22/linux-kill-parent-and-child-process.html\nhttps://blog.csdn.net/aaashen/article/details/50667533\nhttps://juejin.im/post/6844903927989665806\nhttp://morningcoffee.io/killing-a-process-and-all-of-its-descendants.html\n","permalink":"https://sr-c.github.io/2020/09/02/kill-task-in-batch/","tags":["linux"],"title":"批量中止进程"},{"categories":["Genomics"],"contents":"AGAT是Another Gff Analysis Toolkit的缩写, 是一个用于操作GTF/GFF文件的工具。它集合了一系列perl脚本，并且可以通过conda安装。\n环境配置 为了最小化对conda的base环境的影响，推荐新建一个agatEnv环境用于AGAT的安装。\n1 2 3  conda create -n agatEnv conda activate agatEnv conda install -c bioconda agat   说明事项 其中包含的部分脚本如下\n   task tool     check, fix, pad missing information into sorted and standardised gff3 agat_convert_sp_gxf2gxf.pl   make feature statistics agat_sp_statistics.pl   make function statistics agat_sp_functional_statistics.pl   extract any type of sequence agat_sp_extract_sequences.pl   extract attributes agat_sp_extract_attributes.pl   complement annotations (non-overlapping loci) agat_sp_complement_annotations.pl   merge annotations agat_sp_merge_annotations.pl   filter gene models by ORF size agat_sp_filter_by_ORF_size.pl   filter to keep only longest isoforms agat_sp_keep_longest_isoform.pl   create introns features agat_sp_add_introns.pl   fix cds phases agat_sp_fix_cds_phases.pl   manage IDs agat_sp_manage_IDs.pl   manage UTRs agat_sp_manage_UTRs.pl   manage introns agat_sp_manage_introns.pl   manage functional annotation agat_sp_manage_functional_annotation.pl   specificity sensitivity agat_sp_sensitivity_specificity.pl   fusion / split analysis between two annotations agat_sp_compare_two_annotations.pl   analyze differences between BUSCO results agat_sp_compare_two_BUSCOs.pl   convert any GTF/GFF into tabulated format agat_sp_to_tabulated.pl   convert any GTF/GFF into BED format agat_convert_sp_gff2bed.pl   convert any GTF/GFF into GTF format agat_convert_sp_gff2gtf.pl   convert any GTF/GFF into any GTF/GFF (bioperl) format agat_convert_sp_gxf2gxf.pl   convert BED format into GFF3 format agat_convert_bed2gff.pl   convert EMBL format into GFF3 format agat_convert_embl2gff.pl   convert genscan format into GFF3 format agat_convert_genscan2gff.pl   convert mfannot format into GFF3 format agat_convert_mfannot2gff.pl   \u0026hellip; and much more \u0026hellip; \u0026hellip; see here \u0026hellip;    其中脚本名称中的_sp_与_sq_具有特殊含义。\n_sp_代表SLURP, 即脚本会一次性全部读入gff文件至内存。这样虽然消耗的内存更多，但处理效率更高，也能够修复整个文本中的一些错误。\n_sq_代表SEQUENTIAL, 即脚本会从头至尾逐行读入gff文件，而不经过完整性检验。这样的脚本具有相对更高的内存效率。\n使用 以agat_convert_sp_gxf2gxf.pl为例，它能够检测并修复gtf/gff文件中的错误与缺失信息，得到排序后符合标准的gff3文件。\nexample 8 - 只含有CDS信息 1 2 3 4 5 6 7  ##gff-version 3 Tob1_contig1\tProdigal:2.60\tCDS\t476\t670\t.\t-\t0\tID=Tob1_00001;locus_tag=Tob1_00001;product=hypothetical protein Tob1_contig1\tProdigal:2.60\tCDS\t34266\t35222\t.\t+\t0\tID=Tob1_00024;locus_tag=Tob1_00024;product=hypothetical protein Tob1_contig1\tSignalP:4.1\tsig_peptide\t34266\t34298\t.\t+\t0\tinference=ab initio prediction:SignalP:4.1;note=predicted cleavage at residue 33;product=putative signal peptide Tob1_contig1\tProdigal:2.60\tCDS\t35267\t37444\t.\t-\t0\tID=Tob1_00025;locus_tag=Tob1_00025; Tob1_contig1\tSignalP:4.1\tsig_peptide\t37420\t37444\t.\t-\t0\tinference=ab initio prediction:SignalP:4.1;note=predicted cleavage at residue 25;product=putative signal peptide Tob1_contig1\tProdigal:2.60\tCDS\t38304\t39338\t.\t-\t0\tID=Tob1_00026;locus_tag=Tob1_00026;   补足第一级(gene)和第二级(mRNA)信息。agat_convert_sp_gxf2gxf.pl --gff 8_test.gff:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  ##gff-version 3 Tob1_contig1\tProdigal:2.60\tgene\t476\t670\t.\t-\t0\tID=nbis_NEW-gene-1;locus_tag=Tob1_00001;product=hypothetical protein Tob1_contig1\tProdigal:2.60\tmRNA\t476\t670\t.\t-\t0\tID=nbis_nol2id-cds-1;Parent=nbis_NEW-gene-1;locus_tag=Tob1_00001;product=hypothetical protein Tob1_contig1\tProdigal:2.60\texon\t476\t670\t.\t-\t.\tID=nbis_NEW-exon-1;Parent=nbis_nol2id-cds-1;locus_tag=Tob1_00001;product=hypothetical protein Tob1_contig1\tProdigal:2.60\tCDS\t476\t670\t.\t-\t0\tID=Tob1_00001;Parent=nbis_nol2id-cds-1;locus_tag=Tob1_00001;product=hypothetical protein Tob1_contig1\tProdigal:2.60\tgene\t34266\t35222\t.\t+\t0\tID=nbis_NEW-gene-2;locus_tag=Tob1_00024;product=hypothetical protein Tob1_contig1\tProdigal:2.60\tmRNA\t34266\t35222\t.\t+\t0\tID=nbis_nol2id-cds-2;Parent=nbis_NEW-gene-2;locus_tag=Tob1_00024;product=hypothetical protein Tob1_contig1\tProdigal:2.60\texon\t34266\t35222\t.\t+\t.\tID=nbis_NEW-exon-2;Parent=nbis_nol2id-cds-2;locus_tag=Tob1_00024;product=hypothetical protein Tob1_contig1\tProdigal:2.60\tCDS\t34266\t35222\t.\t+\t0\tID=Tob1_00024;Parent=nbis_nol2id-cds-2;locus_tag=Tob1_00024;product=hypothetical protein Tob1_contig1\tSignalP:4.1\tsig_peptide\t34266\t34298\t.\t+\t0\tID=sig_peptide-1;Parent=nbis_nol2id-cds-2;inference=ab initio prediction:SignalP:4.1;note=predicted cleavage at residue 33;product=putative signal peptide Tob1_contig1\tProdigal:2.60\tgene\t35267\t37444\t.\t-\t0\tID=nbis_NEW-gene-3;locus_tag=Tob1_00025 Tob1_contig1\tProdigal:2.60\tmRNA\t35267\t37444\t.\t-\t0\tID=nbis_nol2id-cds-3;Parent=nbis_NEW-gene-3;locus_tag=Tob1_00025 Tob1_contig1\tProdigal:2.60\texon\t35267\t37444\t.\t-\t.\tID=nbis_NEW-exon-3;Parent=nbis_nol2id-cds-3;locus_tag=Tob1_00025 Tob1_contig1\tProdigal:2.60\tCDS\t35267\t37444\t.\t-\t0\tID=Tob1_00025;Parent=nbis_nol2id-cds-3;locus_tag=Tob1_00025 Tob1_contig1\tSignalP:4.1\tsig_peptide\t37420\t37444\t.\t-\t0\tID=sig_peptide-2;Parent=nbis_nol2id-cds-3;inference=ab initio prediction:SignalP:4.1;note=predicted cleavage at residue 25;product=putative signal peptide Tob1_contig1\tProdigal:2.60\tgene\t38304\t39338\t.\t-\t0\tID=nbis_NEW-gene-4;locus_tag=Tob1_00026 Tob1_contig1\tProdigal:2.60\tmRNA\t38304\t39338\t.\t-\t0\tID=nbis_nol2id-cds-4;Parent=nbis_NEW-gene-4;locus_tag=Tob1_00026 Tob1_contig1\tProdigal:2.60\texon\t38304\t39338\t.\t-\t.\tID=nbis_NEW-exon-4;Parent=nbis_nol2id-cds-4;locus_tag=Tob1_00026 Tob1_contig1\tProdigal:2.60\tCDS\t38304\t39338\t.\t-\t0\tID=Tob1_00026;Parent=nbis_nol2id-cds-4;locus_tag=Tob1_00026   example 9 - 第二级 (mRNA) 与第三级(UTRs)特征信息缺失: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  ##gff-version 3 #!gff-spec-version 1.14 #!source-version NCBI C++ formatter 0.2 ##Type DNA NC_003070.9 NC_003070.9\tRefSeq\tsource\t1\t30427671\t.\t+\t.\torganism=Arabidopsis thaliana;mol_type=genomic DNA;db_xref=taxon:3702;chromosome=1;ecotype=Columbia NC_003070.9\tRefSeq\tgene\t3631\t5899\t.\t+\t.\tID=NC_003070.9:NAC001;locus_tag=AT1G01010; NC_003070.9\tRefSeq\texon\t3631\t3913\t.\t+\t.\tID=NM_099983.2;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010; NC_003070.9\tRefSeq\texon\t3996\t4276\t.\t+\t.\tID=NM_099983.2;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010; NC_003070.9\tRefSeq\texon\t4486\t4605\t.\t+\t.\tID=NM_099983.2;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010; NC_003070.9\tRefSeq\texon\t4706\t5095\t.\t+\t.\tID=NM_099983.2;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010; NC_003070.9\tRefSeq\texon\t5174\t5326\t.\t+\t.\tID=NM_099983.2;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010; NC_003070.9\tRefSeq\texon\t5439\t5899\t.\t+\t.\tID=NM_099983.2;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010; NC_003070.9\tRefSeq\tCDS\t3760\t3913\t.\t+\t0\tID=NM_099983.2;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010; NC_003070.9\tRefSeq\tCDS\t3996\t4276\t.\t+\t2\tID=NM_099983.2;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010; NC_003070.9\tRefSeq\tCDS\t4486\t4605\t.\t+\t0\tID=NM_099983.2;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010; NC_003070.9\tRefSeq\tCDS\t4706\t5095\t.\t+\t0\tID=NM_099983.2;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010; NC_003070.9\tRefSeq\tCDS\t5174\t5326\t.\t+\t0\tID=NM_099983.2;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010; NC_003070.9\tRefSeq\tCDS\t5439\t5627\t.\t+\t0\tID=NM_099983.2;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010; NC_003070.9\tRefSeq\tstart_codon\t3760\t3762\t.\t+\t0\tID=NM_099983.2;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010; NC_003070.9\tRefSeq\tstop_codon\t5628\t5630\t.\t+\t0\tID=NM_099983.2;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010;   agat_convert_sp_gxf2gxf.pl --gff 8_test.gff:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  ##gff-version 3 #!gff-spec-version 1.14 #!source-version NCBI C++ formatter 0.2 ##Type DNA NC_003070.9 NC_003070.9\tRefSeq\tsource\t1\t30427671\t.\t+\t.\tID=source-1;chromosome=1;db_xref=taxon:3702;ecotype=Columbia;mol_type=genomic DNA;organism=Arabidopsis thaliana NC_003070.9\tRefSeq\tgene\t3631\t5899\t.\t+\t.\tID=nbis_NEW-gene-1;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tmRNA\t3631\t5899\t.\t+\t.\tID=NC_003070.9:NAC001;Parent=nbis_NEW-gene-1;locus_tag=AT1G01010 NC_003070.9\tRefSeq\texon\t3631\t3913\t.\t+\t.\tID=NM_099983.2;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010 NC_003070.9\tRefSeq\texon\t3996\t4276\t.\t+\t.\tID=nbis_NEW-exon-1;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010 NC_003070.9\tRefSeq\texon\t4486\t4605\t.\t+\t.\tID=nbis_NEW-exon-2;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010 NC_003070.9\tRefSeq\texon\t4706\t5095\t.\t+\t.\tID=nbis_NEW-exon-3;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010 NC_003070.9\tRefSeq\texon\t5174\t5326\t.\t+\t.\tID=nbis_NEW-exon-4;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010 NC_003070.9\tRefSeq\texon\t5439\t5899\t.\t+\t.\tID=nbis_NEW-exon-5;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tCDS\t3760\t3913\t.\t+\t0\tID=nbis_NEW-cds-1;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tCDS\t3996\t4276\t.\t+\t2\tID=nbis_NEW-cds-1;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tCDS\t4486\t4605\t.\t+\t0\tID=nbis_NEW-cds-1;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tCDS\t4706\t5095\t.\t+\t0\tID=nbis_NEW-cds-1;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tCDS\t5174\t5326\t.\t+\t0\tID=nbis_NEW-cds-1;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tCDS\t5439\t5627\t.\t+\t0\tID=nbis_NEW-cds-1;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tfive_prime_UTR\t3631\t3759\t.\t+\t.\tID=nbis_NEW-five_prime_utr-1;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tstart_codon\t3760\t3762\t.\t+\t0\tID=nbis_NEW-start_codon-1;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tstop_codon\t5628\t5630\t.\t+\t0\tID=nbis_NEW-stop_codon-1;Parent=NC_003070.9:NAC001;locus_tag=AT1G01010 NC_003070.9\tRefSeq\tthree_prime_UTR\t5628\t5899\t.\t+\t.\tID=nbis_NEW-three_prime_utr-1;Parent=NC_003070.9:NAC001;gbkey=mRNA;locus_tag=AT1G01010   example 18 - 特定基因的相关特征信息未经排序，零散地分布在文件中 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  ##gff-version 3 scaffold625\tmaker\tgene\t337818\t343277\t.\t+\t.\tID=CLUHARG00000005458;Name=TUBB3_2 scaffold625\tmaker\tmRNA\t337818\t343277\t.\t+\t.\tID=CLUHART00000008717;Parent=CLUHARG00000005458 scaffold625\tmaker\texon\t337818\t337971\t.\t+\t.\tID=CLUHART00000008717:exon:1404;Parent=CLUHART00000008717 scaffold625\tmaker\texon\t340733\t340841\t.\t+\t.\tID=CLUHART00000008717:exon:1405;Parent=CLUHART00000008717 scaffold789\tmaker\tthree_prime_UTR\t564589\t564780\t.\t+\t.\tID=CLUHART00000006146:three_prime_utr;Parent=CLUHART00000006146 scaffold789\tmaker\tmRNA\t558184\t564780\t.\t+\t.\tID=CLUHART00000006147;Parent=CLUHARG00000003852 scaffold625\tmaker\tCDS\t337915\t337971\t.\t+\t0\tID=CLUHART00000008717:cds;Parent=CLUHART00000008717 scaffold625\tmaker\tCDS\t340733\t340841\t.\t+\t0\tID=CLUHART00000008717:cds;Parent=CLUHART00000008717 scaffold625\tmaker\tCDS\t341518\t341628\t.\t+\t2\tID=CLUHART00000008717:cds;Parent=CLUHART00000008717 scaffold625\tmaker\tCDS\t341964\t343033\t.\t+\t2\tID=CLUHART00000008717:cds;Parent=CLUHART00000008717 scaffold625\tmaker\tfive_prime_UTR\t337818\t337914\t.\t+\t.\tID=CLUHART00000008717:five_prime_utr;Parent=CLUHART00000008717 scaffold625\tmaker\tthree_prime_UTR\t343034\t343277\t.\t+\t.\tID=CLUHART00000008717:three_prime_utr;Parent=CLUHART00000008717 scaffold789\tmaker\tgene\t558184\t564780\t.\t+\t.\tID=CLUHARG00000003852;Name=PF11_0240 scaffold789\tmaker\tmRNA\t558184\t564780\t.\t+\t.\tID=CLUHART00000006146;Parent=CLUHARG00000003852 scaffold789\tmaker\texon\t558184\t560123\t.\t+\t.\tID=CLUHART00000006146:exon:995;Parent=CLUHART00000006146 scaffold789\tmaker\texon\t561401\t561519\t.\t+\t.\tID=CLUHART00000006146:exon:996;Parent=CLUHART00000006146 scaffold789\tmaker\texon\t564171\t564235\t.\t+\t.\tID=CLUHART00000006146:exon:997;Parent=CLUHART00000006146 scaffold789\tmaker\texon\t564372\t564780\t.\t+\t.\tID=CLUHART00000006146:exon:998;Parent=CLUHART00000006146 scaffold789\tmaker\tCDS\t558191\t560123\t.\t+\t0\tID=CLUHART00000006146:cds;Parent=CLUHART00000006146 scaffold789\tmaker\tCDS\t561401\t561519\t.\t+\t2\tID=CLUHART00000006146:cds;Parent=CLUHART00000006146 scaffold625\tmaker\texon\t341518\t341628\t.\t+\t.\tID=CLUHART00000008717:exon:1406;Parent=CLUHART00000008717 scaffold625\tmaker\texon\t341964\t343277\t.\t+\t.\tID=CLUHART00000008717:exon:1407;Parent=CLUHART00000008717 scaffold789\tmaker\tCDS\t564171\t564235\t.\t+\t0\tID=CLUHART00000006146:cds;Parent=CLUHART00000006146 scaffold789\tmaker\tCDS\t564372\t564588\t.\t+\t1\tID=CLUHART00000006146:cds;Parent=CLUHART00000006146 scaffold789\tmaker\tfive_prime_UTR\t558184\t558190\t.\t+\t.\tID=CLUHART00000006146:five_prime_utr;Parent=CLUHART00000006146 scaffold789\tmaker\texon\t558184\t560123\t.\t+\t.\tID=CLUHART00000006147:exon:997;Parent=CLUHART00000006147 scaffold789\tmaker\texon\t561401\t561519\t.\t+\t.\tID=CLUHART00000006147:exon:998;Parent=CLUHART00000006147 scaffold789\tmaker\texon\t562057\t562121\t.\t+\t.\tID=CLUHART00000006147:exon:999;Parent=CLUHART00000006147 scaffold789\tmaker\texon\t564372\t564780\t.\t+\t.\tID=CLUHART00000006147:exon:1000;Parent=CLUHART00000006147 scaffold789\tmaker\tCDS\t558191\t560123\t.\t+\t0\tID=CLUHART00000006147:cds;Parent=CLUHART00000006147 scaffold789\tmaker\tCDS\t561401\t561519\t.\t+\t2\tID=CLUHART00000006147:cds;Parent=CLUHART00000006147 scaffold789\tmaker\tCDS\t562057\t562121\t.\t+\t0\tID=CLUHART00000006147:cds;Parent=CLUHART00000006147 scaffold789\tmaker\tCDS\t564372\t564588\t.\t+\t1\tID=CLUHART00000006147:cds;Parent=CLUHART00000006147 scaffold789\tmaker\tfive_prime_UTR\t558184\t558190\t.\t+\t.\tID=CLUHART00000006147:five_prime_utr;Parent=CLUHART00000006147 scaffold789\tmaker\tthree_prime_UTR\t564589\t564780\t.\t+\t.\tID=CLUHART00000006147:three_prime_utr;Parent=CLUHART00000006147   agat_convert_sp_gxf2gxf.pl --gff 18_test.gff:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  ##gff-version 3 scaffold625\tmaker\tgene\t337818\t343277\t.\t+\t.\tID=CLUHARG00000005458;Name=TUBB3_2 scaffold625\tmaker\tmRNA\t337818\t343277\t.\t+\t.\tID=CLUHART00000008717;Parent=CLUHARG00000005458 scaffold625\tmaker\texon\t337818\t337971\t.\t+\t.\tID=CLUHART00000008717:exon:1404;Parent=CLUHART00000008717 scaffold625\tmaker\texon\t340733\t340841\t.\t+\t.\tID=CLUHART00000008717:exon:1405;Parent=CLUHART00000008717 scaffold625\tmaker\texon\t341518\t341628\t.\t+\t.\tID=CLUHART00000008717:exon:1406;Parent=CLUHART00000008717 scaffold625\tmaker\texon\t341964\t343277\t.\t+\t.\tID=CLUHART00000008717:exon:1407;Parent=CLUHART00000008717 scaffold625\tmaker\tCDS\t337915\t337971\t.\t+\t0\tID=CLUHART00000008717:cds;Parent=CLUHART00000008717 scaffold625\tmaker\tCDS\t340733\t340841\t.\t+\t0\tID=CLUHART00000008717:cds;Parent=CLUHART00000008717 scaffold625\tmaker\tCDS\t341518\t341628\t.\t+\t2\tID=CLUHART00000008717:cds;Parent=CLUHART00000008717 scaffold625\tmaker\tCDS\t341964\t343033\t.\t+\t2\tID=CLUHART00000008717:cds;Parent=CLUHART00000008717 scaffold625\tmaker\tfive_prime_UTR\t337818\t337914\t.\t+\t.\tID=CLUHART00000008717:five_prime_utr;Parent=CLUHART00000008717 scaffold625\tmaker\tthree_prime_UTR\t343034\t343277\t.\t+\t.\tID=CLUHART00000008717:three_prime_utr;Parent=CLUHART00000008717 scaffold789\tmaker\tgene\t558184\t564780\t.\t+\t.\tID=CLUHARG00000003852;Name=PF11_0240 scaffold789\tmaker\tmRNA\t558184\t564780\t.\t+\t.\tID=CLUHART00000006146;Parent=CLUHARG00000003852 scaffold789\tmaker\texon\t558184\t560123\t.\t+\t.\tID=CLUHART00000006146:exon:995;Parent=CLUHART00000006146 scaffold789\tmaker\texon\t561401\t561519\t.\t+\t.\tID=CLUHART00000006146:exon:996;Parent=CLUHART00000006146 scaffold789\tmaker\texon\t564171\t564235\t.\t+\t.\tID=CLUHART00000006146:exon:997;Parent=CLUHART00000006146 scaffold789\tmaker\texon\t564372\t564780\t.\t+\t.\tID=CLUHART00000006146:exon:998;Parent=CLUHART00000006146 scaffold789\tmaker\tCDS\t558191\t560123\t.\t+\t0\tID=CLUHART00000006146:cds;Parent=CLUHART00000006146 scaffold789\tmaker\tCDS\t561401\t561519\t.\t+\t2\tID=CLUHART00000006146:cds;Parent=CLUHART00000006146 scaffold789\tmaker\tCDS\t564171\t564235\t.\t+\t0\tID=CLUHART00000006146:cds;Parent=CLUHART00000006146 scaffold789\tmaker\tCDS\t564372\t564588\t.\t+\t1\tID=CLUHART00000006146:cds;Parent=CLUHART00000006146 scaffold789\tmaker\tfive_prime_UTR\t558184\t558190\t.\t+\t.\tID=CLUHART00000006146:five_prime_utr;Parent=CLUHART00000006146 scaffold789\tmaker\tthree_prime_UTR\t564589\t564780\t.\t+\t.\tID=CLUHART00000006146:three_prime_utr;Parent=CLUHART00000006146 scaffold789\tmaker\tmRNA\t558184\t564780\t.\t+\t.\tID=CLUHART00000006147;Parent=CLUHARG00000003852 scaffold789\tmaker\texon\t558184\t560123\t.\t+\t.\tID=CLUHART00000006147:exon:997;Parent=CLUHART00000006147 scaffold789\tmaker\texon\t561401\t561519\t.\t+\t.\tID=CLUHART00000006147:exon:998;Parent=CLUHART00000006147 scaffold789\tmaker\texon\t562057\t562121\t.\t+\t.\tID=CLUHART00000006147:exon:999;Parent=CLUHART00000006147 scaffold789\tmaker\texon\t564372\t564780\t.\t+\t.\tID=CLUHART00000006147:exon:1000;Parent=CLUHART00000006147 scaffold789\tmaker\tCDS\t558191\t560123\t.\t+\t0\tID=CLUHART00000006147:cds;Parent=CLUHART00000006147 scaffold789\tmaker\tCDS\t561401\t561519\t.\t+\t2\tID=CLUHART00000006147:cds;Parent=CLUHART00000006147 scaffold789\tmaker\tCDS\t562057\t562121\t.\t+\t0\tID=CLUHART00000006147:cds;Parent=CLUHART00000006147 scaffold789\tmaker\tCDS\t564372\t564588\t.\t+\t1\tID=CLUHART00000006147:cds;Parent=CLUHART00000006147 scaffold789\tmaker\tfive_prime_UTR\t558184\t558190\t.\t+\t.\tID=CLUHART00000006147:five_prime_utr;Parent=CLUHART00000006147 scaffold789\tmaker\tthree_prime_UTR\t564589\t564780\t.\t+\t.\tID=CLUHART00000006147:three_prime_utr;Parent=CLUHART00000006147   参考来源 https://github.com/NBISweden/AGAT\n","permalink":"https://sr-c.github.io/2020/08/28/AGAT-usage/","tags":["gff","gtf","perl"],"title":"AGAT使用"},{"categories":null,"contents":"错误发现率(False Discovery Rates, FDR)在高通量测序的RNA-seq分析中非常常见，甚至还使用过，但是对其中原理还需要更多探究。\n主旨  False Discovery Rates are a tool to weed out bad date that looks good.\n 背景 在转录组分析中，我们希望比较不同样品中某个基因的表达量是否存在差异。\n假设我们的多次抽样均来自同一个分布。那么各个样本中该基因对应的read counts应当符合正态分布。\n两次抽样间比较的p值应该在大部分时候(95%)都大于我们的常见的阈值(0.05)。\n小概率情况下，我们取样的结果之间存在较大差异，统计分析结果会认为两次抽样分布来自两个不同的分布，即两组样本之间存在显著差异。此时的结果我们称之为”假阳性“(false-positive)。\n理论假设 抽样来自同一个分布 我们假设两次抽样都来自同一个分布，检验两次抽样的结果是否存在显著差异。 多次重复这一试验过程，然后统计差异检验结果p值的分布。可以发现p值均匀分布在(0, 1)的区间内。那么那些p值落在(0, 0.05)区间的试验结果就是”假阳性“的结果，使得本没有差异的抽样被认为存在差异。 抽样来自不同的分布 如果两次抽样确实来自不同的分布，那么大部分假设检验的结果p值都会小于0.05,\n统计10000次检验结果p值的分布，可以看到严重偏向于0. 大部分的p值落在(0, 0.05)区间内，而小部分(5%)的p值大于0.05，即”假阴性“，使得本来存在差异的取样被认为来自于同一个分布。\n实际情况 在转录组分析中，对于大部分真核生物，其基因组中的蛋白编码基因的数量在$$10^4$$水平。\n我们假设目标物种存在11000个蛋白编码基因，实验处理（如药物摄入）影响了其中1000个基因的表达量，对另外10000个基因的表达没有影响。\n 对于这10000个未受实验处理影响的蛋白编码基因，其表达量应当来自于同一个分布，但是由于”假阳性“结果的存在，就会有约500个基因被错误地认为表达量存在差异，被归类于差异表达基因； 对于那1000个实际收到影响的基因，其中会有约50个基因由于”假阴性“结果的存在，被错误地归类为非差异表达基因。  因此，由于真核生物基因组中蛋白编码基因的基数，导致直接统计检验获得的错误结果较多（550个）。解决方案就是Benjamini-Hochberg method, 其中应用到了FDR的概念。严格来说，FDR本身并不是一个统计方法，\nBenjamini-Hochberg method 将两种基因差异检验结果的p值分布综合起来，就可以得到总体上实际的分布情况。我们可以通过肉眼获得来自于表达量未受影响的基因的p值分布（红色虚线），在(0, 0.05)区间内，位于红色虚线上方的蓝色部分那些真阳性的试验结果（差异表达基因）。\n ”假阴性“的差异基因数据较少，在此可忽略。\n 事实上，对于(0, 0.05)区间内的p值，其分布也是极度偏向0的。因此，我们选择最小的那些p-value作为真阳性的结果。\nBH校正方法正是对上述方案的算法实现。校正后获得的adjusted p value (padjust)会扩大原来的p值，使得假阳性的p值(\u0026lt;0.05)变得不显著(\u0026gt;0.05)。\n首先，将每个样本对应的p值从小到大排列。\n然后每个样本的padjust取以下两个值中的较小值。\n a: 前一个样本的padjust\nb: 该基因的p值乘以样本总数与该样本p值排序的商\n 结果使得所有样本的p值都扩大了，原先假阳性样本的p值经过校正后就不再显著了。\n参考来源 https://statquest.org/?s=FDR\nhttps://www.youtube.com/watch?v=K8LQSvtjcEo\n","permalink":"https://sr-c.github.io/2020/08/09/False-Discovery-Rates-StatQuest/","tags":["StatQuest","FDR","RNA-seq"],"title":"【StatQuest】错误发现率(FDR)"},{"categories":["Genomics"],"contents":"分析原核/病毒基因组中的重复序列\n寻找基因间区 首先，计算基因组中的基因编码区。然后，通过interval取反得到基因间区。若只为得到基因间区，重叠的CDS区域不用合并，并不影响与基因组全长取差。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  from interval import Interval, IntervalSet f = open(\u0026#34;XGV_0.7.2.cds.intervals\u0026#34;, \u0026#34;r\u0026#34;) cds = f.readlines() f.close cdIn = [] for i in cds: [s, e] = i.strip().split(\u0026#34;\\t\u0026#34;) cdIn.append(Interval(int(s), int(e))) ## Get intervals for all CDS  allcds = IntervalSet(cdIn) ## Define IntervalSet for the genome. full_len = IntervalSet([Interval(1, 116875)]) ## ^ get the symmetric difference outCDS = full_len ^ allcds print(outCDS)   检测简单重复序列 TRF, REPuter是常用的检测简单重复序列的工具，可在全基因组范围内寻找，也可针对特定基因间区进行搜索。\nTRF一般用于寻找串联重复序列，可使用在线服务。\nREPuter不仅可寻找串联重复，对于4种类型的重复序列（如下）均可检索。使用比勒费尔德大学生信中心提供的在线服务，结果可通过网页可视化。\n forward(direct) match\n reverse match\n complement match\n palindromic match\n  ​\nblast2seq 是NCBI较为古老的BLAST工具中的一员。可直接对两段序列进行比较，不需要make database. 对同一段序列进行自比较，并过滤去同位置完全匹配的结果，即可用于发现其中存在的重复序列。\n检测回文序列 EMBOSS palindrome可检测序列内部的回文序列。通过参数可设定回文序列的长度范围，错配的碱基数。\n参考来源 https://pythonhosted.org/intervalset/\n","permalink":"https://sr-c.github.io/2020/07/22/repeat-region-search/","tags":["python"],"title":"搜索简单基因组中的重复序列"},{"categories":["Blog"],"contents":"参照其他的优秀方案对even主题进行部分自定义修改。\n修改主题配色 even主题的样式文件位于${Hugo-Site}/themes/even/assets/sass/_variables.scss\n修改其中的white与global-font-color结果如下\n1 2 3 4 5 6 7 8 9 10 11 12 13  ... // ========== Color ========== // $black: #002a35 !default; $white: #fdf6e3 !default; $light-gray: #e6e6e6 !default; $gray: #cacaca !default; $dark-gray: #8a8a8a !default; // ========== Global ========== // // Text color of the body. $global-font-color: #657b83 !default; ...   添加搜索菜单 lryong在issues#289中提到添加搜索功能的方案hugo-search-fuse-js.\n 下载hugo-search-fuse-js至theme目录 添加\u0026quot;hugo-search-fuse-js\u0026quot;至config.toml 新建一个content/search.md文件，用于配置search页面  上述步骤参照官方的说明文档进行即可。\n由于站点的layouts目录优先级高于主题的layouts目录。因此，我们可以将自定义的内容放置于站点的layouts目录，而不用修改主题的内容，避免主题升级时出现问题。\n将${Hugo-sites}\\themes\\even\\layouts\\_default\\baseof.html拷至${Hugo-sites}\\layouts\\_default目录中。然后在合适的位置插入main与footerblock. 修改后的baseof.html如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ... \u0026lt;main id=\u0026#34;main\u0026#34; class=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;content-wrapper\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;content\u0026#34; class=\u0026#34;content\u0026#34;\u0026gt; {{ block \u0026#34;main\u0026#34; . }}{{ end }} {{ block \u0026#34;content\u0026#34; . }}{{ end }} \u0026lt;/div\u0026gt; {{ partial \u0026#34;comments.html\u0026#34; . }} \u0026lt;/div\u0026gt; \u0026lt;/main\u0026gt; {{ if not .Params.hideHeaderAndFooter -}} \u0026lt;footer id=\u0026#34;footer\u0026#34; class=\u0026#34;footer\u0026#34;\u0026gt; {{ partial \u0026#34;footer.html\u0026#34; . }} {{ block \u0026#34;footer\u0026#34; . }}{{ end }} \u0026lt;/footer\u0026gt; {{- end }} ...   在站点的配置文件config.toml中添加Search菜单\n1 2 3 4 5  [[menu.main]] name = \u0026#34;搜索\u0026#34; weight = 50 identifier = \u0026#34;search\u0026#34; url = \u0026#34;/search/\u0026#34;   添加评论区 even主题现已加入utterances支持，参照文档，其已经设定为GitHub App安装非常方便，步骤如下：\n 在GitHub中新建一个repo (sr-c/blog-comment)用于存放评论 在GitHub中一键安装utterances app 在设置页面选择 repo: sr-c/blog-comment 在站点config.toml中的[params.utterances]字段中填入owner = \u0026quot;sr-c\u0026quot;与repo = \u0026quot;blog-comment\u0026quot;  查看themes/even/layouts/paritals/comments.html, 可以看到其中已经集成了utterances的script字段，无需再做修改。\n1 2 3 4 5 6 7 8 9 10 11  \u0026lt;!-- utterances --\u0026gt; {{- if .Site.Params.utterances.owner}} \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;{{ .Site.Params.utterances.owner }}/{{ .Site.Params.utterances.repo }}\u0026#34; issue-term=\u0026#34;pathname\u0026#34; theme=\u0026#34;github-light\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; \u0026lt;noscript\u0026gt;Please enable JavaScript to view the \u0026lt;a href=\u0026#34;https://github.com/utterance\u0026#34;\u0026gt;comments powered by utterances.\u0026lt;/a\u0026gt;\u0026lt;/noscript\u0026gt; {{- end }}   添加RSS订阅 参考官方文档，添加/layouts/rss.xml文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \u0026lt;rss version=\u0026#34;2.0\u0026#34; xmlns:atom=\u0026#34;http://www.w3.org/2005/Atom\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;{{ with .Title }}{{.}} on {{ end }}{{ .Site.Title }}\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;{{ .Permalink }}\u0026lt;/link\u0026gt; \u0026lt;description\u0026gt;Recent content {{ with .Title }}in {{.}} {{ end }}on {{ .Site.Title }}\u0026lt;/description\u0026gt; \u0026lt;generator\u0026gt;Hugo -- gohugo.io\u0026lt;/generator\u0026gt;{{ with .Site.LanguageCode }} \u0026lt;language\u0026gt;{{.}}\u0026lt;/language\u0026gt;{{end}}{{ with .Site.Author.email }} \u0026lt;managingEditor\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/managingEditor\u0026gt;{{end}}{{ with .Site.Author.email }} \u0026lt;webMaster\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/webMaster\u0026gt;{{end}}{{ with .Site.Copyright }} \u0026lt;copyright\u0026gt;{{.}}\u0026lt;/copyright\u0026gt;{{end}}{{ if not .Date.IsZero }} \u0026lt;lastBuildDate\u0026gt;{{ .Date.Format \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; | safeHTML }}\u0026lt;/lastBuildDate\u0026gt;{{ end }} \u0026lt;atom:link href=\u0026#34;{{.URL}}\u0026#34; rel=\u0026#34;self\u0026#34; type=\u0026#34;application/rss+xml\u0026#34; /\u0026gt; {{ range first 15 .Data.Pages }} \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;{{ .Permalink }}\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;{{ .Date.Format \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; | safeHTML }}\u0026lt;/pubDate\u0026gt; {{ with .Site.Author.email }}\u0026lt;author\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/author\u0026gt;{{end}} \u0026lt;guid\u0026gt;{{ .Permalink }}\u0026lt;/guid\u0026gt; \u0026lt;description\u0026gt;{{ .Content | html }}\u0026lt;/description\u0026gt; \u0026lt;/item\u0026gt; {{ end }} \u0026lt;/channel\u0026gt; \u0026lt;/rss\u0026gt;   参考来源 http://www.herbert.top:18080/2020/07/09/how_change_hugo_even_font/\nhttps://github.com/olOwOlo/hugo-theme-even/issues/289#issuecomment-657229431\nhttps://www.dazhuanlan.com/2019/12/05/5de8934e6f081/\n","permalink":"https://sr-c.github.io/2020/07/21/Hugo-custom/","tags":["Hugo"],"title":"自定义Hugo主题样式"},{"categories":["Visualization"],"contents":"最近发表在Bioinformatics的DNA Features Viewer是一个可视化DNA序列特征的Python工具。此为尝鲜。\n安装 官方文档目前只提供pip安装方式。issue中有反映要求conda安装方式，评论中有个人提供的linux conda版本，但我没有安装成功。\n1 2  conda create -n biopython biopython -y ## create a biopython env pip install dna_features_viewer ## install via pip   使用 自定义序列特征绘图 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  \u0026#34;\u0026#34;\u0026#34;Simple example where a few features are defined \u0026#34;by hand\u0026#34; and are displayed and exported as PNG, first with a linear view, then with a circular view. \u0026#34;\u0026#34;\u0026#34; from dna_features_viewer import ( GraphicFeature, GraphicRecord, CircularGraphicRecord, ) features = [ GraphicFeature( start=5, end=20, strand=+1, color=\u0026#34;#ffd700\u0026#34;, label=\u0026#34;Small feature\u0026#34; ), GraphicFeature( start=20, end=500, strand=+1, color=\u0026#34;#ffcccc\u0026#34;, label=\u0026#34;Gene 1 with a very long name\u0026#34;, ), GraphicFeature( start=400, end=700, strand=-1, color=\u0026#34;#cffccc\u0026#34;, label=\u0026#34;Gene 2\u0026#34; ), GraphicFeature( start=600, end=900, strand=+1, color=\u0026#34;#ccccff\u0026#34;, label=\u0026#34;Gene 3\u0026#34; ), ] # PLOT AND EXPORT A LINEAR VIEW OF THE CONSTRUCT record = GraphicRecord(sequence_length=1000, features=features) ax, _ = record.plot(figure_width=5) ax.figure.savefig(\u0026#34;graphic_record_defined_by_hand.png\u0026#34;) # PLOT AND EXPORT A CIRCULAR VIEW OF THE CONSTRUCT circular_rec = CircularGraphicRecord(sequence_length=1000, features=features) ax2, _ = circular_rec.plot(figure_width=4) ax2.figure.tight_layout() ax2.figure.savefig( \u0026#34;graphic_record_defined_by_hand_circular.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34; )   读取genbank数据绘图 dna_features_viewer中含有translate_record函数可读取GenBank格式的数据。但是，将读取到的这些特征解释为绘图内容就可以使用其提供的BiopythonTranslator或自定义MyCustomTranslator.\n自定义特征解释器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  from dna_features_viewer import BiopythonTranslator class MyCustomTranslator(BiopythonTranslator): \u0026#34;\u0026#34;\u0026#34;Custom translator implementing the following theme: - Color terminators in green, CDS in blue, all other features in gold. - Do not display features that are restriction sites unless they are BamHI - Do not display labels for restriction sites. - For CDS labels just write \u0026#34;CDS here\u0026#34; instead of the name of the gene. \u0026#34;\u0026#34;\u0026#34; def compute_feature_color(self, feature): if feature.type == \u0026#34;CDS\u0026#34;: return \u0026#34;blue\u0026#34; elif feature.type == \u0026#34;terminator\u0026#34;: return \u0026#34;green\u0026#34; else: return \u0026#34;gold\u0026#34; def compute_feature_label(self, feature): if feature.type == \u0026#39;restriction_site\u0026#39;: return None elif feature.type == \u0026#34;CDS\u0026#34;: return \u0026#34;CDS here\u0026#34; else: return BiopythonTranslator.compute_feature_label(self, feature) def compute_filtered_features(self, features): \u0026#34;\u0026#34;\u0026#34;Do not display promoters. Just because.\u0026#34;\u0026#34;\u0026#34; return [ feature for feature in features if (feature.type != \u0026#34;restriction_site\u0026#34;) or (\u0026#34;BamHI\u0026#34; in str(feature.qualifiers.get(\u0026#34;label\u0026#34;, \u0026#39;\u0026#39;))) ] graphic_record = MyCustomTranslator().translate_record(\u0026#34;example_sequence.gb\u0026#34;) ax, _ = graphic_record.plot(figure_width=10) ax.figure.tight_layout() ax.figure.savefig(\u0026#34;custom_biopython_translator.png\u0026#34;)   Biopython默认特征解释器 1 2 3 4 5 6  from dna_features_viewer import BiopythonTranslator graphic_record = BiopythonTranslator().translate_record(\u0026#34;example_sequence.gb\u0026#34;) ax, _ = graphic_record.plot(figure_width=10, strand_in_label_threshold=7) ax.figure.tight_layout() ax.figure.savefig(\u0026#34;from_genbank.png\u0026#34;)   可以绘制圈图，但是所有的注释信息都只能标注在图像上方，而不是像一般的Snapgene之类的工具紧密地随特征标注。\n显示GC比例的线性图 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  \u0026#34;\u0026#34;\u0026#34;In this example we plot a record\u0026#39;s annotations on top of the curve of the local GC content in the record\u0026#39;s sequence. \u0026#34;\u0026#34;\u0026#34; import matplotlib.pyplot as plt from dna_features_viewer import BiopythonTranslator from Bio import SeqIO import numpy as np def plot_local_gc_content(record, window_size, ax): \u0026#34;\u0026#34;\u0026#34;Plot windowed GC content on a designated Matplotlib ax.\u0026#34;\u0026#34;\u0026#34; def gc_content(s): return 100.0 * len([c for c in s if c in \u0026#34;GC\u0026#34;]) / len(s) yy = [ gc_content(record.seq[i : i + window_size]) for i in range(len(record.seq) - window_size) ] xx = np.arange(len(record.seq) - window_size) + 25 ax.fill_between(xx, yy, alpha=0.3) ax.set_ylim(bottom=0) ax.set_ylabel(\u0026#34;GC(%)\u0026#34;) record = SeqIO.read(\u0026#34;example_sequence.gb\u0026#34;, \u0026#34;genbank\u0026#34;) translator = BiopythonTranslator() graphic_record = translator.translate_record(record) fig, (ax1, ax2) = plt.subplots( 2, 1, figsize=(10, 3), sharex=True, gridspec_kw={\u0026#34;height_ratios\u0026#34;: [4, 1]} ) graphic_record.plot(ax=ax1, with_ruler=False, strand_in_label_threshold=4) plot_local_gc_content(record, window_size=50, ax=ax2) fig.tight_layout() # Resize the figure to the right height fig.savefig(\u0026#34;with_gc_plot.png\u0026#34;)   氨基酸编码序列 1 2 3 4 5 6 7 8 9 10 11 12  from dna_features_viewer import GraphicFeature, GraphicRecord record = GraphicRecord(sequence=\u0026#34;ATGCATGCATGCATGCATGCATGCATGC\u0026#34;, features=[ GraphicFeature(start=5, end=10, strand=+1, color=\u0026#39;#ffcccc\u0026#39;), GraphicFeature(start=8, end=15, strand=+1, color=\u0026#39;#ccccff\u0026#39;) ]) ax, _ = record.plot(figure_width=6) record.plot_sequence(ax, guides_intensity=0.2) fontdict = {\u0026#39;weight\u0026#39;: \u0026#39;bold\u0026#39;} record.plot_translation(ax, (8, 23), fontdict=fontdict, guides_intensity=0.8) ax.figure.savefig(\u0026#39;sequence_and_translation.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;)   GIF 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  \u0026#34;\u0026#34;\u0026#34;An example with GIF generation at the end. How cool is that! This example requires the Moviepy library installed (pip install moviepy). \u0026#34;\u0026#34;\u0026#34; from Bio import Entrez, SeqIO import moviepy.editor as mpe from moviepy.video.io.bindings import mplfig_to_npimage import matplotlib.pyplot as plt from dna_features_viewer import BiopythonTranslator, CircularGraphicRecord # DOWNLOAD THE PLASMID\u0026#39;s RECORD FROM NCBI handle = Entrez.efetch( db=\u0026#34;nucleotide\u0026#34;, id=1473096477, rettype=\u0026#34;gb\u0026#34;, retmode=\u0026#34;text\u0026#34; ) record = SeqIO.read(handle, \u0026#34;genbank\u0026#34;) # CREATE THE GRAPHIC RECORD WITH DNA_FEATURES_VIEWER color_map = { \u0026#34;rep_origin\u0026#34;: \u0026#34;yellow\u0026#34;, \u0026#34;CDS\u0026#34;: \u0026#34;orange\u0026#34;, \u0026#34;regulatory\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;misc_recomb\u0026#34;: \u0026#34;darkblue\u0026#34;, \u0026#34;misc_feature\u0026#34;: \u0026#34;lightblue\u0026#34;, } translator = BiopythonTranslator( features_filters=(lambda f: f.type not in [\u0026#34;gene\u0026#34;, \u0026#34;source\u0026#34;],), features_properties=lambda f: {\u0026#34;color\u0026#34;: color_map.get(f.type, \u0026#34;white\u0026#34;)}, ) translator.max_line_length = 15 graphic_record = translator.translate_record( record, record_class=CircularGraphicRecord ) graphic_record.labels_spacing = 15 # ANIMATE INTO A GIF WITH MOVIEPY duration = 5 def make_frame(t): top_nucleotide_index = t * graphic_record.sequence_length / duration graphic_record.top_position = top_nucleotide_index ax, _ = graphic_record.plot(figure_width=8, figure_height=11) ax.set_ylim(top=2) np_image = mplfig_to_npimage(ax.figure) plt.close(ax.figure) return np_image clip = mpe.VideoClip(make_frame, duration=duration) small_clip = clip.crop(x1=60, x2=-60, y1=100, y2=-100).resize(0.5) small_clip.write_gif(\u0026#34;example_with_gif.gif\u0026#34;, fps=15)   其他 issues #28中表示希望圈图的注释信息只能位于上方，不能够自定义位置。作者表示可以通过调整CircularGraphicRecord.py实现在上下两个区域标注信息，完整地自定义注释位置目前实现起来还比较困难。\n参考来源 https://edinburgh-genome-foundry.github.io/DnaFeaturesViewer/examples.html\nhttps://edinburgh-genome-foundry.github.io/DnaFeaturesViewer/\n","permalink":"https://sr-c.github.io/2020/07/21/DNA-Features-Viewer-quickstart/","tags":["python","conda","biopython"],"title":"DNA Features Viewer使用"},{"categories":null,"contents":"使用Anaconda配置python环境，在VScode中使用\n安装 anaconda和VScode的安装都很方便，默认安装即可。Windows中安装在C://Program Files目录下可能会存在权限问题，不推荐。\n配置 .condarc 使用conda config命令新建.condarc文件。一般的路径如下：\n  macOS: /Users/Username Linux: ~/.condarc Windows: C:\\Users\\Username   1 2 3 4 5  ssl_verify:truechannels:- conda-forge- bioconda- defaults  文件规范了各个conda源的优先级。还可以设定镜像软件源的地址，但现在官方源的速度也不错，可以不做设置。\n新建conda环境 conda虽然方便，但为了降低BUG几率，最好保障base环境干净。因此，一般都在新建的环境中进行配置。\n1  conda create -n \u0026#34;scipy\u0026#34; pandas biopython   新建一个scipy环境，安装了pandas,biopython方便日常调用。\n在VScode中调用conda环境 参考官方文档，首先在VScode扩展市场中安装Microsoft官方的python扩展。\n选择环境 使用Ctrl+Shift+P打开Phthon: Select Interpreter选择python解释器。\n在其中可直接选择conda环境中的python，然后在VScode界面左下角可以看到当前应用的解释器。\n手动设置解释器 在settings.json中可使用python.pythonPath字段设置Python的路径。使用Ctrl + ,打开settings.json\n1  \u0026#34;python.pythonPath\u0026#34;: \u0026#34;\u0026lt;path-to-your-interpreter\u0026gt;\\\\python.exe\u0026#34;   终端自动激活目标环境 在settings.json中配置终端自动激活conda环境，添加如下字段\n1  \u0026#34;terminal.integrated.shellArgs.windows\u0026#34;: [\u0026#34;/K\u0026#34;, \u0026#34;C:\\\\\u0026lt;path-to-conda-installation\u0026gt;\\\\Scripts\\\\activate.bat C:\\\\\u0026lt;path-to-conda-installation\u0026gt; \u0026amp; conda activate \u0026lt;your-env-name\u0026gt;\u0026#34;]   推荐在Windows中，VScode默认调用的终端改为cmd.exe, 在settings.json中添加\n1  \u0026#34;terminal.integrated.shell.windows\u0026#34;:\u0026#34;C:\\\\Windows\\\\System32\\\\cmd.exe\u0026#34;   参考来源 https://code.visualstudio.com/docs/python/environments\nhttps://medium.com/@udiyosovzon/how-to-activate-conda-environment-in-vs-code-ce599497f20d\nhttps://stackoverflow.com/a/50993392\n","permalink":"https://sr-c.github.io/2020/07/21/python-environment-in-vscode/","tags":["python","VScode"],"title":"在Vscode中配置Python环境"},{"categories":["Blog"],"contents":"Github最近总是邮件提醒我有些插件过时了，存在安全问题。随着文章数量增多，Hexo的速度越来越慢。因此决定切换到Hugo\n安装 安装 Git 和 Go Hugo是用Golang编写的，使用Hugo前需要安装Git 和 Go 语言开发环境。\n安装Hugo 参考官方文档有很多安装方式。对于Windows用户，可直接下载二进制文件，将放置路径添加至环境变量PATH即可。\n生成博客 1 2 3 4  hugo version # 查看版本 hugo new site myblog # 使用Hugo新建myblog项目，名称自拟 cd myblog # 进入myblog目录 hugo server # 启动本地调试服务   安装主题 Hugo没有自带的主题，但其官网的主题库也非常丰富，应该能满足大部分人的需要。安装方式也与Hexo类似，在themes目录中git clone需要的主题即可。目前使用的是even, 推荐的还有LeaveIt,maupassant\n配置文件 默认的配置文件为站点目录中的config.toml，该文件可直接从主题的exampleSite/config.toml直接替换，再进一步自定义。\n文章迁移 之前Hexo默认的URL为如下形式\n1 2 3  /2017/01/02/xxxxx /2017/03/04/yyyyy /2017/05/06/zzzzz   在Hugo的配置文件中，需要做如下设置才能达到相同结构的URL\n1 2  [permalinks] posts = \u0026#34;/:year/:month/:day/:filename\u0026#34;   Front-matter Markdown文档开头起说明作用的Front-matter字段应书写规范，Hugo的要求似乎比Hexo更严格，需要检查一下哪些文章没有写好。\n 在tags, categories等字段设置了多个值，则应该以tags: [mongodb, test]的方式记录。 Front-matter字段首尾的---分割符不能缺少。  其他项目 1 2 3 4  ## 保持分类的原始名字（false会做转小写处理） preserveTaxonomyNames = true ## 是否禁止URL Path转小写 disablePathToLower = true   调试 1 2  hugo server hugo server -D ##强制渲染非草稿的文章   Github项目迁移 之前，为了Hexo项目的多地部署，我在sr-c.github.io新开了一个hexo分支用于同步Hexo站点的配置目录。每次更新文章，先将站点目录git push到hexo分支，再使用hexo g -d调用git将public目录的内容提交至master分支。\n顺应这个思路，我又新建了一个hugo分支用于同步站点目录，再将每次更新生成的public目录推送至master分支。\n向GitHub添加SSH key 参考之前的设置方式，对于已有的SSH key，可直接添加至GitHub.\n1 2 3 4 5  # Add SSH key to clipboard clip \u0026lt; ~/.ssh/id_rsa.pub # Paste the key to GitHub Accout Settings page. # Test the Key ssh -T git@github.com   提交hugo分支 在新建立的Hugo站点目录中\n1 2 3 4 5 6 7  cd myblog # 位于新建立的站点目录中 git init # git初始化 git remote add origin https://\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git # 添加仓库地址 git checkout -b hugo # 新建分支hexo并切换到新建的分支hexo git add . # 添加所有本地文件到git git commit -m \u0026#34;version_log\u0026#34; # git提交 git push origin hugo # 文件推送到hugo分支   提交master分支 Hugo官方文档推荐使用子项目的方式管理并提交public目录至master分支。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  rm -rf public #git submodule add -b master https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public ## deploy.sh #!/bin/sh # If a command fails then the deploy stops set -e printf \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\u0026#34; # Build the project. hugo # if using a theme, replace with `hugo -t \u0026lt;YOURTHEME\u0026gt;` # Go To Public folder cd public # Ensure submit to object branch git clone git@github.com:sr-c/sr-c.github.io.git master # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site $(date)\u0026#34; if [ -n \u0026#34;$*\u0026#34; ]; then msg=\u0026#34;$*\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master   注意，此处要求先删去已有的public目录，但是public已被注册称为子项目，最好在git中删除该submodule，否则如果下一步提交不成功，那么可能会存在问题。删除submodule的方式记录如下：\n1 2 3 4 5 6 7 8  # Remove the submodule entry from .git/config git submodule deinit -f path/to/submodule # Remove the submodule directory from the superproject\u0026#39;s .git/modules directory rm -rf .git/modules/path/to/submodule # Remove the entry in .gitmodules and remove the submodule directory located at path/to/submodule git rm -f path/to/submodule   托管至master分支中的docs目录 https://sspai.com/post/59904中推荐将Github Page使用docs目录进行部署，但是实测目前该方法已经失效。\nGithub对于个人用户的\u0026lt;username\u0026gt;.github.io或\u0026lt;orgname\u0026gt;.github.io的Page页面现在只能通过master分支进行部署，不支持自定义，也不支持使用docs目录了。\n参考来源 https://scarletsky.github.io/2019/05/02/migrate-hexo-to-hugo/\nhttps://www.flysnow.org/2018/07/29/from-hexo-to-hugo.html\nhttps://io-oi.me/tech/hugo-vs-hexo/\nhttps://ouuan.github.io/post/from-hexo-to-hugo/\nhttps://blog.eric7.site/2020/01/05/%E8%BF%81%E7%A7%BBhexo%E5%8D%9A%E5%AE%A2%E5%88%B0hugo/\n","permalink":"https://sr-c.github.io/2020/07/18/Hexo-to-Hugo/","tags":["Hugo","Hexo","blog"],"title":"将博客从Hexo迁移至Hugo"},{"categories":null,"contents":"环境配置 interval只有1.0.0一个版本，并不兼容于python3. 无法使用conda按照，因此直接使用pip安装。\n1  pip install interval   测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  from interval import Interval, IntervalSet # coding r1 = IntervalSet([Interval(1, 1000), Interval(1100, 1200)]) r2 = IntervalSet([Interval(30, 50), Interval(60, 200), Interval(1150, 1300)]) r3 = IntervalSet([Interval(1000, 3000)]) r4 = IntervalSet([Interval(1000, 3000)]) r5 = IntervalSet([Interval(30000, 12000)]) print (r3 - r4), (r4 - r3), r3 \u0026amp; r4 print len(IntervalSet.empty()) if r3 \u0026amp; r4 == r4: print \u0026#39;yes\u0026#39; print(r3 \u0026amp; r4) if (r3 - r4).empty(): print \u0026#34;true\u0026#34; print (r3 - r4).empty() # output #\u0026lt;Empty\u0026gt; \u0026lt;Empty\u0026gt; [1000..3000] #0 #yes #[1000..3000] #\u0026lt;Empty\u0026gt; data = [(2,4), (9,13), (6,12)] Intervals = IntervalSet([Interval.between(min, max) for min, max in data]) print [(_.lower_bound, _.upper_bound) for _ in Intervals] # output #[(2, 4), (6, 13)]   参考来源 https://www.bbsmax.com/A/rV57nE0RJP/\nhttps://www.jianshu.com/p/635069917a83\nhttps://segmentfault.com/q/1010000014045492\n","permalink":"https://sr-c.github.io/2020/07/17/interval-caculating/","tags":["python","interval"],"title":"interval进行区间计算"},{"categories":null,"contents":"环境配置 安装TexLive与vscode, 具体流程参考其他说明。\n文献管理格式BibTeX 使用BibTex管理参考文献。\n规范格式 1 2 3 4 5 6  @article{mrx05, auTHor = \u0026#34;Mr. X\u0026#34;, Title = {Something Great}, publisher = \u0026#34;nob\u0026#34; # \u0026#34;ody\u0026#34;, YEAR = 2005, }   CNKI导出BibTeX CNKI官方系统并不支持直接导出BibTeX，如果不能使用Google schloar，那就只能使用曲线方案。\n 使用Vopaaz开发的转化方案，从CNKI导出为NoteExpress格式，再转化为BibTeX。 将文献导入Zotero/Endnote, 再导出为BibTeX  注意事项 德语中的一些曲音字母（如 ä,ö,ü ）在直接编译时存在问题。这时我们可写成{\\\u0026quot;a}, {\\\u0026quot;o}, {\\\u0026quot;u}来解决问题。\n参考来源 https://blog.csdn.net/ddydavie/article/details/83020615\nhttps://stackoverflow.com/a/58667452\n","permalink":"https://sr-c.github.io/2020/06/19/LaTeX-beginer/","tags":["LaTex"],"title":"LaTeX-beginer"},{"categories":null,"contents":"已知基因组的注释信息，可以得到任一CDS的位置区间，求所有CDS在基因组中的覆盖度。\n解决方案 根据feature table或gff/gtf注释信息可提取到CDS的位置区间。\n1 2  grep \u0026#34;CDS\u0026#34; feature_table.ft | \\  awk \u0026#39;{if ($2\u0026gt;$1) printf \u0026#34;%d\\t%d\\n\u0026#34;,$1,$2; else printf \u0026#34;%d\\t%d\\n\u0026#34;,$2,$1}\u0026#39; \u0026gt; cds.interval   问题的关键在于多个区间的合并。这其实是Leetcode56 的题目，因此我们参考使用其答案中的解决方案。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  # Definition for an interval. class Interval: def __init__(self, s=0, e=0): self.start = s self.end = e # Solution class Solution: def merge(self, intervals): \u0026#34;\u0026#34;\u0026#34; :type intervals: List[Interval] :rtype: List[Interval] \u0026#34;\u0026#34;\u0026#34; if len(intervals) \u0026lt;= 1: return intervals res = [] intervals = sorted(intervals,key = lambda start: start.start) l = intervals[0].start h = intervals[0].end for i in range(1,len(intervals)): if intervals[i].start \u0026lt;= h: h = max(h,intervals[i].end) else: res.append([l,h]) l = intervals[i].start h = intervals[i].end res.append([l,h]) return res if __name__ == \u0026#39;__main__\u0026#39;: # Test intervals s= Solution() i1 = Interval(1,3) i2 = Interval(8,10) i3 = Interval(2,6) i4 = Interval(15,18) intervals = [i1,i2,i3,i4] res = s.merge(intervals) # Read input CDS intervals. f = open(\u0026#34;cds.intervals\u0026#34;) cds = f.readlines() f.close # Merge the intervals. cdIn = [] for i in cds: [s, e] = i.strip().split(\u0026#34;\\t\u0026#34;, maxsplit=2) cdIn.append(Interval(int(s), int(e))) allcds = Solution().merge(cdIn) # Count the length of all intervals. cdsLength = 0 for [s, e] in allcds: cdslen = e - s + 1 cdsLength += cdslen print(cdsLength)   参考来源 https://blog.csdn.net/qq_34364995/article/details/80788049\n","permalink":"https://sr-c.github.io/2020/06/07/cds-coverage/","tags":["cds","python"],"title":"计算基因组中CDS的覆盖度"},{"categories":["Genomics"],"contents":"数据准备 获取区域关联关系 Mummer匹配 获取已硬屏蔽了重复序列的基因组序列，使用nucmer(Mummer)对其进行匹配\n1 2 3 4 5 6 7 8 9 10  #比对 nucmer --prefix=ref_qry ref.fasta qry.fasta #过滤，除去不太适合的部分，但结果不适合读 delta-filter -q ref_qry.delta \u0026gt; ref_qry.filter #将结果转换为以人类可读的格式显示匹配的坐标 show-coords -rcl ref_qry.delta \u0026gt; ref_qry.coords ##Genome Linkage info ##以一定标准（如匹配区域长度\u0026gt;2 kb, 相似度\u0026gt;80%）过滤匹配的区域 awk \u0026#39;$5\u0026gt;50000, OFS=\u0026#34;\\t\u0026#34;{print $12,$1,$2,$13,$3,$4}\u0026#39; ../../Mummer/out.1coords \u0026gt; req_qry.50k.LinkedRegion.info   基因组核型（染色体长度） 1 2 3 4 5 6 7  seqkit fx2tab ~/reference/genome/ref.fasta -l -n \u0026gt; ref.SeqLen seqkit fx2tab ~/reference/genome/qry.fasta -l -n \u0026gt; qry.SeqLen cat ref.SeqLen qry.SeqLen \u0026gt; all.SeqLen awk \u0026#39;{print \u0026#34;chr\\t-\\t\u0026#34;$1\u0026#34;\\t\u0026#34;$1\u0026#34;\\t\u0026#34;0\u0026#34;\\t\u0026#34;$2\u0026#34;\\tblack\u0026#34;}\u0026#39; ref.SeqLen \u0026gt; karyotype.ref.txt awk \u0026#39;{print \u0026#34;chr\\t-\\t\u0026#34;$1\u0026#34;\\t\u0026#34;$1\u0026#34;\\t\u0026#34;0\u0026#34;\\t\u0026#34;$2\u0026#34;\\tblack\u0026#34;}\u0026#39; qry.SeqLen \u0026gt; karyotype.qry.txt cat karyotype.ref.txt karyotype.qry.txt \u0026gt; karyotype.both.txt   基因/重复序列密度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ## Gene position awk \u0026#39;$3==\u0026#34;gene\u0026#34;, OFS=\u0026#34;\\t\u0026#34;{print $1,$4,$5,1,$7}\u0026#39; ~/reference/annotation/ref.gff3 \u0026gt; ref.genePos awk \u0026#39;$3==\u0026#34;gene\u0026#34;, OFS=\u0026#34;\\t\u0026#34;{print $1,$4,$5,1,$7}\u0026#39; ~/reference/annotation/qry.gff3 \u0026gt; qry.genePos cat ref.genePos qry.genePos \u0026gt; All.GenePos ## Repeat Region position awk \u0026#39;OFS=\u0026#34;\\t\u0026#34;{print $1,$4,$5,1,$7}\u0026#39; RepeatMasker/ref.fasta.out.gff \u0026gt; ref.RepPos awk \u0026#39;OFS=\u0026#34;\\t\u0026#34;{print $1,$4,$5,1,$7}\u0026#39; RepeatMasker/qry.fasta.out.gff \u0026gt; qry.RepPos cat ref.RepPos qry.RepPos | grep -v \u0026#34;#\u0026#34; \u0026gt; All.RepPos ## 设定滑动窗口 bedtools makewindows -g Os.AllContigLen -w 100000 \u0026gt; both.windows ## 计算基因/重复序列单元的覆盖度 bedtools coverage -a All.GenePos -b both.windows | cut -f 1-4 \u0026gt; genes_num.txt bedtools coverage -a All.RepPos -b both.windows| cut -f 1-4 \u0026gt; reps_num.txt   GC含量 1 2 3 4 5 6 7 8  ## 临时合并ref与qry的基因组序列，方便同时计算 cat ~/reference/genome/ref.fasta ~/reference/genome/qry.fasta \u0026gt; tmp.fasta ## 设定滑动窗口 bedtools makewindows -g All.SeqLen -w 100000 \u0026gt; both.windows ## 使用bedtools nuc计算GC含量，默认输出的第5列为GC含量 bedtools nuc -fi tmp.fasta -bed both.windows | cut -f 1-3,5 | sed \u0026#39;1d\u0026#39; \u0026gt; both.GCrate.txt ## 删除临时文件 rm tmp.fasta tmp.fasta.fai   环境配置 1 2 3 4 5 6 7 8  # 安装circos conda create -c bioconda -n circos circos # 测试circos conda activate circos # 确认安装 circos -V # 输出如下 # circos | v 0.69-8 | 15 Jun 2019 | Perl 5.026002   Circos绘图 circos.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94  karyotype = karyotype.ref.txt,karyotype.qry.txt ## 设定显示的染色体 chromosomes_display_default = no chromosomes = refContig01;refContig02;refContig03;refContig04;refContig05;refContig06;refContig07;refContig08;refContig09;refContig10;refContig11;refContig12;refContig13;refContig14;refContig15;refContig16;qryContig01;qryContig02;qryContig03;qryContig04;qryContig05;qryContig06;qryContig07;qryContig08;qryContig09;qryContig10;qryContig11;qryContig12;qryContig13;qryContig14;qryContig15;qryContig16;qryContig17;qryContig18 ## 设定染色体的排列顺序 chromosomes_order = qryContig18,qryContig17,qryContig16,qryContig15,qryContig14,qryContig13,qryContig12,qryContig11,qryContig10,qryContig09,qryContig08,qryContig07,qryContig06,qryContig05,qryContig04,qryContig03,qryContig02,qryContig01 ## 逆转qry序列的方向（从最大值向零沿逆时针反向排列），正则匹配符合的染色体 chromosomes_reverse = /qryContig/ ## 设定染色体颜色 chromosomes_color = /ref/=pastel1-3-qual-1,/qry/=pastel1-3-qual-2 ## 此设定会ref与qry的染色体各占半圈，但每个染色体的长度都被缩放至同一长度 # chromosomes_scale = /refContig/:0.5rn;/qryContig/:0.5rn ## 设定染色体上的标尺 \u0026lt;\u0026lt;include ticks.conf\u0026gt;\u0026gt; \u0026lt;ideogram\u0026gt; \u0026lt;spacing\u0026gt; ## 染色体之间默认的间距 default = 1u ## 设定ref与qry之间的间隙稍大于一般值 \u0026lt;pairwise refContig01 qryContig01\u0026gt; spacing = 4u \u0026lt;/pairwise\u0026gt; \u0026lt;pairwise refContig16 qryContig18\u0026gt; spacing = 4u \u0026lt;/pairwise\u0026gt; \u0026lt;/spacing\u0026gt; radius = 0.90r thickness = 20p fill = yes stroke_color = dgrey stroke_thickness = 2p show_label = yes #展示label label_font = default # 字体 label_radius = dims(ideogram,radius) + 0.08r #位置 label_size = 16 # 字体大小 label_parallel = no # 是否平行 label_format = eval(sprintf(\u0026#34;%s\u0026#34;,var(chr))) # 格式 \u0026lt;/ideogram\u0026gt; \u0026lt;plots\u0026gt; \u0026lt;plot\u0026gt; ## 基因密度 type = line thickness = 1 max_gap = 1u file = genes_num.txt color = red r0 = 0.86r r1 = 0.95r \u0026lt;/plot\u0026gt; ## 重复序列密度 \u0026lt;plot\u0026gt; type = line thickness = 1 max_gap = 1u file = reps_num.txt color = blue r0 = 0.76r r1 = 0.85r \u0026lt;/plot\u0026gt; ## GC含量 \u0026lt;plot\u0026gt; type = heatmap file = both.GCrate.txt color = ylorrd-9-seq r1 = 0.95r r0 = 0.99r \u0026lt;/plot\u0026gt; \u0026lt;/plots\u0026gt; ## 设定染色体之间的关联信息 \u0026lt;\u0026lt;include ./etc/links.conf\u0026gt;\u0026gt; \u0026lt;image\u0026gt; angle_offset* = -88 ## 选项后加*表示覆盖已有的设置 dir* = . # 输出文件夹 radius* = 500p # 图片半径，默认为1500p svg* = yes # 是否输出svg \u0026lt;\u0026lt;include etc/image.conf\u0026gt;\u0026gt; \u0026lt;/image\u0026gt; \u0026lt;\u0026lt;include etc/colors_fonts_patterns.conf\u0026gt;\u0026gt; \u0026lt;\u0026lt;include etc/housekeeping.conf\u0026gt;\u0026gt;   links.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  \u0026lt;links\u0026gt; \u0026lt;link\u0026gt; file = req_qry.50k.LinkedRegion.info radius = 0.75r color = greys-9-seq-3 ribbon = yes \u0026lt;rules\u0026gt; \u0026lt;rule\u0026gt; condition = var(chr1) eq \u0026#34;refContig01\u0026#34; color=spectral-9-div-1 \u0026lt;/rule\u0026gt; \u0026lt;rule\u0026gt; condition = var(chr1) eq \u0026#34;refContig02\u0026#34; color=spectral-11-div-3 \u0026lt;/rule\u0026gt; \u0026lt;rule\u0026gt; condition = var(chr1) eq \u0026#34;refContig03\u0026#34; color=spectral-11-div-5 \u0026lt;/rule\u0026gt; \u0026lt;rule\u0026gt; condition = var(chr1) eq \u0026#34;refContig04\u0026#34; color=spectral-11-div-6 \u0026lt;/rule\u0026gt; \u0026lt;rule\u0026gt; condition = var(chr1) eq \u0026#34;refContig05\u0026#34; color=spectral-11-div-8 \u0026lt;/rule\u0026gt; \u0026lt;rule\u0026gt; condition = var(chr1) eq \u0026#34;refContig06\u0026#34; color=spectral-11-div-9 \u0026lt;/rule\u0026gt; \u0026lt;rule\u0026gt; condition = var(chr1) eq \u0026#34;refContig07\u0026#34; color=spectral-11-div-10 \u0026lt;/rule\u0026gt; \u0026lt;rule\u0026gt; condition = var(chr1) eq \u0026#34;refContig08\u0026#34; color=spectral-11-div-11 \u0026lt;/rule\u0026gt; \u0026lt;rule\u0026gt; condition = var(chr1) eq \u0026#34;refContig09\u0026#34; color=blue_a4 \u0026lt;/rule\u0026gt; \u0026lt;/rules\u0026gt; \u0026lt;/link\u0026gt; \u0026lt;/links\u0026gt;   ticks.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  chromosomes_units = 1000000 show_ticks = yes show_tick_labels = yes \u0026lt;ticks\u0026gt; radius = 1r color = black thickness = 2p multiplier = 1e-6 #输出的标签为实际长度与其相乘 format = %d # %d表示显示整数 \u0026lt;tick\u0026gt; spacing = 1u size = 5p \u0026lt;/tick\u0026gt; \u0026lt;tick\u0026gt; spacing = 5u size = 10p show_label = yes label_size = 10p label_offset = 10p format = %d \u0026lt;/tick\u0026gt; \u0026lt;/ticks\u0026gt;   单位  上面出现了控制图形不同元素大小的三个单位，p,r,u。p(pixels), 表示绝对大小， r(relative), 相对大小， u(chromosome unit)。 如果使用p作为单位，需要考虑最终输出图形\u0026lt;image\u0026gt;定义的radius。 而r是相对大小，会随着最终图形大小而发生变换。u一般在显示刻度时使用。\n 颜色 Circos中颜色的命名格式为PALETTE-NUMCOLORS-TYPE-IDX:\n PALETTE:调色版名，如rdylbu NUMCOLORS: 颜色数目, 11 调色版类型: div(diverging), seq(sequential), qual(qualitative) IDX: 调色版中的颜色索引  Circos的颜色设置来自https://colorbrewer2.org/。因此，gnbu-9-seq对应的是就是下图的9-class GnBu。因此，gnbu-9-seq对应的是就是下图的9-class GnBu。\n参考来源 http://xuzhougeng.top/tags/circos\nhttps://www.royfrancis.com/beautiful-circos-plots-in-r/\nhttp://xuzhougeng.top/archives/Using-MUMmer-to-align-genome\nhttp://xuzhougeng.top/archives/Visualization-of-MUMMER-Result\n","permalink":"https://sr-c.github.io/2020/06/03/Circo-Synteny-Panel/","tags":["Circos","Mummer"],"title":"Circos共线性比较"},{"categories":null,"contents":" 现已不推荐使用RPKM/FPKM对RNA-seq数据进行定量比较，详情可查看\n 计算原理 $$ RPKM = (10^9 * C) / (N * L) $$\nC = Number of reads mapped to a gene\nN = Total mapped reads in the experiment\nL = gene length in base-pairs for a gene\n使用方法 1  perl rpkm_script_beta.pl sample_count_test.count 2:9 28 \u0026gt; sample_count_test.rpkm   其中，sample_count_test.count为用于计算的count矩阵，其中第2-9列为需要计算的样品的位置(C)，第28列为每个基因的有效长度(L)。\n参考来源 https://github.com/decodebiology/rpkm_rnaseq_count\nhttps://www.bioinfo-scrounger.com/archives/342/\n","permalink":"https://sr-c.github.io/2020/05/26/RPKM-normilization/","tags":["RNA-seq","FPKM"],"title":"RPKM-normilization"},{"categories":null,"contents":"使用nf-core，能够很方便地进行RNA-seq的上游分析，得到表达矩阵。但是，大多数时候，我们更关心的是筛选差异表达基因。别担心，Trinity提供了一些简便的分析脚本供我们使用。\n整理上游数据 表达矩阵 在nf-core的结果目录中，featureCounts目录下的merged_gene_counts.txt为相应工具计算得到的表达矩阵。\n 如果在nf-core流程中还通过--pseudo_aligner salmon参数引入了salmon作为比对工具的话。在结果目录的salmon路径下的salmon_merged_gene_counts.csv为基因水平的表达矩阵，salmon_merged_transcript_counts.csv为转录本水平的表达矩阵。\n 经过简单的文本处理后，可以得到供后续分析的表达矩阵。\n1 2 3 4  cat merged_gene_counts.txt | \\  cut -f 1,3-23 | \\  sed \u0026#39;s/_1.cleanAligned.sortedByCoord.out.bam//g\u0026#39; | \\  sed \u0026#39;s/Geneid//\u0026#39; \u0026gt; featureCounts_merged_gene_counts.tab   featureCounts_merged_gene_counts.tab的内容形式如下\n1 2 3 4 5 6  GroupD3 GroupA5 GroupC1 GroupC2 GroupA2 GroupA1 GroupC5 GroupB6 GroupB1 GroupD2 GroupC3 GroupA3 GroupA6 GroupA4 GroupC4 GroupB5 GroupD1 GroupB2 GroupB4 IOZ07G0001 6005 8108 5144 6331 6539 10535 4612 7234 5743 5152 9328 5992 6663 9983 3619 7062 5809 8536 IOZ07G0002 2970 3090 3035 3306 1924 2709 3816 4076 4293 2196 4437 1816 1862 3397 3354 2791 2657 4181 IOZ07G0003 1380 2109 2057 2247 1966 2718 1941 1969 2012 1526 1860 1682 1735 2338 1899 1408 1613 1942 IOZ07G0004 6778 4952 9068 7378 4473 6409 6472 5752 6326 6779 6370 3759 3337 6875 5866 5574 6853 6540   分组信息 如果分组不太复杂的话，可手动编辑一个以换行符分隔的分组矩阵samples.txt，形式如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  GroupA GroupA1 GroupA GroupA2 GroupA GroupA3 GroupA GroupA4 GroupA GroupA5 GroupA GroupA6 GroupB GroupB1 GroupB GroupB2 GroupB GroupB3 GroupB GroupB4 GroupB GroupB5 GroupB GroupB6 GroupC GroupC1 GroupC GroupC2 GroupC GroupC3 GroupC GroupC4 GroupC GroupC5 GroupC GroupC6 GroupD GroupD1 GroupD GroupD2 GroupD GroupD3   简便差异分析 通过Trinity提供的脚本Analysis/DifferentialExpression/run_DE_analysis.pl可快速地调用DESeq2或voom进行差异表达分析。因此，运行前提要求当前的R环境中已安装好相应的软件包。\n1 2 3 4 5 6 7  matrix=[featureCounts_merged_gene_counts.tab, salmon_merged_gene_counts.tab] method=[DESeq2, voom] ~/software/trinityrnaseq-Trinity-v2.8.5/Analysis/DifferentialExpression/run_DE_analysis.pl \\  --matrix featureCounts_merged_gene_counts.tab \\  --samples_file samples.txt \\  --method DESeq2 \\  --output DESeq2.featureCounts.Organism.dir   通过--output指定输出路径，即可得到分析结果。\n一般来说，还需要通过FDR \u0026lt; 0.05 和 log2|FC| \u0026gt; 1的阈值筛选得到目标差异基因。\n参考来源 https://bioconductor.org/packages/3.7/bioc/vignettes/DESeq2/inst/doc/DESeq2.html\nhttps://abego.cn/2019/05/31/deseq-analysis-the-different-expression-gene/\nhttp://xuchunhui.top/2020/03/28/%E4%BD%BF%E7%94%A8DEseq2%E5%88%86%E6%9E%90RNA-seq%E6%95%B0%E6%8D%AE/\n","permalink":"https://sr-c.github.io/2020/05/25/RNA-seq-DiffExp-analysis/","tags":["nf-core","RNA-seq","Trinity"],"title":"简易计算RNA-seq差异表达基因"},{"categories":null,"contents":"#概述\n我们使用目前最流行的多序列比对工具MAFFT进行多序列比对。此外，还使用NCBI的GenBank来判断一些序列的同源序列。\n#比对\n可视化 使用AliView快速观察多序列比对结果。对于不适应的区域可以手动选择，重新局部比对。\n提取保守区域 使用\n参考来源 https://github.com/mmatschiner/tutorials/blob/master/multiple_sequence_alignment/README.md\n","permalink":"https://sr-c.github.io/2020/03/01/tutorials-Multiple-Sequence-Alignment/","tags":[],"title":"tutorials-Multiple-Sequence-Alignment"},{"categories":null,"contents":"nf-core使用nextflow作为流程管理工具，搭建了一系列分析流程，现已发表在Nat Biotechnol。其中，最为常用，关注也最多的自然是RNA-seq的流程。简单的解决方案可拉到最后。\n#安装\n每个nf-core的流程都支持3种配置方式，docker, singularity与conda. 使用这3种方式都可以配置，但是，前两者的安装都需要root权限，而作为普通用户，你的选择可能还是只有可爱的conda了。\nNextflow 首先需要环境中java的版本在1.8以上，然后安装到用户自定义的路径中，以方便后续的升级管理。\n1 2 3 4 5 6 7 8 9 10  # Make sure that Java v8+ is installed: java -version # Install Nextflow curl -fsSL get.nextflow.io | bash # Add Nextflow binary to your user\u0026#39;s PATH: mv nextflow ~/bin/ # OR system-wide installation: # sudo mv nextflow /usr/local/bin   或者，通过Bioconda安装\n1  conda install -c bioconda nextflow   Nextflow的升级就很简单， nextflow self-update 或conda update nextflow（取决于你的安装方式）即可。\n流程工具 Docker，Singularly或者Conda，前两者以容器形式运行，可重复性好，但其安装需要管理员权限。\n首先，需要确认你已经安装好了所需要的软件环境(Nextflow + Docker / Singularity / Conda)，尝试Nextflow的\u0026quot;hello world\u0026rdquo;\n1  nextflow run hello   流程工具的运行需要联网，会自动运行最新的流程配置。如果需要离线运行指定流程，可以参考官方提供的说明。\n1  nextflow run nf-core/rnaseq -profile test,\u0026lt;docker/singularity/conda\u0026gt;   测试运行nf-core/rnaseq流程。\n注意，请配置好conda的环境，即~/.condarc，否则conda的连接速度可能会很慢，导致报错。推荐的源配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  channels: - bioconda - defaults - conda-forge show_channel_urls: True channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r custom_channels: bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud   #参考基因组\n许多流程的序列比对，注释等流程都会用到参考基因组文件。nf-core默认可以使用Illumina iGenomes中支持物种的参考基因组。但是，如果需要使用自定义的参考基因组，也可以使用--fasta与--gtf参数，传递参考基因组序列与注释文件。\n其中，注释文件的推荐格式为gtf，要求其中有gene_biotype属性，指定基因为protein_coding, lincRNA, rRNA或其他。\n也可提供一个gff文件，但是其会被先转换为gtf文件。\ngtf文件的格式最好以ENSEMBL版本为参考。若为GENCODE格式，则需提供--gencode参数注明。\n其他 Nextflow的活动流程要求其进程需要一直运行，直至流程完成。因此，推荐将其运行在screen/tmux的后台环境中，或者投递到集群的作业系统中。\n同时，官方还推荐在系统环境配置中限制NextflowJava虚拟机的内存占用，向~/.bashrc或~/.bash_profile中添加\n1  NXF_OPTS=\u0026#39;-Xms1g -Xmx4g\u0026#39;   快速开始 一个命令完成RNA-seq的方式如下:\n1 2 3 4 5 6 7 8 9 10  nextflow run nf-core/rnaseq \\ --reads \u0026#39;input_reads/*_{1,2}.clean.fq.gz\u0026#39; \\ # 路径必须以单引号包括 --fasta /path/to/reference/genome/ref.fasta \\ --gtf /path/to/reference/annotation/ref.gtf \\ -profile conda \\ --pseudo_aligner salmon \\ # 除了HISAT2外，还使用salmon进行比对 -name name_for_the_pipeline_run \\ --max_cpus 16 \\ --max_memory \u0026#39;8.GB\u0026#39; # --reverseStranded   如果RNA-seq的是链特异性文库，可以手动指定文库的方向，如--reverseStranded指定在HISAT2比对时指定--rna-strandness RF\n参考来源 https://nf-co.re/pipelines\nhttps://github.com/nf-core/rnaseq/blob/master/docs/usage.md#transcript-ids-in-fasta-files\n","permalink":"https://sr-c.github.io/2020/03/01/nf-core-rnaseq/","tags":["nf-core","RNA-seq"],"title":"nf-core一步完成RNA-seq上游分析"},{"categories":["Genomics"],"contents":"向NCBI提交基因组数据的关键在于提供符合NCBI规范的注释信息。NCBI要求其专门的5列的注释形式，以.tbl文件提供。当时该文件的生成比较\n获取locus_tag locus_tag其实是一个定位的标签，指向所提交基因组信息的BioProject与BioSample组合。在申请完成BioProject与BioSample后，会在NCBI返回的邮件中告知。若没有收到，也可以邮件向NCBI询问。\n准备序列文件 Fasta文件 如果没有基因组的注释信息，则可以直接提交fasta文件。\n.sqn文件 具有注释信息的基因组序列，应当以.sqn文件提交。\n准备.sqn文件需要的\n 序列fasta文件 数据提交方和出版方信息的模板文件 注释文件 序列质量文件（可选）  其中，最关键的注释文件，可以以NCBI要求的5列的列表文件给出。现在，可以通过比较常见的gtf或gff文件形式给出，使用table2asn_GFF工具整合注释信息。\n校验注释文件完整规范 通过genometools提供的gff工具可检验gff文件是否符合规范。\nGenBank对注释信息的要求  GFF3/GTF文件中第一列的序列名称必须与fasta文件中的序列名称一致 contig, supercontig, chromosome 等信息并不必要，而且在整合过程中被忽略。 gene和mRNA注释可以提供，但不是必须。只有CDS信息是必须的，而其他信息会根据CDS信息自动生成。  准备注释信息 检验报错信息 报错处理 发现的错误往往包括许多类型。但应当保证其中没有ERROR\n参考来源 https://www.ncbi.nlm.nih.gov/genbank/genomesubmit/\nhttps://www.ncbi.nlm.nih.gov/genbank/genomes_gff/\n","permalink":"https://sr-c.github.io/2020/02/25/submit-genome-to-NCBI/","tags":["NCBI"],"title":"向GenBank提交基因组数据"},{"categories":null,"contents":"在manjaro中替换fcitx输入法为ibus, 设置启动项，并配置环境调用。\n安装 1 2  yay -S ibus yay -S ibus-rime   配置启动项 添加ibus启动项 在/etc/xdg/autostart（或~/.config/autostart）路径中新建一个可执行文件ibus.desktop, 内容如下:\n1 2 3 4 5 6 7 8 9  [Desktop Entry] Exec=ibus-daemon -xdr GenericName=IBus Name[zh_CN]=IBus Name=IBus Name[en_US]=IBus StartupNotify=true Terminal=false Type=Application   并设置可执行权限\n1  chmod 755 ibus.desktop   更改fcitx启动项 fcitx的启动项可能会在/etc/xdg/autostart或~/.config/autostart中，注意删除或重命名备份之。\n配置与其他应用配合 在/etc/profile或~/.xprofile中加入以下内容\n1 2 3  export GTK_IM_MODULE=ibus export QT_IM_MODULE=ibus export XMODIFIERS=@im=ibus   配置rime输入法 RIME输入法，准确地说是RIME输入法引擎，是“聪明的输入法懂我心意”。简单配置以下就可快速上手。\n切换繁简中文输入 按组合键Ctrl + 或F4即可呼出输入方案选项。第一栏为目前正在应用的输入法，第二栏为目前输入法的设置，之后的选项为其他可选输入法。\n在第二栏中，选择4，将默认的繁体输入改为简体即可。此外，其中还可配置全角半角等。\n配置双拼 默认的【朙月拼音】中存在几个流行的双拼方案：自然码、MSPY、智能ABC、小双拼、拼音加加，在~/.config/ibus/rime中新建配置文件default.custom.yaml, 在其中写入\n1 2 3 4  patch: schema_list: - schema: luna_pinyin - schema: double_pinyin   默认方案似乎就是自然码。\n详细配置 参考配置文件说明\n参考来源 https://www.lulinux.com/archives/5297\nhttps://www.linuxsecrets.com/archlinux-wiki/wiki.archlinux.org/index.php/Rime_IME.html\n","permalink":"https://sr-c.github.io/2019/12/21/ibus-rime/","tags":["ibus","rime"],"title":"使用ibus输入法"},{"categories":null,"contents":"安装 1 2 3 4 5  #安装一个田间实验设计方面的包，为我们提供LSD.test()等好用的函数 install.packages(\u0026#34;agricolae\u0026#34;) library(agricolae) ##读入数据 data(sweetpotato)   安装过程可能会报错，需要我们手动安装udunits，gdal等依赖。\n方差齐性检验 1  oneway.test()   方差分析 1  model \u0026lt;- aov(yield~virus, data = sweetpotato)   均值的多重比较 LSD法 1 2 3 4 5 6  #多重比较，不矫正P值 out \u0026lt;- LSD.test(model, \u0026#34;virus\u0026#34;, p.adj = \u0026#34;none\u0026#34;) ##标记字母法显示比较结果 out$group #可视化 plot(out)   参考来源 http://vlambda.com/wz_x45GtwxNzx.html\nhttps://data-flair.training/blogs/anova-in-r/\nhttps://www.jianshu.com/p/553cc5987f61\n","permalink":"https://sr-c.github.io/2019/11/15/ANOVA-in-R/","tags":["R","ANOVA"],"title":"ANOVA-in-R"},{"categories":null,"contents":"目前一般的方法有VICUNA, PRICE, IVA, 因此，我们尝试使用这3种方法进行组装。\n本地工具 VICUNA VICUNA出自大名鼎鼎的borad研究所，但在2012年后，就停止了开发。\nPRICE DeRisi实验室出品，2014年停止开发。组装需提供一个seed sequence\nIVA 总结之前的工具，发表于Gigascience\n网页工具 VirAmp 默认使用velvet进行组装\n VirAmp workflow\nVirus Assembly Pipeline contains a set of programs to assemble the the virus genome from shotgun sequence in a fast and accurate way with a related genome as guide. In general, it contains the following steps:\n Quality Control  This step trims out the low quality bases in reads from both end using seqtk toolkit\nDiginal Normalization  Digital normalization (diginorm) uses a single-pass computational algorithm to reduce the high and redundant coverage in shotgun sequence dataset, thus significantly decreasing time and memory requirements for the de novo assembly while retaining the accuracy.\nNotice Since Digital Normalization is designed to deal with extreme high coverage data (x1000 or above), by default it\u0026rsquo;s not turned on. One can choose to turn on at the input panel.\nDe novo assembly (velvet)  This step uses the velvet program to do a de novo assembly from the shotgun sequence. This step results in a set of contigs of various lengths.\nReference-guided Scaffolding (AMOScmp)  Contigs generated from velvet in step 3 are oriented and connected into scaffolds by the reference-guided assembly tool AMOScmp\nScaffolding using paired-end information (SSPACE)  Uses paired-end reads from the original dataset to extend and scaffold contigs\nAssembly assessment  QUAST (Quality Assessment Tool for Genome Assemblies) program is used for the statistical assessment of the assemblies resulting from step 4\nSNV generation  MUMmer is used to align the assembly results to the reference genome and report SNVs.\nDraft and reference genome comparison  MUMmer is used to compare the assembly results (draft genome) with the reference genome.\n VirusTAP https://www.frontiersin.org/articles/10.3389/fmicb.2016.00032/full\n需注册\n参考来源 ","permalink":"https://sr-c.github.io/2019/10/26/Virus-genome-assembly/","tags":["assembly"],"title":"Viral-genome-assembly"},{"categories":null,"contents":"安装 1  sudo pacman -S docker   为普通用户配置权限 创建docker用户组 1  sudo groupadd docker   将当前用户添加到docker用户组中 1  sudo usermod -aG docker $USER   注销后重新登录，或直接使修改生效 1  newgrp docker   验证 1  docker run hello-world   设置镜像加速 对于使用systemd的系统，新建/etc/docker/daemon.json，并写入\n1 2 3 4 5 6  { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://dockerhub.azk8s.cn\u0026#34;, \u0026#34;https://reg-mirror.qiniu.com\u0026#34; ] }   然后，重启服务\n1 2  sudo systemctl daemon-reload sudo systemctl restart docker   使用nginx服务 小试牛刀，使用docker提供的nginx服务\n1  docker run -d -p 80:80 --restart=always nginx:latest   后台运行的Docker中的nginx就指向的本机的80端口。打开http://localhost:80就可打开nginx的欢迎页。\n查看已有的容器\n1 2 3 4  docker container ls --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b37e7689fdfe nginx:latest \u0026#34;nginx -g \u0026#39;daemon of…\u0026#34; 3 minutes ago Up 3 minutes 0.0.0.0:80-\u0026gt;80/tcp competent_mirzakhani dfe9c2dbb13b hello-world \u0026#34;/hello\u0026#34; 3 hours ago Exited (0) 3 hours ago upbeat_lederberg   可以看到每个容器对应的唯一ID\n直接修改欢迎页的内容\n1 2 3  docker exec -it b37e7689fdfe bash echo \u0026#39;\u0026lt;h1\u0026gt;Hello Docker\u0026lt;h1/\u0026gt;\u0026#39; \u0026gt; /usr/share/nginx/html/index.html   然后Ctrl + D或exit退出容器，刷新http://localhost:80即可看到修改后的主页。\n参考来源 https://docker_practice.gitee.io/\nhttps://docs.docker.com/get-started/\nhttps://juejin.im/post/5c2c69cee51d450d9707236e\nhttps://jimolonely.github.io/2018/04/02/cloudcompute/004-docker/\n","permalink":"https://sr-c.github.io/2019/07/19/Docker-basic/","tags":["Docker"],"title":"Docker初探"},{"categories":null,"contents":"PCA 主成分分析是处理多维数据时降维的方法。举例说明，不同处理组中上万个基因的表达量矩阵，也就对应着上万个维度。\nStep 1 投射Gene1 与Gene2 的表达量。取其中点（重心）为原点，平移图像。在取得最适合的过原点的直线。其到各点的距离(b)和最短，或各点在直线上映射点到原点的距离(c)和最大。\n 由于 $$ a^2 = b^2 + c^2 $$\n 计算映射点到原点的距离平方之和 $$ d1^2 + d2^2 + d3^3+d4^2+d5^2+d6^2=sum of squared distances=SS(distances) $$ 取得的这一直线就被称为principal component one (PC1)\nPC1 的斜率反映了Gene1 与Gene2在PC1上的分布\n how the data are spread out\n 特征向量\nTerminology Alert!!! linear combination PC1 is a linear combination of variables\nPCA with Singular Value Decomposition (SVD)\n特征向量 Singular Vector / Eigenvector Eigenvalue for PC1\n词汇表 projected point 映射点\ninversely related 负相关\nintuitively 直觉地\nrotate the line\nspread out\nPour over ice and serve!\nproportion\n e.g. proportions of each gene are called loading scores.\n perpendicular 垂直的\napproximation 近似\ninformative 有信息的\nsubstantial 相当可观的\n参考来源 https://www.youtube.com/watch?v=FgakZw6K1QQ\nhttps://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors\n","permalink":"https://sr-c.github.io/2019/07/19/StatQuest-PCA/","tags":["StatQuest","PCA"],"title":"StatQuest-PCA"},{"categories":null,"contents":"PBJelly在2012年发表于PLoS ONE, 其设计目的是使用PacBio RS Ⅱ 数据来提升assembly的连续性。目前版本停滞在2015年9月1日的15.8.24，但仍然有19 downloads/week的下载热度。\n安装篇 legacy-blasr PBJelly的安装说明表示依赖1.3.1版本的blasr，但此版本在2012年，现已难以安装。\n几番尝试后，我们选择使用PacBio提供的pitchfork来编译该过时的软件。\n1 2 3 4 5 6 7  cd /path/to/software git clone https://github.com/PacificBiosciences/pitchfork.git cd pitchfork git checkout legacy_blasr echo PREFIX=/opt/mybuild \u0026gt; settings.mk make init make blasr   此过程会下载依赖的各种软件，python3环境中会报错，需要切换到python2.7环境。\n此外，hdf5-1.8.16的下载路径失效，自行下载hdf5-1.8.16.tar.gz至pitchfork/ports/thirdparty/hdf5中，重新make blasr可解决。\n编译完成后，实际的blasr版本为5.0.994e5fc，将其导入环境变量\n1  source /opt/mybuild/setup-env.sh   networkx v1.1 说明文档要求1.1版本，实测conda安装的1.1.1版本也可成功运行。\n1  conda install networkx==1.1.1    p.s. 在extraction步骤，存在警告\n2019-07-04 20:40:10,451 [INFO] Opening GML Files 2019-07-04 20:40:10,452 [WARNING] It is unknown if networkx version 1.11 will work. 2019-07-04 20:40:10,452 [WARNING] If you get an error here, please report it!!!\n PBJelly 1 2 3 4 5 6  wget https://sourceforge.net/projects/pb-jelly/files/PBSuite_15.8.24.tgz tar zxf PBSuite_15.8.24.tgz cd PBSuite_15.8.24 ## edit SWEETPATH in setup.sh vi setup.sh source setup.sh   测试篇 按照示例教程进行测试\n1 2 3 4 5 6 7 8 9 10 11 12 13  cd /path/to/PBSuite/docs/jellyExample ## edit the paths in the \u0026lt;reference\u0026gt; , \u0026lt;outputDir\u0026gt; and the baseDir attribute in the inputs tag to the full path in Protocol.xml vi Protocol.xml #Jelly.py \u0026lt;stage\u0026gt; Protocol.xml Jelly.py setup \u0026lt;jellyprotocol.xml\u0026gt; Jelly.py mapping \u0026lt;jellyprotocol.xml\u0026gt; Jelly.py support \u0026lt;jellyprotocol.xml\u0026gt; Jelly.py extraction \u0026lt;jellyprotocol.xml\u0026gt; Jelly.py assembly \u0026lt;jellyprotocol.xml\u0026gt; -x “--nproc=4” Jelly.py output \u0026lt;jellyprotocol.xml\u0026gt; grep -Ho N jelly.out.fasta | uniq -c   运行成功，会将输入的3条contig连接为1条，N的数目由1494降至0\n检查输入 1 2  sumarizeAssembly.py \u0026lt;reference.fasta\u0026gt; readSummary.py \u0026lt;Protocol.xml\u0026gt;   PBJelly要求的reference输入格式只能是fasta格式，且后缀只能是.fasta；reads的输入格式可以为fastq或fasta，但输入fasta时要求有相应的质量文件\nDebug 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  Jelly.py setup \u0026lt;jellyprotocol.xml\u0026gt; ##切换到screen中运行 screen -S PBJelly Jelly.py mapping \u0026lt;jellyprotocol.xml\u0026gt; ##比对步骤很快结束，查验mapping文件夹内的mapping/xxx.fastq.err cat mapping/xxx.fastq.err 2019-07-02 20:03:34,546 [INFO] Running /path/to/software/PBSuite_15.8.24/bin/m4pie.py /path/to/workdir/04.genome_assembling/gap-filling/PBJelly/yourAssembly/mapping/AssemblyXXX.fastq.m4 /path/to/workdir/04.genome_assembling/gap-filling/PBJelly/yourAssembly/data/reads/AssemblyXXX.fastq /path/to/workdir/04.genome_assembling/gap-filling/PBJelly/yourAssembly/data/reference/scaffolds_FINAL.fasta --nproc 4 -i Traceback (most recent call last): File \u0026#34;/path/to/software/PBSuite_15.8.24/bin/m4pie.py\u0026#34;, line 209, in \u0026lt;module\u0026gt; run(sys.argv[1:]) File \u0026#34;/path/to/software/PBSuite_15.8.24/bin/m4pie.py\u0026#34;, line 171, in run aligns = M4File(args.m4) File \u0026#34;/path/to/software/PBSuite_15.8.24/pbsuite/utils/FileHandlers.py\u0026#34;, line 484, in __init__ file = open(file,\u0026#39;r\u0026#39;) IOError: [Errno 2] No such file or directory: \u0026#39;/path/to/workdir/04.genome_assembling/gap-filling/PBJelly/yourAssembly/mapping/AssemblyXXX.fastq.m4   搜索PBSuite_15.8.24/bin/m4pie.py no such file，能发现很多类似情况的报错\n https://sourceforge.net/p/pb-jelly/discussion/pbjtiks/thread/0c780910/?limit=25\nhttps://www.biostars.org/p/198519/\nhttps://sourceforge.net/p/pb-jelly/discussion/pbjtiks/thread/e351eed7/#a608\n 原因不一，包括新旧版本blasr在参数输入中，使用-与--的不同。\n在pbsuite/utils/CommandRunner.py的第18行的subprocess.Popen()函数中添加executable=\u0026quot;/bin/bash\u0026quot;\n但此次报错的可能原因在于screen后的环境配置与原环境不同，使blasr不能正常运行。\n判断依据 在screen屏幕中，blasr不可执行\n1 2  blasr --help blasr: error while loading shared libraries: libpbihdf.so: cannot open shared object file: No such file or directory   在原环境中执行Jelly.py mapping \u0026lt;jellyprotocol.xml\u0026gt;, blasr即可正常运行。\n1 2  ## 传递debug参数，帮助排查问题 Jelly.py mapping \u0026lt;jellyprotocol.xml\u0026gt; -x \u0026#34;--debug\u0026#34;   小tips Ctrl + C手动终止mapping任务时，需注意，blasr不会相应终止，需要在任务管理器中终止。\n总结 运行PBJelly之前，需要依次激活下列环境\n1 2 3 4 5 6  ## pitchfork编译的legacy-blasr环境 source ~/opt/pitchfock/setup-env.sh ## 安装有networkx1.1的python2.7环境 source activate SALSA ## PBSuite环境 source ~/software/PBSuite_15.8.24/setup.sh   参考来源 http://onsnetwork.org/kubu4/2017/10/30/software-installation-pb-jelly-suite-and-blasr-on-emu/\nhttps://genefish.wordpress.com/2017/04/06/pbjelly-pt-2/\nhttps://github.com/alvaralmstedt/Tutorials/wiki/Gap-closing-with-PBJelly\n","permalink":"https://sr-c.github.io/2019/07/02/PBJelly-and-blasr-installation/","tags":["PBJelly","blasr","PacBio"],"title":"使用PBJelly与blasr进行补洞"},{"categories":null,"contents":"初始配置 1 2 3 4  library(limma) library(Glimma) library(edgeR) library(Mus.musculus)   读入数据 1 2 3 4 5 6 7  #使用DGEList读取counts矩阵 genelist \u0026lt;- DGEList(counts = salmon_counts[2:31], group = group) #使用tximport读入数据 ## 读入分组信息 ##读入基因注释信息 genelist$genes   过滤低表达基因 推荐使用edgeR提供的filterByExpr函数进行过滤\n1 2 3 4 5 6 7  #直接根据所有样品中的表达量进行过滤 table(rowSums(genelist$counts==0)==30) #使用 keep.exprs \u0026lt;- filterByExpr(genelist, group = group) genelist \u0026lt;- genelist[keep.exprs,, keep.lib.sizes=FALSE] dim(genelist)   标准化 1 2 3  x \u0026lt;- calcNormFactors(genelist, method = \u0026#34;TMM\u0026#34;) x$genes \u0026lt;- x$genes[rownames(x$counts),]   差异表达分析 limma-voom 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # 输入设计矩阵 design \u0026lt;- model.matrix(~0+group) colnames(design) \u0026lt;- levels(group) logcpm \u0026lt;- cpm(x, log = TRUE, prior.count = 2) contr.matrix \u0026lt;- makeContrasts( IOZ07.vs.1621 = Mature - Mature1621, Mature.vs.vitro = Mature - Invitro, vitro.vs.1621 = Proliferation - Mature, Mature.vs.Transition = Mature - Transition, Transition.vs.Mycelium = Transition - Mycelium, levels = colnames(design) ) v \u0026lt;- voom(x, design, plot = TRUE) vfit \u0026lt;- lmFit(v, design) vfit \u0026lt;- contrasts.fit(vfit, contrasts = contr.matrix) efit \u0026lt;- eBayes(vfit) plotSA(efit, main=\u0026#34;Final model: Mean-variance trend\u0026#34;) summary(decideTests(efit))   参考来源 https://bioconductor.org/packages/release/workflows/vignettes/RNAseq123/inst/doc/limmaWorkflow_CHN.html https://github.com/COMBINE-lab/continuous_analysis_rnaseq https://bioconductor.org/packages/devel/bioc/vignettes/tximport/inst/doc/tximport.html#limma-voom\n","permalink":"https://sr-c.github.io/2019/06/20/RNA-seq-123/","tags":["RNAS-seq"],"title":"RNA-seq-123"},{"categories":null,"contents":"升级方案 node.js 使用n模块管理node版本 1 2 3 4 5 6 7 8 9 10  #查看当前node版本 node -v #清除npm chche npm cache clean -f #安装n模块 npm install -g n #使用n模块安装最新稳定版 n stable #或者使用n模块安装指定版本 n 8.1.2   Windows下还可以以安装包直接覆盖安装，进行升级。\n1 2  #查看node安装路径 where node   Hexo 直接使用npm i hexo-cli -g 和 npm update升级并不完整。正确的姿势是\n1 2 3 4 5 6 7 8 9  #安装npm-check与npm-upgrade npm install -g npm-check npm install -g npm-upgrade #检查并更新 npm-check npm-upgrade #一路回车确认安装即可 #在本地生产环境安装 npm install --save   最后一步的作用如下\n 会把依赖包安装到 node_modules 目录中 会在package.json的dependencies属性下添加依赖包的名称和版本号 之后运行npm install命令时，会自动安装依赖包到node_modules目录中  查看package.json中的dependencies属性可确认更新。\nNext Hexo主题的更新说明是直接通过git pull更新，但是跨度较大时，你很可能会发现配置文件挂掉。因此，我们需要采取单独的配置策略\n简单来说，就是将站点和主题的配置一起放置在source/_data/next.yml，而不需要单独去管理站点配置_config.yml和主题配置theme/next/_config.yml\n首先，确认主题默认配置theme/next/_config.yml中的override设置为false。然后，将站点配置_config.yml的全部内容和主题配置theme/next/_config.yml中自定义的部分，整合到source/_data/next.yml，以后该文件即可完成所有的配置。\n参考来源 https://juejin.im/post/5c09e47ee51d45721d71087d\nhttps://hexo.imydl.tech/archives/51612.html\n","permalink":"https://sr-c.github.io/2019/06/05/hexo-upgrade/","tags":["node.js","Hexo","Next"],"title":"Hexo升级"},{"categories":null,"contents":"Scallop Scallop可组装出新转录本，作为特定条件下的补充。\nSalmon 构建索引\n1  salmon index -t athal.fa.gz -i athal_index --type quasi -k 31   直接定量\n1  salmon quant -i transcripts_index -l A -1 reads_1.fastq.gz -2 reads_2.fastq.gz -o transcripts_valid -p 16 --validateMappings   结果导入\n1 2  library(tximport) txi.salmon \u0026lt;- tximport(\u0026#39;quant.sf\u0026#39;, type = \u0026#34;salmon\u0026#34;, tx2gene = tx2gene)   多样本处理 循环处理 1 2 3 4 5 6 7 8 9 10  #!/bin/bash for fn in ERR1698{194..209}; do samp=`basename ${fn}` echo \u0026#34;Processin sample ${sampe}\u0026#34; salmon quant -i athal_index -l A \\  -1 ${samp}_1.fastq.gz \\  -2 ${samp}_2.fastq.gz \\  -p 8 -o quants/${samp}_valid \\  --validateMappings done   声明结果文件路径 1 2 3 4 5 6  dir \u0026lt;- \u0026#34;C:/Users/Xu/Desktop/\u0026#34; list.files(dir) sample \u0026lt;- paste0(\u0026#34;ERR1698\u0026#34;,seq(194,209),\u0026#34;_valid\u0026#34;) files \u0026lt;- file.path(dir,\u0026#34;quants\u0026#34;,sample,\u0026#34;quant.sf\u0026#34;) names(files) \u0026lt;- paste0(\u0026#34;sample\u0026#34;,c(1:16)) all(file.exists(files))   准备转录本与基因的对应关系 1 2 3 4 5 6 7 8 9 10 11 12 13  # 从公共数据获取 library(AnnotationHub) ah \u0026lt;- AnnotationHub() ath \u0026lt;- query(ah,\u0026#39;thaliana\u0026#39;) ath_tx \u0026lt;- ath[[\u0026#39;AH52247\u0026#39;]]columns(ath_tx) k \u0026lt;- keys(ath_tx,keytype = \u0026#34;GENEID\u0026#34;) df \u0026lt;- select(ath_tx, keys=k, keytype = \u0026#34;GENEID\u0026#34;,columns = \u0026#34;TXNAME\u0026#34;) tx2gene \u0026lt;- df[,2:1] # TXID在前， GENEID在后 # 或自行制备 GENEID \u0026lt;- paste0(\u0026#34;ObjectGene\u0026#34;,sprintf(\u0026#34;%04d\u0026#34;, c(1:8946))) TXNAME \u0026lt;- paste0(\u0026#34;ObjectGene\u0026#34;,sprintf(\u0026#34;%04d\u0026#34;, c(1:8946))) tx2gene \u0026lt;- data.frame(GENEID, TXNAME)   导入数据 1 2 3 4 5 6 7 8 9 10 11 12  # install.packages(\u0026#34;readr\u0026#34;) # install.packages(\u0026#34;rsjon\u0026#34;) library(\u0026#34;tximport\u0026#34;) library(\u0026#34;readr\u0026#34;) txi \u0026lt;- tximport(files, type = \u0026#34;salmon\u0026#34;, tx2gene = tx2gene) names(txi) head(txi$length) head(txi$counts) write.csv(txi$abundance, \u0026#34;salmon_valid_TPM.csv\u0026#34;, quote = FALSE)   计算差异表达 1 2 3 4  library(\u0026#34;DESeq2\u0026#34;) sampleTable \u0026lt;- data.frame(condition = factor(rep(c(\u0026#34;Day0\u0026#34;,\u0026#34;Day1\u0026#34;,\u0026#34;Day2\u0026#34;,\u0026#34;Day3\u0026#34;),each=4))) rownames(sampleTable) \u0026lt;- colnames(txi$counts) dds \u0026lt;- DESeqDataSetFromTximport(txi, sampleTable, ~condition)   参考来源 http://www.biotrainee.com/thread-1586-1-1.html\nhttp://www.biotrainee.com/thread-1602-1-1.html\nhttps://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html\nhttps://vidotto.top/post/scallop%E5%92%8Csalmon%E5%BA%94%E7%94%A8%E8%BD%AC%E5%BD%95%E6%9C%AC%E5%AE%9A%E9%87%8F%E5%88%86%E6%9E%90/\n","permalink":"https://sr-c.github.io/2019/05/28/salmon-tximport/","tags":["RNA-seq","Salmon","R","tximport"],"title":"Salmon快速转录本定量"},{"categories":null,"contents":"定量过程可以分为三个水平：基因水平(gene-level), 转录本水平(transcript-level), 外显子水平(exon-usage-level).\n标准化过程 考虑测序深度与转录本长度对表达量的影响\nTPM Transcripts per million (TPM) 是对RNA-seq的推荐标准化方法。\nRPKM/FPKM 由counts直接除以测序深度与转录本长度，计算得到\nFPKM to TPM TPM的计算公式可变形为\n因此，可以由FPKM转化得到TPM\nR code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  countToTpm \u0026lt;- function(counts, effLen) { rate \u0026lt;- log(counts) - log(effLen) denom \u0026lt;- log(sum(exp(rate))) exp(rate - denom + log(1e6)) } countToFpkm \u0026lt;- function(counts, effLen) { N \u0026lt;- sum(counts) exp( log(counts) + log(1e9) - log(effLen) - log(N) ) } fpkmToTpm \u0026lt;- function(fpkm) { exp(log(fpkm) - log(sum(fpkm)) + log(1e6)) } countToEffCounts \u0026lt;- function(counts, len, effLen) { counts * (len / effLen) } ################################################################################ # An example ################################################################################ cnts \u0026lt;- c(4250, 3300, 200, 1750, 50, 0) lens \u0026lt;- c(900, 1020, 2000, 770, 3000, 1777) countDf \u0026lt;- data.frame(count = cnts, length = lens) # assume a mean(FLD) = 203.7 countDf$effLength \u0026lt;- countDf$length - 203.7 + 1 countDf$tpm \u0026lt;- with(countDf, countToTpm(count, effLength)) countDf$fpkm \u0026lt;- with(countDf, countToFpkm(count, effLength)) with(countDf, all.equal(tpm, fpkmToTpm(fpkm))) countDf$effCounts \u0026lt;- with(countDf, countToEffCounts(count, length, effLength))   使用apply函数计算TPM\n1 2  tpms \u0026lt;- apply(expMatrix, 2, fpkmToTpm) tpms[1:3,]   检查每列的总和是否一致\n1  colSums(tpms)   参考来源 https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/\nhttps://www.jianshu.com/p/9dfb65e405e8\nhttps://zhuanlan.zhihu.com/p/26750663\nhttp://www.bioinfo-scrounger.com/archives/342\n","permalink":"https://sr-c.github.io/2019/05/27/RNA-seq-expression-units/","tags":["RNA-seq"],"title":"RNA-seq的表达量计算单元"},{"categories":null,"contents":"最近看老莱视频，了解到用户友好的manjaro(竟然内置steam), 再加上强大的AUG, 忍不住来尝试一下。\n安装 使用dd将安装镜像写入到u盘\n1 2 3 4 5  #确认u盘的挂载位置，写入前取消挂载 df -h umount /dev/sdx #使用dd命令写入 dd if=path/to/manjaro.iso of=/dev/sdx bs=4M status=progress   系统安装过程非常方便，manjaro的安装引导非常友好，在分区过程的提示很细致，还会提示每个磁盘的分区表类型。\n最简单安装只需指定efi的引导分区/boot/efi和/挂载点即可\n 由于 Windows10 本身的引导分区就是 ESP 分区，所以到分区的界面，只需要将 /boot/efi 挂载到 ESP 分区即可。\n 软件源配置 安装时语言设置为en.utf-8, 时区设置为上海，默认就安装了中文支持。\n软件源 1 2 3 4  #选择最快的镜像源 sudo pacman-mirrors -i -c China -m rank #刷新缓存 sudo pacman -Syy   添加ArchLinuxCN源 1 2 3 4 5 6  #在文件/etc/pacman.conf末尾添加如下内容 sudo echo \u0026#39;[archlinuxcn]\\nSigLevel = Optional TrustedOnly\\nServer = https://mirrors.ustc.edu.cn/archlinuxcn/$arch\u0026#39; \u0026gt;\u0026gt; /etc/pacman.conf #导入archlinuxcn-keyring sudo pacman -Syy \u0026amp;\u0026amp; sudo pacman -S archlinuxcn-keyring #再次刷新缓存，下载速度应该就比较令人满意了 sudo panman -Syy   时间设置 对于Windows与linux双系统的用户，系统时间的设置往往是一个比较头疼的问题。\n问题原因 1  timedatectl   可以看到，系统时间实际分为Local time,Universal time与RTC time\nWindows使用的是本地时间Local time, 而UNIX使用的是UTC时间Universal time.\nWindows会认为主板BIOS上的硬件时间就是本地时间，同步时间后也会将时间写入BIOS. UNIX则认为硬件时间是UTC时间，同步时间后会将本地时间按照时区计算后得到的UTC时间写入主板。\n这样，在一个已有Windows的电脑上，BIOS的硬件时间就会被写为本地时间。之后安装的UNIX会将BIOS上的硬件时间识别为UTC时间，再根据时区信息计算出本地时间，显示在系统上。因此，我们会看到UNIX默认显示的时间与实际本地时间相差8个小时。\n解决方案 因此，该问题就存在两种解决方案。一是让Linux使用本地时间local time, 或是让Windows使用UTC时间。以下方案择一即可。\nLinux方案 修改为本地时间（目前使用此方案）\n1 2  timedatectl set-local-rtc 1 --adjust-system-clock timedatectl set-ntp 0   使用NTP同步时间\n1  sudo ntpdate cn.ntp.org.cn #可以换成任何一个公开的NTP Server地址   Windows方案 修改注册表，且需关闭时间同步。（未执行）\n [HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation]\n\u0026ldquo;RealTimeIsUniversal\u0026quot;=dword:00000001\n 常用软件 安装fcitx 1 2 3 4 5 6  sudo pacman -S fcitx-googlepinyin sudo pacman -S fcitx-im sudo pacman -S fcitx-configtool #在桌面环境配置文件中写入相关设置 echo \u0026#34;export GTK_IM_MODULE=fcitx\\nexport QT_IM_MODULE=fcitx\\nexport XMODIFIERS=@im=fcitx\u0026#34; \u0026gt;\u0026gt; ~/.xprofile   注销后即可正常使用fcitx\n 若初始选择安装的英文系统，再安装fcitx可能会使得fcitx中的中文输入法不能正常调用，此时应修改本地化设定中的语言\n 安装yay 1  sudo panman -S yay   由于yaourt目前已停止开发，存在安全问题，故选择yay进行包管理\n其他软件 1  yay -S wps-office nutstore typora sublime-text-imfix visual-studio-code-bin   其他设置 字体配置 XFCE桌面默认的字体配置在外观中设置。默认字体为Noto Sans Regular, 默认等宽字体为Monospace Regular, 已有不错的显示效果。\n字体大小默认为10号字，根据屏幕情况可自行调整至12或14号字。\n自启动配置 配置文件位于～/.config/autostart/目录下，也可通过会话和启动进行设置。\n参考坚果云写入的自启动方式，可自行将启动项写入为\n1  sh -c \u0026#34;(sleep 30 \u0026amp;\u0026amp; nohup /opt/nutstore/bin/nutstore-pydaemon.py \u0026gt;/dev/null 2\u0026gt;\u0026amp;1) \u0026amp;\u0026#34;   参考来源 https://wenqixiang.com/manjaro-guide/\nhttps://www.lanbu.net/d/52/11\nhttp://blog.sina.com.cn/s/blog_18373c26b0102xo4g.html\nhttps://blog.csdn.net/aaazz47/article/details/78696899\n","permalink":"https://sr-c.github.io/2019/05/11/manjaro-quickstart/","tags":["manjaro","arch","linux"],"title":"在manjaro上配置工作环境"},{"categories":null,"contents":"绘制热图的过程分为两个关键的步骤，第一，将数据聚类；第二，将数值矩阵转化为颜色图像。\n将数据聚类 聚类的方法有许多，其中最简单的方法就是分层聚集聚类(hierarchical, agglomerative cluster analysis).\n简单来说，该方法会首先计算矩阵中所有数据点之间的相互距离，然后将相聚最近的数据点连接起来，然后连接距离第二近的数据点，重复直至所有数据点都被聚类。我们得到的聚类树就是这一过程的展示。\n聚类过程中，有时需要将不同的类再次聚类，这个过程也有许多不同的方法，但是最常用的方式是计算两个类中所有点之间的平均距离。\n计算距离 距离代表着两个数据点之间的差异。这与相似性测量是正交的（相互独立的）。\n距离是如何计算的呢？在R的heatmap()或heatmap.2()函数的默认方法是使用dist()函数。dist()函数的默认方式是欧几里得距离。这一方法测量的是数据向量之间的绝对距离，但是却与其表达模式( the “shape” of the “curve”)无关。\n例如，测定4个基因在8个时间点的表达量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  h1 \u0026lt;- c(10,20,10,20,10,20,10,20) h2 \u0026lt;- c(20,10,20,10,20,10,20,10) l1 \u0026lt;- c(1,3,1,3,1,3,1,3) l2 \u0026lt;- c(3,1,3,1,3,1,3,1) mat \u0026lt;- rbind(h1,h2,l1,l2) par(mar=c(4,4,1,1)) plot(1:8,rep(0,8), ylim=c(0,35), pch=\u0026#34;\u0026#34;, xlab=\u0026#34;Time\u0026#34;, ylab=\u0026#34;Gene Expression\u0026#34;) for (i in 1:nrow(mat)) { lines(1:8,mat[i,], lwd=3, col=i) } legend(1,35,rownames(mat), 1:4, cex=0.7)   可以看到，图中表达的基因与两个低表达的基因。关键的是，这两对基因的表达模式完全相反。\n如果我们将这个数据置于dist()函数中，可得到这样的距离矩阵\n1  dist(mat)   1 2 3 4  h1 h2 l1 h2 28.284271 l1 38.470768 40.496913 l2 40.496913 38.470768 5.656854   距离矩阵中最小的值是l1与l2之间的距离(5.65)，因此这两者被聚为一类；然后，次小值是h1与h2之间的距离(28.28)，因此，这两者接下来被聚类；最后，这两个类连接起来。这就产生了一个基于距离矩阵的简单聚类分析。\n1 2  hc \u0026lt;- hclust(dist(mat)) plot(hc)   简单的热图（列水平上不聚类，保持时间点的顺序）\n1 2  library(gplots) heatmap(mat, Colv=NA, col=greenred(10))   聚类过程按照距离进行，l1与l2是最相似的，因此它们被聚为一类；然后，h1与h2被聚类。但是，由于颜色不合适，热图看起来很糟糕。尽管l1与l2被聚为一类，它们的颜色却不是同一个模式；h1与h2也是如此。此外，l1与h1有相同的颜色，尽管它们的表达量相差很大。\n数据缩放(scaling) l2与h2有着相同的颜色，但实际上它们的表达量不同。这是因为它们的表达量被缩放了，然后再转化为热图中的颜色。heatmap()与heatmap.2()中的默认设置都是按照行缩放(scale by row). 数据矩阵按行依次使用scale()函数进行缩放，然后转换为颜色。\n关闭缩放后，我们可以看到热图通过欧几里得距离聚类后的结果。\n1  heatmap(mat, Colv=NA, col=greenred(10), scale=\u0026#34;none\u0026#34;)   这样看起来结果好了一些，但是还不完善。l1与l2都是绿色，而h1与h2都是红色/黑色。\n一般来说，热图中的绿色代表较低的数值；红色代表较高的数值；黑色则代表中间值。在不进行缩放的时候，l1与l2都是较低的值，因此，它们是绿色；h1与h2中的较大值是较高的值，因此为红色；其中较低的值则是中间值，因此为黑色。\n问题所在 通常情况下，我们系统将有相近表达模式的基因聚类在一起。而有些基因的表达量高，有些基因的表达量低。将所有高表达的基因标为红色，低表达的基因表为绿色，而不考虑随时间表达模式的变化。有些时候，我们希望这样，但更多时候，我们系统将相同表达模式的基因聚类在一起。\n因此，heatmap()和heatmap.2()的默认参数结果对我们不可用。它们默认都缩放了数据，这在所有数据点都有着相近表达模式时很有用，但是它们还是用了欧几里得距离，这在我们系统通过表达模式聚类时并不适用。\n我们知道h1与l1有相近的表达模式(similar shape)，h2与l2也有相近的表达模式。但是，dist()函数只考虑绝对距离，并不考虑其表达模式。\n解决方案 使用另一种距离计算方式，相似性计算，皮尔逊相关系数(the pearson correlation co-efficient)。简单来说，它产生一个在-1到1之间的值；1代表两者相同；-1则表达两者正好相对。\n其计算结果如下：\n1  cor(t(mat))   1 2 3 4 5  h1 h2 l1 l2 h1 1 -1 1 -1 h2 -1 1 -1 1 l1 1 -1 1 -1 l2 -1 1 -1 1   结果中，h1与h2的相关系数为-1, 因此它们非常不相似，而h1与l1，h2与l2则完全正相关。\n在聚类过程中，我们需要的时距离矩阵，不是相似性矩阵。因此，我们需要用1减去相似性，转换得到：\n1  1 - cor(t(mat))   1 2 3 4 5  h1 h2 l1 l2 h1 0 2 0 2 h2 2 0 2 0 l1 0 2 0 2 l2 2 0 2 0   这样h1与l1之间的距离为0；h1与h2之间的距离为2. 那么我们可以得到简单的聚类结果：\n1 2  hc \u0026lt;- hclust(as.dist(1-cor(t(mat)))) plot(hc)   简单的热图结果\n1  heatmap(mat, Rowv=as.dendrogram(hc), Colv=NA, col=greenred(10))   如此，相同表达模式的基因被聚类在一起，也具有相同的颜色。\n热图实例 来自ARK-Genomic的实例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  library(gplots) read the data in from URL bots \u0026lt;- read.table(url(\u0026#34;http://genome-www.stanford.edu/cellcycle/data/rawdata/combined.txt\u0026#34;), sep=\u0026#34;t\u0026#34;, header=TRUE) # get just the alpha data abot \u0026lt;- bots[,c(8:25)] rownames(abot) \u0026lt;- bots[,1] abot[1:7,] # get rid of NAs abot[is.na(abot)] \u0026lt;- 0 # we need to find a way of reducing the data. # Sort on max difference and take first 1000 min \u0026lt;-apply(abot, 1, min) max \u0026lt;- apply(abot, 1, max) sabot \u0026lt;- abot[order(max - min, decreasing=TRUE),][1:1000,] # cluster on correlation hc \u0026lt;- hclust(as.dist(1 - cor(t(sabot))), method=\u0026#34;average\u0026#34;) # draw a heatmap heatmap(as.matrix(sabot), Rowv=as.dendrogram(hc), Colv=NA, col=greenred(10), labRow=\u0026#34;\u0026#34;)   \n因此，热图绘制还是一个比较复杂的可视化过程。\n无量纲化 极差法  (观测值-最小值)/极差\n 将数值缩放到0至1之间，又称为归一化(normalization)\nlog函数标准化  log10(X)/log10(MAX)\n 适用于观测值大于等于1的情况，这也是一种特殊的归一化。\nz-score  (观测值-均值)/标准差\n 将数值转化为均值为0，方差为1的标准正态分布的形式，又称作标准化(standrdization)\n参考来源 http://www.opiniomics.org/you-probably-dont-understand-heatmaps/\nhttps://sebastianraschka.com/Articles/2014_about_feature_scaling.html\nhttps://mp.weixin.qq.com/s?__biz=MzU4NjU4ODQ2MQ==\u0026amp;mid=2247485966\u0026amp;idx=1\u0026amp;sn=7c7cfcc79eba37dd377a31dd30e16e3b\u0026amp;chksm=fdf8424cca8fcb5af1d10b141f92ee545d737483b58fd0a08b45ae2802947db4aaf4f3660092\n","permalink":"https://sr-c.github.io/2019/04/30/how-heatmaps-work/","tags":["heatmap","R"],"title":"如何绘制一张热图"},{"categories":null,"contents":"安装 1 2 3 4  if (!requireNamespace(\u0026#34;BiocManager\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;BiocManager\u0026#34;) BiocManager::install(\u0026#34;treeio\u0026#34;, version = \u0026#34;3.8\u0026#34;) BiocManager::install(\u0026#34;ggtree\u0026#34;, version = \u0026#34;3.8\u0026#34;)   输入 ggtree可以直接文本输入Newick tree\n1 2 3 4  library(ggtree) tree_text \u0026lt;- \u0026#34;(((((cow, (whale, dolphin)), (pig2, boar)), camel), fish), seedling);\u0026#34; x \u0026lt;- read.tree(text=tree_text)   也可使用treeio直接入其他程序的标准输出\n1 2  library(treeio) read.r8s()   注意，read.r8s()一次性读入r8s 标准输出中的3种树，\n参考来源 https://yulab-smu.github.io/treedata-book/\n","permalink":"https://sr-c.github.io/2019/04/21/ggtree-quickstart/","tags":["ggtree"],"title":"ggtree-quickstart"},{"categories":null,"contents":"geom_text geom_text是直接向ggplot对象中添加绘制的文字\ntheme 在theme中以element_text()设置字体\n element_text(family = NULL**,** face = NULL**,** colour = NULL**,** size = NULL**,** hjust = NULL**,** vjust = NULL**,** angle = NULL**,** lineheight = NULL)\n 其中，title设置图表的标题，坐标轴标题以及图例的文本样式。axis.text则对应坐标轴刻度标尺的字体。\ntheme_set 使用theme_set函数可全局设置特定主题中的字体大小\n1 2 3 4 5 6 7 8  # 全局设置某个主题的默认字体大小 theme_set(theme_gray(base_size = 18)) qplot(1:10, 1:10) # 或直接在主题中设置base_size qplot(1:10, 1:10) + theme_grey(base_size = 18) ggplot(mtcars, aes(x = mpg, y = cyl)) + geom_point() + theme_grey(base_size = 18)   yyplot::set_font 使用Y叔提供yyplot中提供的set_font函数可以在最后同一设置文字。（对于geom_text, 或theme中的axis.text, text均生效）\n参考来源 https://www.cnblogs.com/wkslearner/p/5701207.html\nhttps://stackoverflow.com/questions/25061822/ggplot-geom-text-font-size-control\nhttps://cloud.tencent.com/developer/ask/65053\nhttps://guangchuangyu.github.io/cn/2017/09/ggplot2-set-font/\nhttps://cran.r-project.org/web/packages/svglite/vignettes/fonts.html\n","permalink":"https://sr-c.github.io/2019/04/19/R-set-font/","tags":["R","ggplot","font"],"title":"ggplot的字体设置"},{"categories":null,"contents":"问题描述 在R中读入外部数据时，得到warning\n1 2 3  Warning message: In scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings, : EOF within quoted string   这可能是由于数据表格中存在引号\u0026quot;或', 而默认设置读入数据的read.table()函数的quote =\u0026quot;\\\u0026quot;'\u0026quot; \n解决方案 在读入数据时，通过quote = \u0026quot;\u0026quot;禁用引用\n1 2 3  cit \u0026lt;- read.csv(\u0026#34;citations.CSV\u0026#34;, quote = \u0026#34;\u0026#34;, row.names = NULL, stringsAsFactors = FALSE)   参考来源 https://codeday.me/bug/20170630/34178.html\nhttps://www.rdocumentation.org/packages/utils/versions/3.5.3/topics/read.table\n","permalink":"https://sr-c.github.io/2019/03/31/debug-EOF-within-quoted-string/","tags":["debug","R"],"title":"【debug】EOF within quoted string"},{"categories":null,"contents":"安装 1 2 3  git clone https://github.com/DRL/kinfin.git cd kinfin ./install   安装程序会下载一些数据，并使用pip安装相应的模块。但若是没有管理员权限，还是使用conda来配置环境比较方便。\n软件的环境要求python模块如下\n scipy==0.19.0 matplotlib==2.0.2 docopt==0.6.2 networkx==1.11 powerlaw==1.4.1 ete3==3.0.0b35\n 1 2 3 4 5 6 7 8  conda create -n kinfin python=2.7 source activate kinfin #matplotlib 2.0.2 与ete3 3.0.0b35的conda环境依赖存在冲突，故直接尝试安装最新版 conda install scipy==0.19.0 matplotlib docopt==0.6.2 networkx==1.11 ete3 #powerlaw目前只能从第三方源安装 conda install -c mlgill powerlaw ./kinfin --help   出现帮助信息表明软件安装完成\n此外，还可以安装一些其他软件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  conda install parallel cd .. wget https://github.com/gephi/gephi/releases/download/v0.9.2/gephi-0.9.2-linux.tar.gz tar zxf gephi-0.9.2-linux.tar.gz # 使用trimAl截短多序列比对后的序列 # http://trimal.cgenomics.org/downloads wget https://github.com/scapella/trimal/archive/v1.4.1.tar.gz -O trimAl_v1.4.1.tar.gz tar zxf trimAl_v1.4.1.tar.gz cd trimal-1.4.1/source make git clone https://github.com/PatrickKueck/FASconCAT.git   测试 KinFin的软件路径中提供了测试数据./test，但我们还可以使用教程中的数据来练习。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86  # $KINFIN/: path to KinFin installation # $MAFFT/: path to MAFFT installation # $TRIMAL/: path to trimal installation # $FASCONCAT/: path to FASconCAT installation # $RAXML/: path to RAxML installation # 1. Clone KinFin manuscript repo git clone https://github.com/DRL/kinfin_manuscript cd kinfin_manuscript/ # 2. Basic analysis tar zxf supplementary_material/supplementary_data_1.tar.gz $KINFIN/kinfin \\ \t-g supplementary_data_1/Orthogroups.I1_5.txt \\  -c supplementary_data_1/kinfin.config.basic.txt \\  -s supplementary_data_1/kinfin.SequenceIDs.txt \\  -o kinfin.basic # 2.1 Get single-copy cluster IDs grep \u0026#39;true\u0026#39; kinfin.basic.kinfin_results/all/all.all.cluster_1to1s.txt | \\ \tcut -f1 \u0026gt; single_copy.cluster_ids.txt # 2.2 Get protein IDs from single-copy cluster IDs $KINFIN/scripts/get_protein_ids_from_cluster.py \\ \t-g supplementary_data_1/Orthogroups.I1_5.txt \\  --cluster_ids single_copy.cluster_ids.txt # 2.3 Extract FASTA sequences based on protein IDs tar zxf supplementary_material/supplementary_data_2.tar.gz cat supplementary_data_2/fastas/*.faa \u0026gt; all.proteins.faa parallel -j8 \u0026#39;\\ grep --no-group-separator -A1 -wFf {} all.proteins.faa \u0026gt; {/.}.faa\\ \u0026#39; ::: Orthogroups.I1_5.OG*.txt # 2.4 Align FASTA sequences parallel -j 8 \u0026#39;\\ $MAFFT/bin/einsi {} \u0026gt; {/.}.einsi.faa\u0026#39; ::: Orthogroup*.faa # 2.5 Trim alignments parallel -j 8 \u0026#39;\\ $TRIMAL/source/trimal -in {} -out {/.}.trimal.faa -automated1 \\ \u0026#39; ::: *einsi.faa ## ERROR: Alignment not loaded: \u0026#34;Orthogroup.I1_5.OG*.einsi.faa\u0026#34; Check the file\u0026#39;s content. # 2.6 Sanitise headers for FASconCAT parallel -j 8 \u0026#39;\\ cut -f1 -d\u0026#34;.\u0026#34; {} \u0026gt; {/.}.sane.fas \\ \u0026#39; ::: *trimal.faa # 2.7 Concatenate alignments $FASCONCAT/FASconCAT_v1.11.pl -s -p -n # 2.8 RAxML 注意耗时较长，推荐后台运行 $RAXML/raxmlHPC-PTHREADS-SSE3 -s FcC_smatrix.phy -m PROTGAMMAGTR -T 32 -n ml -N 20 -p 19 $RAXML/raxmlHPC-PTHREADS-SSE3 -m PROTGAMMAGTR -T 48 -n bs -p 19 -b 19 -# 100 -s FcC_smatrix.phy $RAXML/raxmlHPC-PTHREADS-SSE3 -m GTRCAT -p 19 -f b -t RAxML_bestTree.ml -z RAxML_bootstrap.bs -n final # 3. Advanced analysis (assuming KinFin executable is in your $PATH) $KINFIN/kinfin \\ \t-g supplementary_data_2/Orthogroups.I1_5.txt \\  -c supplementary_data_2/kinfin.config.tree.txt \\  -s supplementary_data_2/kinfin.SequenceIDs.txt \\  -o kinfin.advanced \\  -p supplementary_data_2/kinfin.SpeciesIDs.txt \\  -a supplementary_data_2/fastas/ \\  -t supplementary_data_2/kinfin.tree.nwk \\  -f supplementary_data_2/kinfin.functional_annotation.txt # 4. Infer representative functional annotation (all clusters) $KINFIN/scripts/filter_functional_annotation_of_clusters.py all \\ \t-f kinfin.advanced.kinfin_results/cluster_metrics_domains.IPR.txt \\  -c kinfin.advanced.kinfin_results/cluster_counts_by_taxon.txt \\  -x 0.75 \\  -p 0.75 \\  -o kinfin.IPR # 4. Infer representative functional annotation (synapomorphic clusters) $KINFIN/scripts/filter_functional_annotation_of_clusters.py synapo \\ \t-f kinfin.advanced.kinfin_results/cluster_metrics_domains.IPR.txt \\  -c kinfin.advanced.kinfin_results/cluster_counts_by_taxon.txt \\  -t kinfin.advanced.kinfin_results/tree/tree.cluster_metrics.txt \\  -n 0.75 \\  -x 0.75 \\  -p 0.75 \\  -o kinfin.IPR   进阶使用 综合功能注释信息。\n根据其提供的说明书KinFin_v1.supplementary_methods.pdf。综合功能注释的方法如下：\n功能注释\n 短于30 个氨基酸或其中还有提前终止密码子的序列被过滤filter_fastas_before_clustering.py 非最长的序列被过filter_isoforms_based_on_gff3.py 所有序列通过InterProScan v5.22-61.0的Pfam-30.0与 SignalP-EUK-4.1注释，并通过iprs_to_table.py转化为可识别的格式。  参考来源 https://kinfin.readme.io/docs/running-the-example-dataset\n","permalink":"https://sr-c.github.io/2019/03/23/KinFin-quickstart/","tags":["KinFin"],"title":"使用KinFin进行比较基因组分析"},{"categories":null,"contents":"系统依赖 LACHESIS的系统依赖有\n gcc, the C++ compiler (http://gcc.gnu.org/) The zlib compression library (http://www.zlib.net/) The boost C++ libraries (http://www.boost.org/) The SAMtools toolkit for handling SAM/BAM files (http://samtools.sourceforge.net/) (make sure to use version 0.1.19 or older)  其中，boost的版本应该高于1.52.0，低于1.67.0，samtools的版本不高于0.1.19\n此外，LACHESIS需要系统的堆栈大小最小为10 MB. 可使用ulimit -s查看，若小于10240，可使用ulimit -s 10240设定。\n编译安装boost 1 2 3 4 5  tar zxf boost_1_53_0.tar.gz ./bootstrap.sh --with-libraries=all --with-toolset=gcc ./b2 toolset=gcc #此步骤过程较长 ./b2 install --prefix=/path/to/istall/boost_1_53_0 ./bjam install --prefix=/path/to/istall/boost_1_53_0   此过程中会得到许多failed，error如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ...failed gcc.compile.c++ bin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi/cpplexer/re2clex/cpp_re.o... ...skipped \u0026lt;pbin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;libboost_wave.a(clean) for lack of \u0026lt;pbin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;cpplexer/re2clex/cpp_re.o... ...skipped \u0026lt;pbin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;libboost_wave.a for lack of \u0026lt;pbin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;cpplexer/re2clex/cpp_re.o... ...skipped \u0026lt;pstage/lib\u0026gt;libboost_wave.a for lack of \u0026lt;pbin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;libboost_wave.a... ...failed updating 88 targets... ...skipped 78 targets... ...updated 802 targets... ...failed gcc.compile.c++ bin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi/cpplexer/re2clex/cpp_re.o... ...skipped \u0026lt;pbin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;libboost_wave.a(clean) for lack of \u0026lt;pbin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;cpplexer/re2clex/cpp_re.o... ...skipped \u0026lt;pbin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;libboost_wave.a for lack of \u0026lt;pbin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;cpplexer/re2clex/cpp_re.o... ...skipped \u0026lt;p/Storage/data002/shurh/opt/boost_1_53_0/lib\u0026gt;libboost_wave.a for lack of \u0026lt;pbin.v2/libs/wave/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;libboost_wave.a... ...failed updating 88 targets... ...skipped 78 targets... ...updated 10108 targets...   编译安装samtools 1  ./Makefile   编译LACHESIS 1 2 3  export LACHESIS_BOOST_DIR=/path/to/boost_1_53_0 export LACHESIS_SAMTOOLS_DIR=/path/to/samtools-0.1.19 ./configure --with-samtools=/path/to/samtools-0.1.19 --with-boost=/path/to/boost_1_53_0/   报错\n1 2 3 4 5 6 7 8 9  checking for Boost headers version \u0026gt;= 1.52.0... /Storage/data002/shurh/opt/boost_1_53_0//include checking for Boost\u0026#39;s header version... 1_53 checking for the toolset name used by Boost for g++... configure: WARNING: could not figure out which toolset name to use for g++ checking boost/system/error_code.hpp usability... yes checking boost/system/error_code.hpp presence... yes checking for boost/system/error_code.hpp... yes checking for the Boost system library... no configure: error: cannot find the flags to link with Boost system   提示boost缺少system库文件，查看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ ls ~/opt/boost_1_53_0/lib/ libboost_chrono.a libboost_math_c99f.so.1.53.0 libboost_prg_exec_monitor.a libboost_context.a libboost_math_c99l.a libboost_prg_exec_monitor.so libboost_context.so libboost_math_c99l.so libboost_prg_exec_monitor.so.1.53.0 libboost_context.so.1.53.0 libboost_math_c99l.so.1.53.0 libboost_python.a libboost_date_time.a libboost_math_c99.so libboost_python.so libboost_date_time.so libboost_math_c99.so.1.53.0 libboost_python.so.1.53.0 libboost_date_time.so.1.53.0 libboost_math_tr1.a libboost_random.a libboost_exception.a libboost_math_tr1f.a libboost_random.so libboost_graph.a libboost_math_tr1f.so libboost_random.so.1.53.0 libboost_iostreams.a libboost_math_tr1f.so.1.53.0 libboost_signals.a libboost_iostreams.so libboost_math_tr1l.a libboost_signals.so libboost_iostreams.so.1.53.0 libboost_math_tr1l.so libboost_signals.so.1.53.0 libboost_math_c99.a libboost_math_tr1l.so.1.53.0 libboost_timer.a libboost_math_c99f.a libboost_math_tr1.so libboost_math_c99f.so libboost_math_tr1.so.1.53.0   确实缺少libboost_system.so, 而这在./bjam install过程中的报错信息中存在\n1 2 3 4  ...failed gcc.compile.c++ bin.v2/libs/system/build/gcc-7.3.0/release/link-static/threading-multi/error_code.o... ...skipped \u0026lt;pbin.v2/libs/system/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;libboost_system.a(clean) for lack of \u0026lt;pbin.v2/libs/system/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;error_code.o... ...skipped \u0026lt;pbin.v2/libs/system/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;libboost_system.a for lack of \u0026lt;pbin.v2/libs/system/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;error_code.o... ...skipped \u0026lt;p/usr/local/lib\u0026gt;libboost_system.a for lack of \u0026lt;pbin.v2/libs/system/build/gcc-7.3.0/release/link-static/threading-multi\u0026gt;libboost_system.a...   说明在编译过程中，libboost_system由于某些错误被滤过了，没有被编译。\n自行使用conda配置boost环境 1 2 3 4 5 6  conda create -n LACHESIS python=2.7 source activate LACHESIS conda install boost=1.57 -y export LACHESIS_BOOST_DIR=/path/miniconda/envs/LACHESIS export LACHESIS_SAMTOOLS_DIR=/path/to/samtools-0.1.19 ./configure --with-samtools=/path/to/samtools-0.1.19 --with-boost=/path/to/boost_1_53_0/   Boost system library通过，但得到后续报错\n1 2 3  checking for boost/program_options.hpp... yes checking for the Boost program_options library... no configure: error: cannot find the flags to link with Boost program_options   但是/path/miniconda/envs/LACHESIS/lib中是存在libboost_program_options.so的\n查看./configure中的deflate\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  for ac_header in zlib.h do : ac_fn_cxx_check_header_mongrel \u0026#34;$LINENO\u0026#34; \u0026#34;zlib.h\u0026#34; \u0026#34;ac_cv_header_zlib_h\u0026#34; \u0026#34;$ac_include if test \u0026#34;x$ac_cv_header_zlib_h\u0026#34; = xyes; then : cat \u0026gt;\u0026gt;confdefs.h \u0026lt;\u0026lt;_ACEOF #define HAVE_ZLIB_H 1 _ACEOF else as_fn_error $? \u0026#34;libz is required.\u0026#34; \u0026#34;$LINENO\u0026#34; 5 fi done { $as_echo \u0026#34;$as_me:${as_lineno-$LINENO}: checking for library containing deflate\u0026#34; \u0026gt;\u0026amp;5 $as_echo_n \u0026#34;checking for library containing deflate... \u0026#34; \u0026gt;\u0026amp;6; } if ${ac_cv_search_deflate+:} false; then : $as_echo_n \u0026#34;(cached) \u0026#34; \u0026gt;\u0026amp;6 else ac_func_search_save_LIBS=$LIBS cat confdefs.h - \u0026lt;\u0026lt;_ACEOF \u0026gt;conftest.$ac_ext /* end confdefs.h. */   zlib错误 https://daler.github.io/bioconda-docs/troubleshooting.html#zlib-errors\n有时即使已经安装了zlib, 但在编译程序时仍然报错缺少zlib.h, 这常常时因为zlib的位置常常在编译文件中定义了，\n1  unset LIBRARY_PATH CPATH C_INCLUDE_PATH PKG_CONFIG_PATH CPLUS_INCLUDE_PATH INCLUDE   手动修改src/include/gtools文件夹中SAMStepper.h和SAMStepper.cc的samtools头文件\n参考来源 https://github.com/shendurelab/LACHESIS/issues/30#issuecomment-299240018\nhttps://github.com/shendurelab/LACHESIS/issues/23\nhttps://blog.csdn.net/yzf279533105/article/details/77658367\nhttps://daler.github.io/bioconda-docs/troubleshooting.html#zlib-errors\nhttps://www.tecmint.com/set-unset-environment-variables-in-linux/\n","permalink":"https://sr-c.github.io/2019/03/19/LACHESIS-install/","tags":["LACHESIS","Hi-C"],"title":"LACHESIS安装"},{"categories":null,"contents":"用户目录 R的用户目录在Windows系统下默认位于当前用户的Documents文件夹\n1 2  \u0026gt; normalizePath(\u0026#39;~\u0026#39;) [1] \u0026#34;C:\\\\Users\\\\yourUserName\\\\Documents\u0026#34;    注意，若你使用了Onedrive，并且将文档文件夹加入了自动保护，那么当前用户的Documents文件夹将会被导向至OneDrive文件夹，导致莫名其妙的错误。建议将该功能关闭。\n 配置文件 .Renviron 该文件设置R自身的一些环境变量，仅对R有效，不改变操作系统的设置。\n1  file.edit(\u0026#39;~/.Renviron\u0026#39;)   推荐在此设置R_LIBS_USER，指定R的附加包安装目录。\n1  R_LIBS_USER=\u0026#34;~/R\u0026#34;   若该目录不存在，需要自行手动创建。\n1  dir.creat(\u0026#39;~/R\u0026#39;)   .Rprofile 该文件其实是一个R代码文件，在R启动时，该文件会被首先执行。因此，我们可以将一些个人选项写在其中。\n1  file.edit(\u0026#39;~/.Rprofile\u0026#39;)   使用options()函数设置R运行时的一些选项，如CRAN的镜像地址\n1 2  options(\u0026#34;repos\u0026#34; = c(CRAN=\u0026#34;https://mirrors.tuna.tsinghua.edu.cn/CRAN/\u0026#34;, CRANextra = \u0026#34;http://www.stats.ox.ac.uk/pub/RWin\u0026#34;))   PATH环境变量  “我的电脑”（右键）–\u0026gt;“属性”–\u0026gt;“高级”–\u0026gt;“环境变量”–\u0026gt;“系统变量”–\u0026gt;PATH\n 其实R在安装的时候，默认会将配置写入注册表\nRconsole 该文件是R在Windows下独有的一个配置文件，可以直接使用文本编辑器编辑。其中配置了Windows下R界面的属性，如界面语言或字体等。\n1  file.path(R.home(\u0026#39;etc\u0026#39;), \u0026#39;Rconsole\u0026#39;)   1  language = en # 修改界面语言为英文   升级方式 Windows下R的升级方式很不灵活，官方推荐的方式竟然是重新安装，手动复制附加包到新的library文件夹。\n实测我们可以先卸载旧版本的R，卸载程序会保留library文件夹。然后再安装新版本R，手动指定安装目录至原路径。最后再新版本的R中运行\n1  update.packages(checkBuilt=TRUE, ask=FALSE)   由于之前的配置文件如.Renviron, .Rprofile等存在于用户目录下，升级过程中不受影响。\n参考来源 https://bookdown.org/yihui/r-ninja/setup.html#configure-r\n","permalink":"https://sr-c.github.io/2019/03/16/R-setup/","tags":["R"],"title":"Windows中R的配置"},{"categories":null,"contents":"bootstraping 通过bootstrap来计算置信区间，帮助我们更好地理解置信区间。\n假设我们需要测量雌性小鼠的体重，对总体抽样12次，测得12个样本。\nSampling with replacement: bootstrap的过程就是重复对这12个样本抽样。\n重复这一抽样过程许多次，如10000次，每次都计算抽样后的均值。\n那么95% 置信区间覆盖就是这10000个均值中95%的取值范围。\nConfidence interval 的意义 这是一个可视化的统计值，在置信区间外的取值就相对不可信。\n假设这12个样本均值的置信区间为(21, 31)，那么所有雌鼠体重的“真实”均值小于20 的p-value一定小于0.05，这不太可能。或者说，雌鼠体重的均值与小于20的数值之间存在显著差异。\n This is unlikeyly and beacuase fo this, we can say there is a statistically significant difference between the true mean and any value less than 20.\n Compare two samples 如果两批样本均值的置信区间没有重叠，那么这两组样品之间一定存在显著差异。\nOne Caveat 如果两组样本均值的置信区间存在重叠，它们也可能存在显著差异。只是这时，我们必须做t-检验来判定了。\n参考来源 ","permalink":"https://sr-c.github.io/2019/03/16/StatQuest-confidence-intervals/","tags":["StatQuest"],"title":"【StatQuest】置信区间"},{"categories":null,"contents":"Q1: 参考基因组及注释文件下载地址 列出人，小鼠，拟南芥的基因组序列，转录组cDNA序列，基因组注释gtf文件下载地址\n Human (GRCh38.p12)\nftp://ftp.ensembl.org/pub/release-95/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.toplevel.fa.gz\nftp://ftp.ensembl.org/pub/release-95/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz\nftp://ftp.ensembl.org/pub/release-95/gtf/homo_sapiens/Homo_sapiens.GRCh38.95.gtf.gz\nMouse (GRCm38.p6)\nftp://ftp.ensembl.org/pub/release-95/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.toplevel.fa.gz\nftp://ftp.ensembl.org/pub/release-95/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz\nftp://ftp.ensembl.org/pub/release-95/gtf/mus_musculus/Mus_musculus.GRCm38.95.gtf.gz\nArabidopsis thaliana (TAIR10)\nftp://ftp.ensemblgenomes.org/pub/plants/release-42/fasta/arabidopsis_thaliana/dna/Arabidopsis_thaliana.TAIR10.dna.toplevel.fa.gz\nftp://ftp.ensemblgenomes.org/pub/plants/release-42/fasta/arabidopsis_thaliana/cdna/Arabidopsis_thaliana.TAIR10.cdna.all.fa.gz\nftp://ftp.ensemblgenomes.org/pub/plants/release-42/gtf/arabidopsis_thaliana/Arabidopsis_thaliana.TAIR10.42.gtf.gz\n Q2: 找到文章的测序数据 2018年12月的NC文章：Spatially and functionally distinct subclasses of breast cancer-associated fibroblasts revealed by single cell RNA sequencing 使用成熟的单细胞转录组( Smart-seq2 )手段探索了癌相关的成纤维细胞 CAFs的功能和空间异质性。\n1  # 根据Data availability，数据位于GEO, GSE111229   Q3：下载测序数据 主要是理解GEO链接： GSE111229 和原始测序数据：SRP133642 两个链接\n1 2 3  library(GEOquery) gset = getGEO(\u0026#39;GSE111229\u0026#39;) exprSet = exprs(gset[[1]])   1 2 3  # 直接下载 wget -c ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE111nnn/GSE111229/matrix/GSE111229_series_matrix.txt.gz for i in `cat SraAccList.txt`; do prefetch $i; done   参考来源 ","permalink":"https://sr-c.github.io/2019/03/11/RNA-seq-exercise/","tags":["RNA-seq","生信技能树","biotrainee"],"title":"RNA-seq小考核"},{"categories":null,"contents":"http://www.bio-info-trainee.com/2900.html\n 在任意文件夹下面创建形如 1/2/3/4/5/6/7/8/9 格式的文件夹系列。  1  mkdir -p 1/2/3/4/5/6/7/8/9   在创建好的文件夹下面，比如我的是 /Users/jimmy/tmp/1/2/3/4/5/6/7/8/9 ，里面创建文本文件 me.txt  1  touch me.txt   在文本文件 me.txt 里面输入内容:  1 2 3 4 5  cat \u0026gt; me.txt Go to: http://www.biotrainee.com/ I love bioinfomatics. And you ? #Ctrl + D   删除上面创建的文件夹 1/2/3/4/5/6/7/8/9 及文本文件 me.txt  1 2  cd ~/tmp/ rm -rf tmp/   在任意文件夹下面创建 folder1~5这5个文件夹，然后每个文件夹下面继续创建 folder1~5这5个文件夹  1 2 3 4 5  for m in {1..5}; do for n in {1..5}; do mkdir -p folder${m}/folder${n} done done   在第五题创建的每一个文件夹下面都 创建第二题文本文件 me.txt ，内容也要一样。  1 2 3 4 5 6 7 8 9 10  cat \u0026gt; me.txt Go to: http://www.biotrainee.com/ I love bioinfomatics. And you ? #Ctrl + D for m in {1..5}; do for n in {1..5}; do cp me.txt folder${m}/folder${n} done done   再次删除掉前面几个步骤建立的文件夹及文件  1  rm -rf ~/temp   下载 http://www.biotrainee.com/jmzeng/igv/test.bed 文件，后在里面选择含有 H3K4me3 的那一行是第几行，该文件总共有几行。  1 2 3 4 5  wget http://www.biotrainee.com/jmzeng/igv/test.bed # 结果为第8行 grep -n H3K4me3 test.bed # 结果为10行 wc -l test.bed   下载 http://www.biotrainee.com/jmzeng/rmDuplicate.zip 文件，并且解压，查看里面的文件夹结构  1 2 3  wget http://www.biotrainee.com/jmzeng/rmDuplicate.zip unzip rmDuplicate.zip tree rmDuplicate   打开第九题解压的文件，进入 rmDuplicate/samtools/single 文件夹里面，查看后缀为 .sam 的文件，搞清楚 生物信息学里面的SAM/BAM 定义是什么  1 2  cd rmDuplicate/samtools/single less -S tmp.sam   安装 samtools 软件  1 2  wget https://github.com/samtools/samtools/releases/download/1.9/samtools-1.9.tar.bz2 tar Jxf samtools-1.9.tar.bz2   打开 后缀为BAM 的文件，找到产生该文件的命令。  1 2  samtools view tmp.rmdup.bam | less -S cat readme.txt   上面的后缀为BAM 的文件的第二列，只有 0 和 16 两个数字，用 cut/sort/uniq等命令统计它们的个数。  1 2 3  samtools view tmp.rmdup.bam | cut -f 2 | sort | uniq -c 16 0 12 16   重新打开 rmDuplicate/samtools/paired 文件夹下面的后缀为BAM 的文件，再次查看第二列，并且统计  1 2 3 4 5 6 7 8 9 10 11 12  cd ../paired samtools view tmp.rmdup.bam | cut -f 2| sort | uniq -uc 7 147 2 163 1 323 1 353 1 371 1 387 1 433 2 83 2 97 8 99   下载 http://www.biotrainee.com/jmzeng/sickle/sickle-results.zip 文件，并且解压，查看里面的文件夹结构， 这个文件有2.3M，注意留心下载时间及下载速度。  1 2 3  cd ~/temp wget http://www.biotrainee.com/jmzeng/sickle/sickle-results.zip unzip sickle-results.zip   解压 sickle-results/single_tmp_fastqc.zip 文件，并且进入解压后的文件夹，找到 fastqc_data.txt 文件，并且搜索该文本文件以 \u0026raquo;开头的有多少行？  1 2 3  cd sickle-results unzip single_tmp_fastqc.zip grep -c \u0026#39;\u0026gt;\u0026gt;\u0026#39; fastqc_data.txt   下载 http://www.biotrainee.com/jmzeng/tmp/hg38.tss 文件，去NCBI找到TP53/BRCA1等自己感兴趣的基因对应的 refseq数据库 ID，然后找到它们的hg38.tss 文件的哪一行。  1 2  wget http://www.biotrainee.com/jmzeng/tmp/hg38.tss grep NM_000546 hg38.tss   解析hg38.tss 文件，统计每条染色体的基因个数。  1  cut -f 2 hg38.tss | sort | uniq -c   解析hg38.tss 文件，统计NM和NR开头的熟练，了解NM和NR开头的含义。  1 2 3 4 5 6  cut -f 1 hg38.tss | grep -c \u0026#39;NM\u0026#39; 51064 cut -f 1 hg38.tss | grep -c \u0026#39;NR\u0026#39; 15954 # https://en.wikipedia.org/wiki/RefSeq # NM, mRNA. NR, ncRNA   参考来源 ","permalink":"https://sr-c.github.io/2019/03/11/linux-exercise/","tags":["linux","生信技能树","biotrainee"],"title":"linux练习题"},{"categories":null,"contents":"初级 http://www.bio-info-trainee.com/3793.html\n1. 打开 Rstudio 告诉我它的工作目录。 1  getwd()   2. 新建6个向量，基于不同的原子类型。（重点是字符串，数值，逻辑值） 1 2 3 4 5 6 7 8 9 10 11 12  # Character v_char \u0026lt;- c(\u0026#34;www\u0026#34;,\u0026#34;biotrainee\u0026#34;,\u0026#34;com\u0026#34;) # Numeric v_num \u0026lt;- seq(1,10) # Logical v_logical \u0026lt;- c(TRUE, FALSE, FALSE) # Interger v_inter \u0026lt;- 2L # Complex v_complex \u0026lt;- 2+5i # Raw v_raw \u0026lt;- charToRaw(\u0026#34;Hello\u0026#34;)   3. 告诉我 getwd() 返回的是什么？  当前工作目录\n 4. 新建5个其它数据结构，矩阵，数组，数据框，列表，因子（重点是数据框，矩阵） 1 2 3 4 5 6 7 8 9 10 11 12 13 14  # Matrix matrix1 \u0026lt;- matrix(seq(1,10), nrow = 2, ncol = 5, byrow = TRUE) # Array array1 \u0026lt;- array(c(1,0), dim = c(3, 3, 2)) # data frame data_frame1 \u0026lt;- data.frame( gender = c(\u0026#34;male\u0026#34;, \u0026#34;female\u0026#34;, \u0026#34;male\u0026#34;), height = c(130, 142, 137), age = c(7, 6, 6) ) # List list1 \u0026lt;- list(v_char, v_num, 1) # Factor factor1 \u0026lt;- factor(v_char)   5. 在你新建的数据框进行切片操作，比如首先取第1，3行， 然后取第4，6列 1 2  data_frame1[,1:3] data_frame1[4,6:]   6. 使用data函数来加载R内置数据集 rivers 描述它。并且可以查看更多的R语言内置的数据集： 1  data(rivers)   7. 下载 https://www.ncbi.nlm.nih.gov/sra?term=SRP133642 里面的 RunInfo Table 文件读入到R里面，了解这个数据框，多少列，每一列都是什么属性的元素。 1 2  a \u0026lt;- read.table(\u0026#34;SraRunTable.txt\u0026#34;, header = TRUE, sep = \u0026#39;\\t\u0026#39;) str(a)   8. 下载 https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE111229 里面的样本信息读入到R里面，了解这个数据框，多少列，每一列都是什么属性的元素。 1 2  b \u0026lt;- read.table(\u0026#34;GSE111229_series_matrix.txt.gz\u0026#34;, comment.char = \u0026#39;!\u0026#39;, sep = \u0026#39;\\t\u0026#39;, header = TRUE) str(b)   9. 把两个表关联起来，使用merge函数。 1  c \u0026lt;- merge(a, b, by = \u0026#34;Sample_Name\u0026#34;, all.x = TRUE)   10. 对前面读取的 RunInfo Table 文件在R里面探索其MBases列，包括 箱线图(boxplot)和五分位数(fivenum)，还有频数图(hist)，以及密度图(density) 。 1 2 3 4 5  \u0026gt; boxplot(sra_table$MBases) \u0026gt; fivenum(sra_table$MBases) [1] 0 8 12 16 74 \u0026gt; hist(sra_table$MBases) \u0026gt; density(sra_table$MBases)   11. 把前面读取的样本信息表格的样本名字根据下划线分割看第3列元素的统计情况。第三列代表该样本所在的plate 1 2 3 4  plate=unlist(lapply(e[,2],function(x){ x strsplit(x,\u0026#39;_\u0026#39;)[[1]][3] }))   中级 http://www.bio-info-trainee.com/3750.html\n1. 请根据R包org.Hs.eg.db找到下面ensembl 基因ID 对应的基因名(symbol)  ENSG00000000003.13 ENSG00000000005.5 ENSG00000000419.11 ENSG00000000457.12 ENSG00000000460.15 ENSG00000000938.11\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  library(org.Hs.eg.db) g2s=toTable(org.Hs.egSYMBOL) g2e=toTable(org.Hs.egENSEMBL) g2s_e \u0026lt;- merge(g2s, g2e, by = \u0026#39;gene_id\u0026#39;, all = TRUE) esbID \u0026lt;- c(\u0026#34;ENSG00000000003.13\u0026#34;, + \u0026#34;ENSG00000000005.5\u0026#34;, + \u0026#34;ENSG00000000419.11\u0026#34;, + \u0026#34;ENSG00000000457.12\u0026#34;, + \u0026#34;ENSG00000000460.15\u0026#34;, + \u0026#34;ENSG00000000938.11\u0026#34;) # 使用strsplit处理文本ID，取.前的部分 ID6 \u0026lt;- unlist(lapply(esbID, function(x){ strsplit(x,\u0026#39;.\u0026#39;,fixed = TRUE)[[1]][1] }) ) ID6 \u0026lt;- data.frame(ensembl_id = ID6) merge(ID6, g2s_e, by = \u0026#34;ensembl_id\u0026#34;, all.x = T)   2. 根据R包hgu133a.db找到下面探针对应的基因名(symbol) 1 2 3 4 5 6  rm(list = ls()) options(stringsAsFactors = F) library(hgu133a.db) ids=toTable(hgu133aSYMBOL) pr_id \u0026lt;- read.table(\u0026#34;R_test2.txt\u0026#34;, col.names = c(\u0026#34;probe_id\u0026#34;)) merge(pr_id, ids, by = \u0026#34;probe_id\u0026#34;, all.x = F)   3. 找到R包CLL内置的数据集的表达矩阵里面的TP53基因的表达量，并且绘制在 progres.-stable分组的boxplot图 1 2 3 4 5 6 7 8 9 10 11 12 13  rm(list = ls()) options(stringsAsFactors = F) suppressPackageStartupMessages(library(CLL)) data(sCLLex) exprSet=exprs(sCLLex) library(hgu95av2.db) columns(hgu95av2.db) TP53_prob \u0026lt;- select(hgu95av2.db, keys = c(\u0026#34;TP53\u0026#34;), columns = c(\u0026#34;ALIAS\u0026#34;,\u0026#34;PROBEID\u0026#34;), keytype = \u0026#34;ALIAS\u0026#34;) boxplot(t(exprSet[TP53_prob$PROBEID,])) boxplot(exprSet[1,]~gourp_list)   4. 找到BRCA1基因在TCGA数据库的乳腺癌数据集(Breast Invasive Carcinoma (TCGA, PanCancer Atlas))的表达情况 1 2 3 4 5  # http://www.cbioportal.org/index.do # Select Studies 选项中输入Breast Invasive Carcinoma PanCancer 搜索并选中目标数据集 # Enter Genes 数据框中输入目标基因BRCA1 # 点击Submit Query提交搜索请求 # 结果页面中，Plots选项卡选择合适数据进行绘图，或下载数据   5. 找到TP53基因在TCGA数据库的乳腺癌数据集的表达量分组看其是否影响生存  http://www.oncolnc.org/kaplan/?lower=50\u0026amp;upper=50\u0026amp;cancer=BRCA\u0026amp;gene_id=7157\u0026amp;raw=TP53\u0026amp;species=mRNA\n 6. 下载数据集GSE17215的表达矩阵并且提取下面的基因画热图  ACTR3B ANLN BAG1 BCL2 BIRC5 BLVRA CCNB1 CCNE1 CDC20 CDC6 CDCA1 CDH3 CENPF CEP55 CXXC5 EGFR ERBB2 ESR1 EXO1 FGFR4 FOXA1 FOXC1 GPR160 GRB7 KIF2C KNTC2 KRT14 KRT17 KRT5 MAPT MDM2 MELK MIA MKI67 MLPH MMP11 MYBL2 MYC NAT1 ORC6L PGR PHGDH PTTG1 RRM2 SFRP1 SLC39A6 TMEM45B TYMS UBE2C UBE2T\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # 下载链接ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE17nnn/GSE17215/matrix/GSE17215_series_matrix.txt.gz library(hthgu133a.db) expMa \u0026lt;- read.table(\u0026#34;zhong6/GSE17215_series_matrix.txt.gz\u0026#34;, comment.char = \u0026#34;!\u0026#34;, sep = \u0026#34;\\t\u0026#34;, header = TRUE, row.names = \u0026#34;ID_REF\u0026#34;) ALIAS \u0026lt;- \u0026#34;ACTR3B ANLN BAG1 BCL2 BIRC5 BLVRA CCNB1 CCNE1 CDC20 CDC6 CDCA1 CDH3 CENPF CEP55 CXXC5 EGFR ERBB2 ESR1 EXO1 FGFR4 FOXA1 FOXC1 GPR160 GRB7 KIF2C KNTC2 KRT14 KRT17 KRT5 MAPT MDM2 MELK MIA MKI67 MLPH MMP11 MYBL2 MYC NAT1 ORC6L PGR PHGDH PTTG1 RRM2 SFRP1 SLC39A6 TMEM45B TYMS UBE2C UBE2T\u0026#34; ALIAS_id \u0026lt;- strsplit(ALIAS, split = \u0026#34; \u0026#34;)[[1]] probe_id \u0026lt;- select(hthgu133a.db, keys = ALIAS_id, columns = c(\u0026#34;ALIAS\u0026#34;,\u0026#34;PROBEID\u0026#34;), keytype = \u0026#34;ALIAS\u0026#34;) names(probe_id) \u0026lt;- c(\u0026#34;ALIAS\u0026#34;,\u0026#34;ID_REF\u0026#34;) exprSet \u0026lt;- merge(probe_id, expMa, by = \u0026#34;ID_REF\u0026#34;)[,-2] row.names(exprSet) \u0026lt;- exprSet$ID_REF exprSet \u0026lt;- exprSet[,-1] exprSet \u0026lt;- log(edgeR::cpm(exprSet) + 1) pheatmap::pheatmap(log(exprSet + 1))   7. 下载数据集GSE24673的表达矩阵计算样本的相关性并且绘制热图，需要标记上样本分组信息 1 2 3 4 5 6 7 8 9 10 11 12 13  expMa \u0026lt;- read.table(\u0026#34;zhong7/GSE24673_series_matrix.txt.gz\u0026#34;, comment.char = \u0026#34;!\u0026#34;, sep = \u0026#34;\\t\u0026#34;, header = TRUE, row.names = \u0026#34;ID_REF\u0026#34;) dim(expMa) group_list \u0026lt;- factor(c(rep(\u0026#34;RB_139_09\u0026#34;,3),rep(\u0026#34;RB_535_09\u0026#34;,3), rep(\u0026#34;RB_984_09\u0026#34;,3), rep(\u0026#34;Healthy\u0026#34;, 2))) # pheatmap choosen_gene \u0026lt;- expMa[names(sort(apply(expMa,1,mad),decreasing = TRUE))[1:50],] heatMatrix \u0026lt;- t(scale(t(choosen_gene))) annotation_col \u0026lt;- data.frame(group_list=group_list) rownames(annotation_col) \u0026lt;- colnames(choosen_gene) pheatmap::pheatmap(heatMatrix, annotation_col = annotation_col)   8. 找到 GPL6244 platform of Affymetrix Human Gene 1.0 ST Array 对应的R的bioconductor注释包，并且安装它！ 1 2 3 4 5  options(BioC_mirror=\u0026#34;https://mirrors.ustc.edu.cn/bioc/\u0026#34;) options(\u0026#34;repos\u0026#34; = c(CRAN=\u0026#34;https://mirrors.tuna.tsinghua.edu.cn/CRAN/\u0026#34;)) BiocManager::install(\u0026#34;hugene10sttranscriptcluster.db\u0026#34;, version = \u0026#34;3.8\u0026#34;) # 直接根据关键字搜索，第一个结果可能是pd.hugene.1.0.st.v1, 但它并不是GPL6244正确的注释包。 # 关于芯片与注释包的对应关系，参考jimmy同学的文章https://www.jianshu.com/p/f6906ba703a0   9. 下载数据集GSE42872的表达矩阵，并且分别挑选出 所有样本的(平均表达量/sd/mad/)最大的探针，并且找到它们对应的基因。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  library(GEOquery) gset = getGEO(\u0026#39;GSE42872\u0026#39;, GSEMatrix = TRUE, AnnotGPL = FALSE, getGPL= FALSE) exprSet = exprs(gset[[1]]) # exprSet \u0026lt;- exprSet[apply(exprSet, 1, function(x) sum(x\u0026gt;1) \u0026gt; 5),] # exprSet \u0026lt;- log(edgeR::cpm(exprSet) + 1) # 处理中平均值最大的探针 max_mean \u0026lt;- names(sort(apply(exprSet,1, mean), decreasing = T))[1] # 处理中sd最大的探针 max_sd \u0026lt;- names(sort(apply(exprSet,1, sd), decreasing = T))[1] # 处理中mad最大的探针 max_mad \u0026lt;- names(sort(apply(exprSet,1, mad), decreasing = T))[1] # 查询探针ID对应基因的symbol library(hugene10sttranscriptcluster.db) select(hugene10sttranscriptcluster.db, keys = c(max_mean, max_sd, max_mad), columns = c(\u0026#34;SYMBOL\u0026#34;), keytype = \u0026#34;PROBEID\u0026#34;)   10. 下载数据集GSE42872的表达矩阵，并且根据分组使用limma做差异分析，得到差异结果矩阵 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  gset = getGEO(\u0026#39;GSE42872\u0026#39;, GSEMatrix = TRUE, AnnotGPL = FALSE, getGPL= FALSE) s \u0026lt;- toTable(hugene10sttranscriptclusterSYMBOL) exprSet = exprs(gset[[1]]) group_list \u0026lt;- as.character(c(rep(\u0026#34;Control\u0026#34;,3),rep(\u0026#34;Vemurafenib\u0026#34;,3))) dim(exprSet) # 过滤【只保留和注释文件探针id相同的探针】 efilt \u0026lt;- exprSet[rownames(exprSet)%in%s$probe_id,] dim(efilt) ## 整合1【目的：保证一个基因对应一个探针；如果基因和探针一一对应很好说，但如果一个基因对应多个探针：每个探针取一行的均值-》对应同一基因的探针取表达量最大的探针-》按照基因名给他们建索引，因为是按照基因来过滤探针（不用s$probe_id构建索引的原因是，看清楚我们的目的是让注释包的一个基因对应我们自己表达矩阵的一个探针。如果用s$probe_id那么结果就成了让注释包的一个探针对应我们自己表达矩阵的一个探针，当然这样也运行不成功，因为自己表达矩阵的探针过滤后的数量和注释包的探针数量不相等，这样没法一一对应。但基因名数量是不变的，什么是索引？以不变应万变的就是索引）】 maxp = by(efilt,s$symbol,function(x) rownames(x)[which.max(rowMeans(x))]) uniprobes = as.character(maxp) efilt=efilt[rownames(efilt)%in%uniprobes,] ## 整合2【目的：将我们表达矩阵的行名换成刚才一对一的基因名，并且match这个函数保证了表达矩阵和注释包的顺序是一致的】 rownames(efilt)=s[match(rownames(efilt),s$probe_id),2] # 差异分析 suppressMessages(library(limma)) #limma需要三个矩阵：表达矩阵（efilt）、分组矩阵（design）、比较矩阵（contrast） #先做一个分组矩阵～design，说明MAO是哪几个样本，MNO又是哪几个，其中1代表“是” design \u0026lt;- model.matrix(~0+factor(group_list)) colnames(design) \u0026lt;- levels(factor(group_list)) rownames(design) \u0026lt;- colnames(efilt) design #再做一个比较矩阵【一般是case比control】 contrast\u0026lt;-makeContrasts(paste0(unique(group_list),collapse = \u0026#34;-\u0026#34;),levels = design) contrast DEG \u0026lt;- function(efilt,design,contrast){ ##step1 fit \u0026lt;- lmFit(efilt,design) ##step2 fit2 \u0026lt;- contrasts.fit(fit, contrast) fit2 \u0026lt;- eBayes(fit2) ##step3 mtx = topTable(fit2, coef=1, n=Inf) deg_mtx = na.omit(mtx) return(deg_mtx) } DEG_mtx \u0026lt;- DEG(efilt,design,contrast) #得到全部的差异基因矩阵   高级 http://www.bio-info-trainee.com/3409.html\n1.安装一些R包： 1 2 3 4  数据包： ALL, CLL, pasilla, airway 软件包：limma，DESeq2，clusterProfiler 工具包：reshape2 绘图包：ggplot2   1  BiocManager::install(\u0026#34;reshape2\u0026#34;, version = \u0026#34;3.8\u0026#34;)   2. 了解ExpressionSet对象，比如CLL包里面就有data(sCLLex) ，找到它包含的元素，提取其表达矩阵(使用exprs函数)，查看其大小 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  \u0026gt; library(CLL) # sCLLex是一个ExpressionSet对象，封装了表达矩阵与样本信息 \u0026gt; data(sCLLex) \u0026gt; sCLLex ExpressionSet (storageMode: lockedEnvironment) #ExpressionSet对象 assayData: 12625 features, 22 samples element names: exprs protocolData: none phenoData sampleNames: CLL11.CEL CLL12.CEL ... CLL9.CEL (22 total) #样品名称 varLabels: SampleID Disease #用于分组的变量 varMetadata: labelDescription featureData: none experimentData: use \u0026#39;experimentData(object)\u0026#39; Annotation: hgu95av2 #对应的注释包 # 通过exprs函数提取表达矩阵 \u0026gt; exprMatrix \u0026lt;- exprs(sCLLex) \u0026gt; dim(exprMatrix) # 通过pData函数提取样本分组信息 \u0026gt; meta \u0026lt;- pData(sCLLex) \u0026gt; table(meta$Disease)   3. 了解 str,head,help函数，作用于第二步提取到的表达矩阵 1 2 3 4 5 6 7  \u0026gt; str(exprMatrix) #结果表明这是一个12625行，22列的二维矩阵 num [1:12625, 1:22] 5.74 2.29 3.31 1.09 7.54 ... - attr(*, \u0026#34;dimnames\u0026#34;)=List of 2 ..$ : chr [1:12625] \u0026#34;1000_at\u0026#34; \u0026#34;1001_at\u0026#34; \u0026#34;1002_f_at\u0026#34; \u0026#34;1003_s_at\u0026#34; ... ..$ : chr [1:22] \u0026#34;CLL11.CEL\u0026#34; \u0026#34;CLL12.CEL\u0026#34; \u0026#34;CLL13.CEL\u0026#34; \u0026#34;CLL14.CEL\u0026#34; ... \u0026gt; head(exprMatrix) #得到前6行结果 \u0026gt; help(exprMatrix) #该函数已过时。The function, class, or data object you have asked for has been deprecated or made defunct.   4. 安装并了解 hgu95av2.db 包,看看 ls(\u0026quot;package:hgu95av2.db\u0026quot;) 后 显示的那些变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \u0026gt; BiocManager::install(\u0026#34;hgu95av2.db\u0026#34;, version = \u0026#34;3.8\u0026#34;) \u0026gt; library(hgu95av2.db) \u0026gt; ls(\u0026#34;package:hgu95av2.db\u0026#34;) #可以发现hgu95v2.db中含有24个变量 [1] \u0026#34;hgu95av2\u0026#34; \u0026#34;hgu95av2.db\u0026#34; [3] \u0026#34;hgu95av2_dbconn\u0026#34; \u0026#34;hgu95av2_dbfile\u0026#34; [5] \u0026#34;hgu95av2_dbInfo\u0026#34; \u0026#34;hgu95av2_dbschema\u0026#34; [7] \u0026#34;hgu95av2ACCNUM\u0026#34; \u0026#34;hgu95av2ALIAS2PROBE\u0026#34; [9] \u0026#34;hgu95av2CHR\u0026#34; \u0026#34;hgu95av2CHRLENGTHS\u0026#34; [11] \u0026#34;hgu95av2CHRLOC\u0026#34; \u0026#34;hgu95av2CHRLOCEND\u0026#34; [13] \u0026#34;hgu95av2ENSEMBL\u0026#34; \u0026#34;hgu95av2ENSEMBL2PROBE\u0026#34; [15] \u0026#34;hgu95av2ENTREZID\u0026#34; \u0026#34;hgu95av2ENZYME\u0026#34; [17] \u0026#34;hgu95av2ENZYME2PROBE\u0026#34; \u0026#34;hgu95av2GENENAME\u0026#34; [19] \u0026#34;hgu95av2GO\u0026#34; \u0026#34;hgu95av2GO2ALLPROBES\u0026#34; [21] \u0026#34;hgu95av2GO2PROBE\u0026#34; \u0026#34;hgu95av2MAP\u0026#34; [23] \u0026#34;hgu95av2MAPCOUNTS\u0026#34; \u0026#34;hgu95av2OMIM\u0026#34;   5. 理解 head(toTable(hgu95av2SYMBOL)) 的用法，找到 TP53 基因对应的探针ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14  \u0026gt; head(toTable(hgu95av2SYMBOL)) #查看芯片ID与基因symbol之间的对应关系 probe_id symbol 1 1000_at MAPK3 2 1001_at TIE1 3 1002_f_at CYP2C19 4 1003_s_at CXCR5 5 1004_at CXCR5 6 1005_at DUSP1 \u0026gt; probe_sym \u0026lt;- toTable(hgu95av2SYMBOL) \u0026gt; subset(probe_sym, symbol == \u0026#34;TP53\u0026#34;) #使用subset抽取data.frame中的数据，筛选条件可以多样，使用逻辑关系运算组合 probe_id symbol 966 1939_at TP53 997 1974_s_at TP53 1420 31618_at TP53   6. 理解探针与基因的对应关系，总共多少个基因，基因最多对应多少个探针，是哪些基因，是不是因为这些基因很长，所以在其上面设计多个探针呢？ 1 2 3 4 5 6 7  \u0026gt; library(magrittr) \u0026gt; probe_sym$symbol%\u0026gt;%unique()%\u0026gt;%length() #基因的数目 [1] 8585 \u0026gt; probe_sym$symbol%\u0026gt;%table()%\u0026gt;%sort()%\u0026gt;%tail() #对应最多探针的基因有哪些 . YME1L1 GAPDH INPP4A MYB PTGER3 STAT1 7 8 8 8 8 8   7. 第二步提取到的表达矩阵是12625个探针在22个样本的表达量矩阵，找到那些不在 hgu95av2.db 包收录的对应着SYMBOL的探针。 1  e_filter_out \u0026lt;- exprMatrix[!rownames(exprMatrix)%in%probe_sym$probe_id,]   8. 过滤表达矩阵，删除那1165个没有对应基因名字的探针。 1  e_filter \u0026lt;- exprMatrix[rownames(exprMatrix)%in%probe_sym$probe_id,]   9. 整合表达矩阵，多个探针对应一个基因的情况下，只保留在所有样本里面平均表达量最大的那个探针。 1 2 3 4 5  max_e \u0026lt;- by(e_filter, probe_sym$symbol, function(x) rownames(x)[which.max(rowMeans(x))]) uniprobes \u0026lt;- as.character(max_e) e_filter \u0026lt;- e_filter[rownames(e_filter)%in%uniprobes,]   10. 把过滤后的表达矩阵更改行名为基因的symbol，因为这个时候探针和基因是一对一关系了。 1  rownames(e_filter) \u0026lt;- probe_sym[match(rownames(e_filter), probe_sym$probe_id),2]   11. 对第10步得到的表达矩阵进行探索，先画第一个样本的所有基因的表达量的boxplot,hist,density ， 然后画所有样本的 这些图 1 2 3 4 5  exprM_L1 \u0026lt;- data.frame(symbol = row.names(e_filter), expm = e_filter[,1]) p_box \u0026lt;- ggplot(exprM_L1, aes(y = expm)) + geom_boxplot() p_hist \u0026lt;- ggplot(exprM_L1, aes(x = expm)) + geom_histogram(bins = 50) p_dens \u0026lt;- ggplot(exprM_L1, aes(x = expm)) + geom_density()   12. 理解统计学指标mean,median,max,min,sd,var,mad并计算出每个基因在所有样本的这些统计学指标，最后按照mad值排序，取top 50 mad值的基因，得到列表。  注意：这个题目出的并不合规，请仔细看。\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  e_mean \u0026lt;- rowMeans(e_filter) e_median \u0026lt;- rowMedians(e_filter) e_max \u0026lt;- rowMax(e_filter) e_min \u0026lt;- rowMin(e_filter) e_sd \u0026lt;- apply(e_filter, 1, sd) %\u0026gt;% sort() #没有对应的row系列函数的话，可以使用apply方法 e_var \u0026lt;- apply(e_filter, 1, var) %\u0026gt;% sort() e_mad_50 \u0026lt;- apply(e_filter, 1, mad) %\u0026gt;% sort() %\u0026gt;% tail(50) \u0026gt; names(e_mad_50) [1] \u0026#34;PFN2\u0026#34; \u0026#34;TNFSF9\u0026#34; \u0026#34;ARHGAP44\u0026#34; \u0026#34;P2RY14\u0026#34; \u0026#34;THEMIS2\u0026#34; \u0026#34;LPL\u0026#34; [7] \u0026#34;ANXA4\u0026#34; \u0026#34;DUSP6\u0026#34; \u0026#34;DUSP5\u0026#34; \u0026#34;H1FX\u0026#34; \u0026#34;FLNA\u0026#34; \u0026#34;CLEC2B\u0026#34; [13] \u0026#34;TSPYL2\u0026#34; \u0026#34;ZNF266\u0026#34; \u0026#34;S100A9\u0026#34; \u0026#34;NR4A2\u0026#34; \u0026#34;TGFBI\u0026#34; \u0026#34;ARF6\u0026#34; [19] \u0026#34;APBB2\u0026#34; \u0026#34;VCAN\u0026#34; \u0026#34;RBM38\u0026#34; \u0026#34;CAPG\u0026#34; \u0026#34;PLXNC1\u0026#34; \u0026#34;RGS2\u0026#34; [25] \u0026#34;RNASE6\u0026#34; \u0026#34;VAMP5\u0026#34; \u0026#34;CYBB\u0026#34; \u0026#34;GNLY\u0026#34; \u0026#34;CCL3\u0026#34; \u0026#34;OAS1\u0026#34; [31] \u0026#34;TRIB2\u0026#34; \u0026#34;ZNF804A\u0026#34; \u0026#34;IGH\u0026#34; \u0026#34;PCDH9\u0026#34; \u0026#34;VIPR1\u0026#34; \u0026#34;COBLL1\u0026#34; [37] \u0026#34;GUSBP11\u0026#34; \u0026#34;S100A8\u0026#34; \u0026#34;HBB\u0026#34; \u0026#34;LHFPL2\u0026#34; \u0026#34;FCN1\u0026#34; \u0026#34;ZAP70\u0026#34; [43] \u0026#34;IGLC1\u0026#34; \u0026#34;LGALS1\u0026#34; \u0026#34;FOS\u0026#34; \u0026#34;SLAMF1\u0026#34; \u0026#34;TCF7\u0026#34; \u0026#34;DMD\u0026#34; [49] \u0026#34;IGF2BP3\u0026#34; \u0026#34;FAM30A\u0026#34;   13. 根据第12步骤得到top 50 mad值的基因列表来取表达矩阵的子集，并且热图可视化子表达矩阵。试试看其它5种热图的包的不同效果。 1 2 3 4  e50 \u0026lt;- e_filter[names(e_mad_50),] pheatmap::pheatmap(e50) tt_50 \u0026lt;- t(scale(t(e50))) pheatmap::pheatmap(tt_50)   14. 取不同统计学指标mean,median,max,mean,sd,var,mad的各top50基因列表，使用UpSetR包来看他们之间的overlap情况。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  e_mean_50 \u0026lt;- rowMeans(e_filter) %\u0026gt;% sort() %\u0026gt;% tail(50) e_median_50 \u0026lt;- rowMedians(e_filter) %\u0026gt;% sort() %\u0026gt;% tail(50) e_max_50 \u0026lt;- rowMax(e_filter) %\u0026gt;% sort() %\u0026gt;% tail(50) e_min_50 \u0026lt;- rowMin(e_filter) %\u0026gt;% sort() %\u0026gt;% tail(50) e_sd_50 \u0026lt;- apply(e_filter, 1, sd) %\u0026gt;% sort() %\u0026gt;% tail(50) e_var_50 \u0026lt;- apply(e_filter, 1, var) %\u0026gt;% sort() %\u0026gt;% tail(50) e_mad_50 \u0026lt;- apply(e_filter, 1, mad) %\u0026gt;% sort() %\u0026gt;% tail(50) g_all \u0026lt;- unique(c(names(e_mean_50), names(e_median_50), names(e_max_50), names(e_min_50), names(e_sd_50), names(e_var_50), names(e_mad_50))) g_upset \u0026lt;- data.frame(g_all = g_all, g_mean=ifelse(g_all %in% names(e_mean_50),1,0), g_median=ifelse(g_all %in% names(e_median_50),1,0), g_max=ifelse(g_all %in% names(e_max_50),1,0), g_min=ifelse(g_all %in% names(e_min_50),1,0), g_sd=ifelse(g_all %in% names(e_sd_50),1,0), g_var=ifelse(g_all %in% names(e_var_50),1,0), g_mad=ifelse(g_all %in% names(e_mad_50),1,0) ) UpSetR::upset(g_upset, nsets = 7)   15. 在第二步的基础上面提取CLL包里面的data(sCLLex) 数据对象的样本的表型数据。 1 2 3  pdata=pData(sCLLex) group_list=as.character(pdata[,2]) group_list   16. 对所有样本的表达矩阵进行聚类并且绘图，然后添加样本的临床表型数据信息(更改样本名) 参考来源 https://www.jianshu.com/p/993b4022363f\nhttp://www.bio-info-trainee.com/3415.html\nhttps://www.statmethods.net/stats/withby.html\n","permalink":"https://sr-c.github.io/2019/03/11/R-exercise/","tags":["R","生信技能树","biotrainee"],"title":"R语言练习题"},{"categories":null,"contents":"端粒重复序列 端粒是染色体末端的一端核酸序列，起着保护染色体末端免于降解，防止与相邻的染色体错误融合的作用。其名称来源于希腊语 τέλος (end)和μέρος (part) 。对于脊椎动物，端粒的序列是AGGGTT，其对应的互补序列就是TCCCAA，还有这一个单链的TTAGGG末尾。这段TTAGGG的序列在人类基因组中约重复2500次。人类的平均端粒长度由出生时的约11 kb，在老年时降至短于4 kb. 而男性端粒的平均缩短速度要快于女性。\n在染色体复制时，DNA复制酶在染色体末端不能继续复制，因此在每次复制时，染色体的末端都会缩短。这是由于冈崎片段需要RNA引物连接到后随链的前端。在细胞分裂时，端粒是染色体末端的缓冲区；防止基因被截短。端粒本身被端粒保护蛋白复合体，还有端粒DNA编码的RNA覆盖。\n不同物种的端粒序列不同，目前已知的端粒序列数据在端粒数据库中可以访问，目前的版本在2018年8月更新。\n预测物种的端粒序列 对于一个端粒序列仍然未知的物种，我们怎么发现其端粒的序列呢？\n在最近发表的一篇文章指出，目前由bioserf.org提供的在线服务 sequence repeat finder (SERF) 可以进行。用户提供全基因组组装的结果（FASTA 格式）或二代测序的短读长数据（FASTQ 格式）即可。用户在注册后即可使用。\n输入参数     Telomere Identification Protocol     FASTQ Input Reads from NGS   FASTA Input Assembled Contigs.   Contiguousall Analysis Type   TG BaseType (must contain these base types in the identified repeats)   2 BaseTypeMincount (must have atleast 2 of either type in the identified repeat)   3 MinRepeats to treat as tandem   5 MinBPSize of repeat motif   15 MaxBPSize of repeat motif   90% MatchQuality (Tandem repeats to cover atleast 90% of the entire read. Iterate with lower figure until repeats are identified.)   2000 Base count to split large assembled contigs in FASTA file for faster results. Defaults to 0 for FASTQ reads.   Yes Permutate   Yes Exclude Homopolymers   Yes Exclude Supersets   Yes List Cluster Totals   Notes For large input data (1GB+), analyze in smaller minbp-maxbp chunks 7-10, 11-20, 21-30 etc (instead of 7-60) to see incremental results    输出结果 这是示例公共数据中洋葱的输出结果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Results file (last 20 lines): # ANALYSIS BEGIN Fri Mar 8 01:54:00 2019 # Input = BSF1-AlliumCepa.fastq.gz # Output = Results-32188243-3-5-15-Q90-TG-2-contiguousall.txt # SEGMENT-WISE RESULTS # Motif, Repeats, Position, Size, Bases, Quality, Segment#, SegmentSize TGGGCTCGGTTA, 3, 1, 12, 36, 94.7, 304572, 38 TCGGTTATGGGC, 3, 1, 12, 36, 92.3, 317960, 39 # CLUSTER BY POSITION # Motif, Repeats, Repeats%, Bases, Bases%, Genome%, Position, Size TGGGCTCGGTTA, 3, 50.0, 36, 50.0, 0.0, 1, 12 TGGGCTCGGTTA, 3, 50.0, 36, 50.0, 0.0, 1, 12 # CLUSTER TOTALS # Motif, Repeats, Repeats%, Bases, Bases%, Genome%, Size TGGGCTCGGTTA, 6, 100.0, 72, 100.0, 0.0, 12 # ANALYSIS END Fri Mar 8 02:28:54 2019   参考来源 https://www.future-science.com/doi/full/10.2144/btn-2018-0057#\nhttps://bioserf.org/r/p/swebserf\n","permalink":"https://sr-c.github.io/2019/03/08/SERF-identify-telomere-sequences/","tags":["telomere","端粒"],"title":"使用SERF鉴定端粒序列"},{"categories":null,"contents":"conda升级 conda提示的升级方案为\n1  conda update -n base conda   按照提示进行升级，但升级提示竟然会将原本3.6版本的python降级至2.7\n1 2 3  The following packages will be DOWNGRADED: python: 3.6.6-h5001a0f_3 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge --\u0026gt; 2.7.15-h721da81_1008 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge   这表明，conda源配置中的conda-forge优先级过高，会出现一定的问题。检查一下原来的.condarc文件。\n1 2 3 4 5 6 7 8 9 10  channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ # - bioconda # - conda-forge # - defaults - r show_channel_urls: true   竟然将官方的defaults源注释掉了😓\n不合理的顺序还可能会导致意想不到的错误，举例如下\n 并且举了一个例子,如果你有一个X包来自于conda-forge,一个Y包来自于biconda才能让Z包顺利工作.但是在YAML里面不知道X,Y那个包是来自于conda-forge,那个是来自于bioconda. 比如说conda-forge有一个bzip2-1.0.6,里面有对应的动态.so文件,而defaults也有bzip2-1.0.6,但是没有.so文件. 但是根据按照顺序,conda先从default里面找到了bzip2-1.0.6, 结果你就会在运行软件的时候找不到依赖文件.\n 但hoptop认为一定要将default源放在最后，我认为还是不能同意。因为上述的conda更新问题就是由于default源（或第三方镜像的default源）的优先级低于conda-forge。因此，我认为至少conda-forge的优先级不宜过高。\n合理配置conda源顺序 参照前人的方案，我们需要将conda-forge的优先级降低，仅作为官方源和bioconda源的补充。修改后的配置如下：\n1 2 3 4 5 6 7 8 9 10  channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - bioconda - defaults - r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ - conda-forge show_channel_urls: true   参考来源 https://shengxin.ren/article/432\nhttps://www.jianshu.com/p/7025139ac7d4\nhttp://vincebuffalo.org/notes/2017/08/28/notes-on-anaconda.html\n","permalink":"https://sr-c.github.io/2019/03/03/conda-channels-order/","tags":["conda"],"title":"conda源的配置"},{"categories":null,"contents":"安装 1 2 3 4 5 6 7 8  git clone https://github.com/machinegun/SALSA.git conda create -n SALSA python=2.7 source activate SALSA conda install boost networkx==1.11 cd SALSA make   debug 1 2 3 4 5  g++ -O3 -Wall -Wextra -std=c++11 -o correct_links correct_links.cpp correct_links.cpp:12:10: fatal error: boost/config.hpp: No such file or directory #include \u0026lt;boost/config.hpp\u0026gt; ^~~~~~~~~~~~~~~~~~ compilation terminated.   这表明，编译过程中未配置好之前的boost环境，于是我们查看发现原来已经安装了boost，因此，我们卸载conda环境中的boost直接使用原环境中的boost进行编译，但是仍然得到报错\n1 2 3 4  /usr/include/boost/graph/graph_concepts.hpp:92:18: error: using invalid field \u0026#39;boost::concepts::IncidenceGraph\u0026lt;G\u0026gt;::p\u0026#39; e = *p.first; ^ make: *** [correct_links] Error 1   经过查询似乎是原来环境中的boost版本过低导致，因此，还是需要手动指定conda环境中的boost\n1 2  export CPLUS_INCLUDE_PATH=~/miniconda3/envs/SALSA/include:$CPLUS_INCLUDE_PATH export BOOST_INCLUDE=~/miniconda3/envs/SALSA/include   之后可完成编译\n1 2 3 4 5 6 7 8 9 10 11 12  \u0026gt; source ~/.bashrc \u0026gt; make clean \u0026gt; make g++ -O3 -Wall -Wextra -std=c++11 -o correct_links correct_links.cpp correct_links.cpp: In function \u0026#39;std::vector\u0026lt;std::__cxx11::basic_string\u0026lt;char\u0026gt; \u0026gt; split(const string\u0026amp;, size_t)\u0026#39;: correct_links.cpp:46:19: warning: comparison between signed and unsigned integer expressions [-Wsign-compare] for(int i = 0;i \u0026lt; s.length();i+=count) ~~^~~~~~~~~~~~ correct_links.cpp: In function \u0026#39;int main(int, char**)\u0026#39;: correct_links.cpp:237:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare] for(int i = 0;i \u0026lt; orientations.size();i++) ~~^~~~~~~~~~~~~~~~~~~~~   查看环境中boost的版本 1 2 3 4 5 6  cat /usr/include/boost/version.hpp | grep \u0026#34;BOOST_LIB_VERSION\u0026#34; // BOOST_LIB_VERSION must be defined to be the same as BOOST_VERSION #define BOOST_LIB_VERSION \u0026#34;1_41\u0026#34; cat ~/miniconda3/envs/SALSA/include/boost/version.hpp | grep \u0026#34;BOOST_LIB_VERSION\u0026#34; // BOOST_LIB_VERSION must be defined to be the same as BOOST_VERSION #define BOOST_LIB_VERSION \u0026#34;1_69\u0026#34;   参考来源 https://blog.csdn.net/lijingpengchina/article/details/9100775\nhttps://blog.csdn.net/dongchongyang/article/details/72718687\nhttps://blog.csdn.net/Doubao93/article/details/80176537\nhttps://askubuntu.com/questions/147474/how-can-i-find-boost-version\n","permalink":"https://sr-c.github.io/2019/02/28/SALSA-install/","tags":["SALSA","debug","Hi-C"],"title":"SALSA安装"},{"categories":null,"contents":"在使用iTOL或者Evolview可视化进化树时，往往nwk格式的bootstrap值需要转化为百分数\n使用re正则匹配 参考作者使用re正则查找到以)开头，:结尾，中间是四位小数的数值，去掉头尾之后，转化为浮点数，乘以100，四舍五入，转化为整数，最后转化为字符串。然后使用re.sun()进行替换，将原来的小数转化为百分数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  #!/usr/bin/python # Usage: python convertBootstrap2Integer.py tree.nwk \u0026gt; new.tree.nwk # The bootstrap data of phylogenetic tree is decimal. This script convert decimal data to percentage without %. import sys, re inputNwkTree = sys.argv[1] treeData = open(inputNwkTree).readlines()[0] treeData = treeData.strip() totalDecimalData = re.finditer(\u0026#39;\\)(\\d\\.\\d{4})\\:\u0026#39;, treeData) for item in totalDecimalData: matchedData = item.group() matchedData = matchedData[1:-1] convertedData = \u0026#39;%s\u0026#39; %(int(round(float(matchedData)*100))) treeData = re.sub(matchedData, convertedData, treeData) print treeData   Debug 大部分情况下，该程序工作良好。但在某些时候，得到的进化树存在错误。\n这是由于程序直接使用数值进行替换。若在进化树中，该数值还出现在非bootstrap的位置，则同样会被替换，导致生成的树文件出错。要解决此问题，我们可以直接连带bootstrap值前后的)与:一起作为字符串进行替换，从而避免错误的匹配。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  #!/usr/bin/env python # Usage: python convertBootstrap2Integer.py tree.nwk new.tree.nwk # The bootstrap data of phylogenetic tree is decimal. This script convert decimal data to percentage without %. import sys import re inputNwkTree = sys.argv[1] treeData = open(inputNwkTree).readline().strip() totalDecimalData = re.finditer(\u0026#39;\\)(\\d\\.\\d{4})\\:\u0026#39;, treeData) for item in totalDecimalData: matchedData = item.group() matchedNum = matchedData[1:-1] convertedData = \u0026#39;)%s:\u0026#39; % (int(round(float(matchedNum) * 100))) treeData = re.sub(re.escape(matchedData), convertedData, treeData) with open(sys.argv[2], \u0026#39;w\u0026#39;) as o: o.write(treeData)   值得注意的是，在第13行treeData = re.sub(re.escape(matchedData), convertedData, treeData)中，若直接使用treeData = re.sub(matchedData, convertedData, treeData)，则会出现re.error: unbalanced parenthesis at position 0报错。这是由于在正则替换时，括号)是需要转义的，此时，我们可以使用re.escape()进行转义。\n参考来源 http://blog.sina.com.cn/s/blog_8dac0f5f0102wyz8.html\nhttps://stackoverflow.com/questions/15947140/python-regex-error-unbalanced-parenthesis\n","permalink":"https://sr-c.github.io/2019/02/22/convert-Bootstrap-to-Integer/","tags":["Python，debug"],"title":"将Bootstrap值转化为百分数"},{"categories":null,"contents":"Overlap Graphs http://rosalind.info/problems/grph/\n基于图论的基本设定，二代测序的组装就是基于kmer的延伸\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  import sys import argparse parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--input\u0026#39;,\u0026#39;-i\u0026#39;, type=str, required=True, help=\u0026#39;input file in fasta format\u0026#39;) args = parser.parse_args() def read_fasta(input): with open(input) as read_fasta: fasta = {} for line in read_fasta: line = line.strip() if line[0] == \u0026#39;\u0026gt;\u0026#39;: header = line[1:] else: sequence = line fasta[header] = fasta.get(header, \u0026#39;\u0026#39;) + sequence return fasta #取得序列的suffix def suffix(str): sub = [] for i in range(len(str)-3,len(str)): sub.append(str[i]) return sub #取得序列的prefix def prefix(str): sub = [] for i in range(3): sub.append(str[i]) return sub fasta = read_fasta(args.input) #print(len(fasta.keys())) for i in fasta.keys(): for k in fasta.keys(): if suffix(fasta[i]) == prefix(fasta[k]) and i != k: print(\u0026#34;{} {}\u0026#34;.format(i,k))   他山之石 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  #!/usr/bin/env python from collections import defaultdict from rosalind import readfasta def overlap(alist): prefixes = defaultdict(list) for name, code in alist: prefixes[code[:3]].append(name) for name, code in alist: suffix = code[-3:] for other in prefixes[suffix]: if other != name: print name, other overlap(readfasta())   Finding a Protein Motif http://rosalind.info/problems/mprt/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  import requests with open(\u0026#34;rosalind_mprt.txt\u0026#34;) as f: ID_list = f.readlines() output = {} for pepID in ID_list: pepID = pepID.strip() pep_fasta = requests.get(\u0026#34;https://www.uniprot.org/uniprot/\u0026#34; + pepID + \u0026#34;.fasta\u0026#34;).text #print(test) lines = pep_fasta.splitlines() #fasta = {} body = \u0026#39;\u0026#39; for line in lines: if line[0] == \u0026#39;\u0026gt;\u0026#39;: header = line[1:] else: body += line print(body) local = [] for i in range(len(body)-4): kmer = body[i:i+4] if kmer[0] == \u0026#39;N\u0026#39; \\ and kmer[1] != \u0026#39;P\u0026#39; \\ and kmer[3] != \u0026#39;P\u0026#39; \\ and (kmer[2] == \u0026#34;S\u0026#34; or kmer[2] == \u0026#34;T\u0026#34;): local.append(str(i+1)) if local != []: output[pepID] = local # for i in output.keys(): # print(i + \u0026#39;\\n\u0026#39; + \u0026#39; \u0026#39;.join(output[i])) with open(\u0026#34;rosalind_mprt.out\u0026#34;,\u0026#39;w\u0026#39;) as o: for i in output.keys(): o.write(i + \u0026#39;\\n\u0026#39; + \u0026#39; \u0026#39;.join(output[i]) + \u0026#39;\\n\u0026#39;)   他山之石 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  from re import finditer from sys import argv from urllib.request import urlopen uniprot = \u0026#34;http://www.uniprot.org/uniprot/%s.fasta\u0026#34; for protein in open(argv[1], \u0026#39;r\u0026#39;).read().strip().splitlines(): # Fetch the protein\u0026#39;s fasta file and get rid of newlines.\u0026#39; f = urlopen(uniprot % protein).read().decode(\u0026#39;utf-8\u0026#39;) f = \u0026#39;\u0026#39;.join(f.splitlines()[1:]) # Find all the positions of the N-glycosylation motif. locs = [g.start()+1 for g in finditer(r\u0026#39;(?=N[^P][ST][^P])\u0026#39;, f)] # Print them out, if any. if locs != []: print(protein) print(\u0026#39; \u0026#39;.join(map(str, locs)))   Consensus and Profile http://rosalind.info/problems/cons/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  import argparse parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--input\u0026#39;,\u0026#39;-i\u0026#39;, type=str, required=True, help=\u0026#34;input file in fasta format with equal length strings\u0026#34;) parser.add_argument(\u0026#39;--output\u0026#39;,\u0026#39;-o\u0026#39;, type=str, default=\u0026#34;rosalind_cons.out\u0026#34;, help=\u0026#34;output file\u0026#34;) args = parser.parse_args() def read_fasta(input): fasta = {} with open(input) as f: lines = f.readlines() for line in lines: line = line.strip() if line[0] == \u0026#39;\u0026gt;\u0026#39;: seqID = line[1:] else: fasta[seqID] = fasta.get(seqID, \u0026#39;\u0026#39;) + line return fasta def get_line(N): for i in fasta.keys(): seq_length = len(fasta[i]) list_a = [0 for x in range(seq_length)] for i in fasta.keys(): for k in range(seq_length): if fasta[i][k] == str(N): list_a[k] += 1 return list_a fasta = read_fasta(args.input) bases = [\u0026#39;A\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;G\u0026#39;,\u0026#39;T\u0026#39;] seq_length = len(get_line(\u0026#39;A\u0026#39;)) consensus = [\u0026#39;\u0026#39;]*seq_length for m in range(seq_length): count_max = 0 for i in bases: if get_line(i)[m] \u0026gt; count_max: count_max = get_line(i)[m] consensus[m] = i with open(args.output, \u0026#39;w\u0026#39;) as o: o.write(\u0026#39;\u0026#39;.join(consensus) + \u0026#39;\\n\u0026#39;) for i in bases: list_N = get_line(i) with open(args.output, \u0026#39;a\u0026#39;) as o: o.write(\u0026#34;{}: {}\\n\u0026#34;.format(i, \u0026#39; \u0026#39;.join([str(i) for i in list_N])))   他山之石 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  strands = [x.strip() for x in f.readlines()] matrix = zip(*strands) profile_matrix = map(lambda x: dict((base, x.count(base)) for base in \u0026#34;ACGT\u0026#34;), matrix) consensus = [max(x,key = x.get) for x in profile_matrix] print \u0026#34;\u0026#34;.join(consensus); for base in \u0026#34;ACGT\u0026#34;: print base+\u0026#34;:\u0026#34;, for x in profile_matrix: print x[base], print \u0026#34;\u0026#34;   总结 需要学习的函数：\n1 2 3 4 5 6  #正则库中的finditer() #from sys import argv 可简便接受输入项 #for循环接受列表 map() zip() f.write(\u0026#39;output.file\u0026#39;,\u0026#39;a\u0026#39;) #追加输出   参考来源 ","permalink":"https://sr-c.github.io/2019/02/22/python-exercise-6/","tags":["python","rosalind"],"title":"Python练习6"},{"categories":null,"contents":"软件安装 软件默认提供了Singularity的镜像，可以直接使用\n环境依赖  The bowtie2 mapper Python (\u0026gt;2.7) with pysam (\u0026gt;=0.8.3), bx-python(\u0026gt;=0.5.0), numpy(\u0026gt;=1.8.2), and scipy(\u0026gt;=0.15.1) libraries. Note that the current version does not support python 3 R with the RColorBrewer and ggplot2 (\u0026gt;2.2.1) packages g++ compiler samtools (\u0026gt;1.1) Unix sort (which support -V option) is required ! For Mac OS user, please install the GNU core utilities !  其中，python模块pysam与bx-python不太好安装，因此，还是使用conda来配置安装环境比较方便。\n1 2 3 4 5 6 7  conda create -n hicpro python=2.7 source activate hicpro conda install -y samtools bowtie2 R conda install -y pysam bx-python numpy scipy R \u0026gt;install.packages(c(\u0026#39;ggplot2\u0026#39;,\u0026#39;RColorBrewer\u0026#39;))   但是我们得到一条报错\n1 2 3 4 5  Warning messages: 1: In download.file(url, destfile = f, quiet = TRUE) : unable to load shared object \u0026#39;/Storage/data002/shurh/miniconda3/envs/hicpro/lib/R/modules//internet.so\u0026#39;: libssl.so.1.0.0: cannot open shared object file: No such file or directory 2: packages ‘ggplot2’, ‘RColorBrewer’ are not available (for R version 3.5.1)   经过错误排查，我们需要升级curl\n1  conda update curl   然后，我们就可以重新在R中安装ggplot2与RColorBrewer了\n软件编译 解决了环境依赖之后，编译安装就很容易了\n1 2 3 4 5  git clone https://github.com/nservant/HiC-Pro.git cd HiC-Pro vi config-install.txt make configure make install   测试使用 测试数据 1 2  wget https://zerkalo.curie.fr/partage/HiC-Pro/HiCPro_testdata.tar.gz \u0026amp;\u0026amp; tar -zxvf HiCPro_testdata.tar.gz wget ftp://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.fna.gz   还可以使用其他已发表的数据来使用\n1 2 3 4 5 6  prefetch SRR824846 wget ftp://ftp.ensemblgenomes.org/pub/bacteria/release-40/fasta/bacteria_20_collection/caulobacter_crescentus_na1000/dna/Caulobacter_crescentus_na1000.ASM2200v1.dna.toplevel.fa.gz gunzip Caulobacter_crescentus_na1000.ASM2200v1.dna.toplevel.fa.gz bowtie2-build Caulobacter_crescentus_na1000.ASM2200v1.dna.toplevel.fa bacteria cat \u0026gt;genome.size Chromosome 4016942   得到注释文件 HiC-Pro需要两个注释文件，分别包含实验对基因组的酶切位点，以及基因组的染色体长度。\nrestricttion fragments file 该文件可以由HiC-Pro提供的一支程序得到\n染色体长度 1 2 3 4 5 6 7 8  ### ## How to generate chromosome size files ? ### require(Biostrings) TargetGenome \u0026lt;- readDNAStringSet(\u0026#34;genome.fasta\u0026#34;, format = \u0026#34;fasta\u0026#34;) chrom.size \u0026lt;- seqlengths(TargetGenome) write.table(chrom.size, file=\u0026#34;genome.size\u0026#34;, quote=FALSE, col.names=TRUE, sep=\u0026#34;\\t\u0026#34;)   参考来源 http://nservant.github.io/HiC-Pro/\nhttps://cloud.tencent.com/developer/article/1178446\nhttps://github.com/JiFansen/Blog/issues/5\nhttps://zhuanlan.zhihu.com/p/48455671\nhttps://cloud.tencent.com/developer/article/1178442\n","permalink":"https://sr-c.github.io/2019/02/20/HiC-pro-quickstart/","tags":["Hi-C"],"title":"HiC-Pro使用手册"},{"categories":null,"contents":"计算肽链的分子量 http://rosalind.info/problems/prtm/\n值得注意的是，最后一个氨基酸中一个水分子的分子量18.01056 Da. 但我的结果中没有加上这个水，仍然提示我为正确答案，可见还是比较宽松的。\n1 2 3 4 5 6 7 8 9 10 11 12  #!/usr/bin/env python mass_table = dict(A=71.03711, C=103.00919, D=115.02694, E=129.04259, F=147.06841, G=57.02146, H=137.05891, I=113.08406, K=128.09496, L=113.08406, M=131.04049, N=114.04293, P=97.05276, Q=128.05858, R=156.10111, S=87.03203, T=101.04768, V=99.06841, W=186.07931, Y=163.06333) prot_mass = 0 with open(\u0026#34;rosalind_prtm.txt\u0026#34;) as prot: pro_line = prot.readline().strip() # for i in range(len(pro_line)): # prot_mass += mass_table[pro_line[i]] for i in pro_line: prot_mass += mass_table[i] print(prot_mass)   他山之石 快速输入字符串，并转换为字典的方式值得学习。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  mmt = \u0026#34;\u0026#34;\u0026#34; A 71.03711 C 103.00919 D 115.02694 E 129.04259 F 147.06841 G 57.02146 H 137.05891 I 113.08406 K 128.09496 L 113.08406 M 131.04049 N 114.04293 P 97.05276 Q 128.05858 R 156.10111 S 87.03203 T 101.04768 V 99.06841 W 186.07931 Y 163.06333 \u0026#34;\u0026#34;\u0026#34;.split() mmt = dict(zip(mmt[::2],map(float,mmt[1::2]))) s = \u0026#34;SKADYEK\u0026#34; print \u0026#34;%.2f\u0026#34;%(sum(map(lambda x:mmt[x],s))+18.01528)   孟德尔第二定律 http://rosalind.info/problems/lia/\n亲本(第0代)的基因型为AaBb，与基因型同为AaBa的个体产生2个后代。之后，每个个体产生2个后代，且均与AaBa基因型的个体婚配。求在第k代的所有个体中，至少有N个个体的基因型为AaBb的概率。\n 题干中专门提示了不需要计算每一代中个体AaBb基因型婚配的具体情况。这是因为每代的个体都与AaBa婚配，因此种群中A/a,B/b基因型的频率保持不变，一直为1/2。因此每一代中个体为AaBb基因型的概率均为1/2*1/2=1/4。\n接下来就是一个高中概率论的排列组合问题。\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  #!/usr/bin/env python with open(\u0026#34;rosalind_lia.txt\u0026#34;) as f: (k, N) = f.readline().strip().split(\u0026#34; \u0026#34;) k = int(k) N = int(N) #计算排列数A(m, n) def aR(m, n): be = 1 for i in range(n): be *= m - i return be #计算组合数C(m, n) def cR(m, n): if m \u0026lt; n: print(\u0026#34;error input\u0026#34;) else: return aR(m, n) / aR(n, n) #计算第k代(2^k个体)中至少含有N个0.25概率事件发生的总概率 rate = 0 for i in range(N): rate += cR(2 ** k, i) * 0.25 ** i * 0.75 ** (2 ** k - i) ans = 1 - rate print(ans)   他山之石 其他人的答案中，使用R能够更方便地进行这个排列组合计算\n1  pbinom(N-1, 2^k, 0.25, lower.tail=F)   python中使用scipy也可以计算排列组合\n1 2 3 4 5  \u0026gt;\u0026gt; from scipy.special import comb, perm \u0026gt;\u0026gt; perm(3, 2) 6.0 \u0026gt;\u0026gt; comb(3, 2) 3.0   参考来源 http://rosalind.info/problems/prtm/solutions/\nhttp://rosalind.info/problems/lia/solutions/\nhttps://blog.csdn.net/lanchunhui/article/details/51824602\n","permalink":"https://sr-c.github.io/2019/02/19/python-exercise-5/","tags":["Python","Rosalind"],"title":"Python练习5"},{"categories":null,"contents":"问题描述 1  Error: C stack usage 9964644 is too close to the limit   这似乎是由于数据过大导致\n解决方案 1 2 3 4 5  $ ulimit -s # print default 10240 $ R --slave -e \u0026#39;Cstack_info()[\u0026#34;size\u0026#34;]\u0026#39; size 9961472   这个值本应该等于10240*1024=10485760，但我们得到的9961472比这略小一些，在此不明原因。\n解决方案是手动设定C stack\n1  ulimit -s 102400   然后重新运行程序，即可解决\n参考来源 https://stackoverflow.com/questions/14719349/error-c-stack-usage-is-too-close-to-the-limit\nhttp://blog.sina.com.cn/s/blog_86c8fc120102w442.html\n","permalink":"https://sr-c.github.io/2019/02/16/Error-C-stack-usage-is-too-close-to-the-limit/","tags":["debug","R"],"title":"【debug】C stack usage is too close to the limit"},{"categories":null,"contents":"在basic plot中实现 1  minor.tick(nx=3, ny=3, tick.ratio=0.5)   ggplot实现 在ggplot中反而没有发现很便捷的解决方案，虽然在scale_x_continuous()函数中有minor_breaks参数，但并不是直接对应刻度线的设置。\n目前找到折中的解决方案是在指定breaks的同时指定lables，而lables在指定次级刻度时留空。其实，如此并没有设定次级刻度，只是在部分刻度上留空，视觉上看起来像是次级刻度罢了。具体方案如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  #首先设定一个函数 #replace every x element with an empty charactor every_nth \u0026lt;- function(x, nth, empty = TRUE, inverse = FALSE) { if (!inverse) { if(empty) { x[1:nth == 1] \u0026lt;- \u0026#34;\u0026#34; x } else { x[1:nth != 1] } } else { if(empty) { x[1:nth != 1] \u0026lt;- \u0026#34;\u0026#34; x } else { x[1:nth == 1] } } } # example library(ggplot2) df \u0026lt;- data.frame(x = rnorm(1000), y = rnorm(1000)) ## ggplot2 default axis labelling p \u0026lt;- ggplot(df, aes(x, y)) + geom_point() + theme_bw() ## Add minor ticks to axes custom_breaks \u0026lt;- seq(-3, 3, 0.25) p + scale_x_continuous(breaks = custom_breaks, labels = every_nth(custom_breaks, 4, inverse = TRUE)) + scale_y_continuous(breaks = custom_breaks, labels = every_nth(custom_breaks, 2, inverse = TRUE))   参考来源 https://www.cnblogs.com/hdu-2010/p/3833088.html\nhttps://stackoverflow.com/questions/34533472/insert-blanks-into-a-vector-for-e-g-minor-tick-labels-in-r/34533473#34533473\nhttps://www.cnblogs.com/hdu-2010/p/3833088.html\n","permalink":"https://sr-c.github.io/2019/02/16/ggplot-minor-tick/","tags":["R","ggplot2"],"title":"ggplot设置次级刻度"},{"categories":null,"contents":"Wide to long 1  gather(data, key, value, ..., na.rm = FALSE, convert = FALSE)    data: The dataset to be modified (in our case, seps) key: the name of the new “naming” variable (year) value: the name of the new “result” variable (value) na.rm: whether missing values are removed (this dataset doesn’t have any, so it isn’t a problem) convert: convert anything that seems like it should be in another format to that other format, e.g. numeric to numeric (since we used read_csv we don’t need this one either)  Long to wide 1  spread(data, key, value, fill = NA, convert = FALSE)   The format of this one is similar to gather():\n data: The data to be reformatted (inprogress) key: The column you want to split apart (Field) value: The column you want to use to populate the new columns (the value column we just created in the spread step) fill: what to substitute if there are combinations that don’t exist (not a problem here) convert: whether to fix incorrect data types as it goes (not a problem here)  参考来源 https://rstudio-pubs-static.s3.amazonaws.com/282405_e280f5f0073544d7be417cde893d78d0.html\nhttp://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/\nhttps://github.com/rstudio/cheatsheets/blob/master/data-import.pdf\nhttps://uc-r.github.io/tidyr\n","permalink":"https://sr-c.github.io/2019/02/15/using-tidyverse-to-go-between-wide-and-long/","tags":["R","tidyverse"],"title":"长短数据格式转换"},{"categories":null,"contents":"预防方法 在数据处理过程中，时常会自动将字符串转变为因子，这对于后续的操作可能会带来困难。为避免此类麻烦，可以在程序中声明禁止chr转为factor\n1  options(stringsAsFactors = FALSE) #禁止chr转成factor   解决方案 若已经出现了这样的问题，或数据就是如此，我们应该如何处理呢\n若直接使用as.numeric()函数则会将factor的不同level转换为对相应的数值，举例如下\n1 2 3 4 5 6 7 8  \u0026gt; directions \u0026lt;- c(\u0026#34;North\u0026#34;, \u0026#34;East\u0026#34;, \u0026#34;South\u0026#34;, \u0026#34;South\u0026#34;) \u0026gt; directions.factor \u0026lt;- factor(directions) \u0026gt; directions.factor [1] North East South South Levels: East North South # 直接转换会得到factor levels对应的numeric codes \u0026gt; as.numeric(directions.factor) [1] 2 1 3 3   若我们的factor本身就是数字，直接使用as.numeric()转换，则会丢失其原本的数值信息，而这往往不是我们想要的结果。因此，我们可以使用varhandle提供的unfactor()函数来方便地转换。\n1 2 3  # install.package(\u0026#34;varhandle\u0026#34;) libarary(varhandle) unfactor(your_factor_variable)   参考来源 https://stackoverflow.com/questions/3418128/how-to-convert-a-factor-to-integer-numeric-without-loss-of-information\nhttps://www.dummies.com/programming/r/how-to-convert-a-factor-in-r/\n","permalink":"https://sr-c.github.io/2019/02/15/convert-factor-to-numeric/","tags":["R","factor"],"title":"将factor转为numeric"},{"categories":null,"contents":"","permalink":"https://sr-c.github.io/2019/02/06/extract-rar/","tags":["rar","7zip"],"title":"解压缩rar文件"},{"categories":null,"contents":"配置中文支持 1  yum groupinstall \u0026#34;Chinese support\u0026#34; -y   安装git yum提供的git版本为1.7.1，推荐使用第三方源提供的版本，或自行编译。\n1 2 3 4 5 6 7 8 9 10 11 12 13  #yum remove git #yum -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm #yum -ivh https://centos6.iuscommunity.org/ius-release.rpm #yum --disablerepo=* --enablerepo=ius install git wget https://www.kernel.org/pub/software/scm/git/git-2.20.1.tar.xz tar Jxf git-2.20.1.tar.xz cd git-2.20.1 make prefix=/usr/local/git all sudo make prefix=/usr/local/git install echo \u0026#39;export PATH=$PATH:/usr/local/git/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc git --version   安装node.js 1 2 3 4 5 6  wget https://nodejs.org/dist/v4.4.4/node-v4.4.4-linux-x64.tar.xz tar Jxf node-v4.4.4-linux-x64.tar.xz mv node-v4.4.4 /opt/ echo \u0026#39;PATH=$PATH:/opt/node-v4.4.4\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc   安装hexo 1 2 3 4 5  git clone -b hexo https://github.com/用户名/仓库名.git cd \u0026lt;folder\u0026gt; npm install -g hexo-cli npm install hexo s   参考来源 https://www.cnblogs.com/007sx/p/6076151.html\n","permalink":"https://sr-c.github.io/2019/01/30/centos6-config/","tags":["CentOS"],"title":"CentOS6安装后配置"},{"categories":null,"contents":"传统的RNA-seq分析方法需要将reads比对到基因组上，然后再统计每个基因的counts，进而计算TPM/FPKM。现在，发展出了许多alignment-free的方法，不需要进行比对，直接统计出count矩阵。\n安装 salmon直接提供了linux的预编译版本，解压后可直接使用。\n1  wget https://github.com/COMBINE-lab/salmon/releases/download/v0.12.0/salmon-0.12.0_linux_x86_64.tar.gz   准备输入数据   参考转录组，即目标物种的cdna序列\n  比对文件(alignment-based mode)或测序文件(quasi-mapping-based mode)\n是的，salmon可以不经过比对，直接以测序数据作为输入\n  建立索引 1 2 3 4 5 6 7 8 9  salmon index -t transcripts.fasta -i transcripts_index --type quasi -k 31 # -t： 参考转录组路径，支持压缩文件 # -i： 设定index名称 # -type： 索引类型，分为fmd, quasi, 建议quasi # -k: k-mers的长度。说明文档中，若reads长度大于75 bp，取值31可获得较好的效果 salmon index -t transcripts.fasta -i transcripts_index # 全部使用默认参数也可创建索引    20190622 更新\n在0.14.0 新版本中，salmon引入了新的索引方式，且旧版本产生的索引不能在0.14.0版本后通用\n新的方式依赖于SalmonTools中提供的脚本，提供基因组，gtf注释与cds序列，生成decoys.txt与gentrome.fa\n1 2 3  bash /path/to/scripts/generateDecoyTranscriptome.sh -a ~/reference/annotation/mm/mm.gtf -g ~/reference/genome/mm/mm.fasta -t ~/reference/collection/mm/mm.cds.fa -o mm cd mm salmon index -t gentrome.fa -d decoys.txt -i ./    定量计算 1 2 3 4 5 6 7  salmon quant -i transcripts_index -l A -1 reads_1.fastq -2 reads_2.fastq -o transcripts_quant # -i： 上一步建立好的index路径 # -l/--libType: 文库类型，详细参见文档，可设置为A，使用软件自动适配 # -1： read1，支持压缩文件 # -2： read2，支持压缩文件 # -o： 输出目录   导出表达量矩阵 使用tximport导入或导出上游软件计算的表达量矩阵\ncount矩阵 1 2 3 4 5 6  files \u0026lt;- file.path(dir, \u0026#34;salmon\u0026#34;, samples$run, \u0026#34;quant.sf.gz\u0026#34;) names(files) \u0026lt;- paste0(\u0026#34;sample\u0026#34;, 1:6) txi.salmon \u0026lt;- tximport(files, type = \u0026#34;salmon\u0026#34;, tx2gene = tx2gene) head(txi.salmon$counts) DEGList(txi.salmon)   参考来源 https://salmon.readthedocs.io/en/latest/salmon.html\nhttps://www.jianshu.com/p/7564eca62354\nhttp://www.bioinfo-scrounger.com/archives/411\nhttp://blog.sciencenet.cn/blog-3334560-1084369.html\n","permalink":"https://sr-c.github.io/2019/01/24/salmon-transcript-quantification/","tags":["salmon","RNA-seq"],"title":"使用salmon计算RNA-seq表达量矩阵"},{"categories":null,"contents":"问题 1 2 3 4  require(ggplot2) d \u0026lt;- data.frame(x=c(0, 0.002, 0.00575), y = 1:3) p \u0026lt;- ggplot(d, aes(x, y)) + geom_point() + xlab(NULL) + ylab(NULL) print(p)   x轴最右端的文本0.006最后一个字符有一半过界了。\n解决方案 设定xlim 1  p + xlim(NA, 0.0062)   设定scale_x_continuous 1  p + scale_x_continuous(limits = c(0, 0.0062), expand = c(0,0))   在默认设置下，expand对于连续型变量， 会在取值区间的两侧添加5%的范围；对于离散型变量，会在取值区间的两侧添加0.6个单位。\n冲突 若同时设置xlim与scale_x_continuous，则后者会覆盖前者的设置。\n参考来源 https://guangchuangyu.github.io/cn/2017/04/ggplot2-overflow/\n","permalink":"https://sr-c.github.io/2019/01/17/ggplot-set-axis-limits/","tags":["ggplot"],"title":"【ggplot】设置坐标轴区域"},{"categories":null,"contents":"安装 1 2 3  install.package(\u0026#34;RIdeogram\u0026#34;) # 直接安装出现报错，需要依赖rsvg包 conda install -c conda-forge librsvg r-rsvg   使用 准备输入文件  karyotype.csv 核型文件，必须。包含5列  第1列 染色体名 第2, 3列 染色体起止位置(0-base) 第4, 5列 着丝粒起止位置(可省略)   heatmap.csv 染色体heatmap，可选。包含4列  第1列 染色体名 第2, 3列 染色体指定位置 第4列 该位置对应的数值   mark.csv 染色体旁形状标记，可选。包含6列  第1列 染色体名 第2列 形状(box, triangle, circle) 第3列 染色体名 第4, 5列 染色体指定位置 第6列 颜色    绘制 1 2 3 4 5 6 7 8 9 10 11 12  ideogram(karyotype = karyotype, #如果没提供easy_input_heatmap.csv，就在下面两行前面加# overlaid = gene_density, #染色体上heatmap的配色 colorset1 = c(\u0026#34;forestgreen\u0026#34;, \u0026#34;gold\u0026#34;, \u0026#34;deeppink3\u0026#34;), #如果没提供easy_input_mark.csv，就在下面这行前面加# label = Random_RNAs_500, #染色体宽度，默认170 width = 200, #图例的位置 Lx = 160, #图例左上角跟左边的距离 Ly = 20) #图例左上角跟顶端的距离   格式转换 1 2 3 4  #svg2tiff, svg2jpg, svg2png函数设定类似 svg2pdf(\u0026#34;chromosome.svg\u0026#34;, width = 12, height = 8, #设定画布大小 dpi = 300)   参考来源 https://cran.r-project.org/web/packages/RIdeogram/vignettes/RIdeogram.html\n","permalink":"https://sr-c.github.io/2019/01/16/RIdeogram/","tags":["R"],"title":"RIdeogram"},{"categories":null,"contents":"分组摘要 摘要函数summaize()结合分组函数group_by()能够很方便地得到统计结果。\n结合使用管道符号%\u0026gt;%，可以省却很多变量命名的麻烦，使代码更加简洁。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # 分组，摘要，结果过滤一步完成 delays \u0026lt;- flights %\u0026gt;% group_by(dest) %\u0026gt;% summarise( count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE) ) %\u0026gt;% filter(count \u0026gt; 20, dest != \u0026#34;HNL\u0026#34;) ggplot(data = delay, mapping = aes(x = dist, y = delay)) + geom_point(aes(size = count), alpha = 1/3) + geom_smooth(se = FALSE) #\u0026gt; `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;   计数 在整合数据时，使用计数函数n()或计数所有非缺失值sum(is.na(x))常常很有帮助。这使得我们能够查看数据是否来自一些非常小的样本。\n1 2 3 4 5 6 7 8 9 10 11  not_cancelled \u0026lt;- flights %\u0026gt;% filter(!is.na(dep_delay), !is.na(arr_delay)) # 按照航班机尾编号进行分组 delays \u0026lt;- not_cancelled %\u0026gt;% group_by(tailnum) %\u0026gt;% summarise( delay = mean(arr_delay) ) ggplot(data = delays, mapping = aes(x = delay)) + geom_freqpoly(binwidth = 10)   有一些航班的延误时间长达300 min，这是我们需要查看一下样本数量，排除偶然偏差的影响。\n绘制一个航班数量对平均延误时间的散点图能够帮助我们直观地理解\n1 2 3 4 5 6 7 8 9  delays \u0026lt;- not_cancelled %\u0026gt;% group_by(tailnum) %\u0026gt;% summarise( delay = mean(arr_delay, na.rm = TRUE), n = n() ) ggplot(data = delays, mapping = aes(x = n, y = delay)) + geom_point(alpha = 1/10)   可以见到，在样本数量较少时，平均延误时间的偏差很大。\n这种情况下，过滤掉最小的哪些观察值能够帮助我们发现规律。\n1 2 3 4  delays %\u0026gt;% filter(n \u0026gt; 25) %\u0026gt;% ggplot(mapping = aes(x = n, y = delay)) + geom_point(alpha = 1/10)   常用的摘要函数   位置度量\nmean(x)\nmedian(x)\n  分散程度度量\nsd(x), 标准差\nIQR(x), 四分位距\nmad(x), 绝对中位差\n  秩的度量\nmin(x), quantile(x, 0.25), max(x)\n  定位度量\nfirst(x), nth(x, 2), last(x)\n1 2 3 4 5 6  not_cancelled %\u0026gt;% group_by(year, month, day) %\u0026gt;% summarise( first_dep = first(dep_time), last_dep = last(dep_time) )     计数\nn()不需要任何参数，返回当前分组的大小\nsum(is.na(x))统计非缺失值的数量\nn_distinct(x)计算唯一值的数量\n  逻辑值的计数和比例\nsum(x \u0026gt; 10)和mean(y == 0)\n  按多个变量分组 当使用多个变量分组时，每次摘要统计会消耗一个分组变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  daily \u0026lt;- group_by(flights, year, month, day) (per_day \u0026lt;- summarise(daily, flights = n())) #\u0026gt; # A tibble: 365 x 4 #\u0026gt; # Groups: year, month [?] #\u0026gt; year month day flights #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 2013 1 1 842 #\u0026gt; 2 2013 1 2 943 #\u0026gt; 3 2013 1 3 914 #\u0026gt; 4 2013 1 4 915 #\u0026gt; 5 2013 1 5 720 #\u0026gt; 6 2013 1 6 832 #\u0026gt; # … with 359 more rows (per_month \u0026lt;- summarise(per_day, flights = sum(flights))) #\u0026gt; # A tibble: 12 x 3 #\u0026gt; # Groups: year [?] #\u0026gt; year month flights #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 2013 1 27004 #\u0026gt; 2 2013 2 24951 #\u0026gt; 3 2013 3 28834 #\u0026gt; 4 2013 4 28330 #\u0026gt; 5 2013 5 28796 #\u0026gt; 6 2013 6 28243 #\u0026gt; # … with 6 more rows (per_year \u0026lt;- summarise(per_month, flights = sum(flights))) #\u0026gt; # A tibble: 1 x 2 #\u0026gt; year flights #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 2013 336776   取消分组 ungroup()\n参考来源 https://r4ds.had.co.nz/transform.html#grouped-summaries-with-summarise\n","permalink":"https://sr-c.github.io/2019/01/12/R4ds-exercises-2/","tags":["R4ds","ggplot"],"title":"R4ds-exercises-2"},{"categories":null,"contents":"安装 软件推荐使用pip安装，但是作为普通用户，还是选择使用conda安装\n1 2 3 4 5  conda create -n jcvi python=2.7 source activate jcvi # 安装前需要确认存在bioconda和r channel，安装前先更新conda conda --add-channel r conda install jcvi last   后续还可能会由于缺少scipy在绘图时报错ImportError: No module named scipy.spatial，因此可提前安装\n1 2 3  # 安装scipy #conda-forge scipy-1.2.0 conda install scipy   准备数据 jcvi支持直接从phytozome下载数据\n1  python -m jcvi.apps.fetch phytozome   按照名称查看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  python -m jcvi.apps.fetch phytozome ... Acoerulea Alyrata Athaliana Bdistachyon Brapa Cclementina Cpapaya Creinhardtii Crubella Csativus Csinensis Csubellipsoidea_C-169 Egrandis Fvesca Gmax Graimondii Lusitatissimum Mdomestica Mesculenta Mguttatus Mpusilla_CCMP1545 Mpusilla_RCC299 Mtruncatula Olucimarinus Osativa Ppatens Ppersica Ptrichocarpa Pvirgatum Pvulgaris Rcommunis Sbicolor Sitalica Slycopersicum Smoellendorffii Stuberosum Tcacao Thalophila Vcarteri Vvinifera Zmays early_release   常见的植物物种都可以直接下载\n1 2 3 4 5 6 7 8  # download sequences and coordinates of grape and peach python -m jcvi.apps.fetch phytozome Vvinifera,Ppersica # convert the GFF to BED file python -m jcvi.formats.gff bed --type=mRNA --key=Name Vvinifera_145_gene.gff3.gz -o grape.bed python -m jcvi.formats.gff bed --type=mRNA --key=Name Ppersica_139_gene.gff3.gz -o peach.bed # clean headers to remove description fiedls from Phytozome FASTA files. python -m jcvi.formats.fasta format --sep=\u0026#34;|\u0026#34; Vvinifera_145_cds.fa.gz grape.cds python -m jcvi.formats.fasta format --sep=\u0026#34;|\u0026#34; Ppersica_139_cds.fa.gz peach.cds   共线性分析 1 2 3  python -m jcvi.compara.catalog ortholog grape peach ls grape.peach.* grape.peach.lifted.anchors grape.peach.anchors grape.peach.last.filtered grape.peach.last grape.peach.pdf   程序默认按照可用线程，调用全部进行LAST比对，速度很快。\n可视化 点阵图 目前版本0.8.12在共线性分析后，自动绘制点阵图，手动运行方式如下\n1  python -m jcvi.graphics.dotplot grape.peach.anchors   在两者的点阵图中，不论水平方向或者竖直方向查看，都可以发现不超过3次的共线性区域。这说明，🍇和🍑之间发生了基因组三倍体化的事件，使得出现了这样3:3的模式。\n如果仔细观察，还可以发现3个共线性区域中常常有一个信号更强，对应着两个基因组之间的直系同源区域。如果我们只想要得到这些1:1直系同源的区域呢？我们只需要重复之前的比对，同时加上选项--cscore=.99即可。C-score是由LAST比对区域到BLAST比对区域的比值确定。(C-score is defined by the ratio of LAST hit to the best BLAST hits to either the query and hit. )0.99的C-score阈值有效地过滤LAST比对结果，从而得到最佳相互比对结果(reciprocal best hit, RBH)。\n1 2 3  rm grape.peach.last.filtered python -m jcvi.compara.catalog ortholog grape peach --cscore=.99 python -m jcvi.graphics.dotplot grape.peach.anchors   还可以手动查看synteny depth分布\n1  python -m jcvi.compara.synteny depth --histogram grape.peach.anchors   共线型图 同样由grape.peach.anchors生成，除了BED和共线性文件，还需要手动设置两个文件用于绘图\n  seqids设定绘制哪些染色体，一般会去掉较小和未定位的scaffolds，下方设定了19个🍇的染色体与8个🍑的染色体\n1 2  chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19 scaffold_1,scaffold_2,scaffold_3,scaffold_4,scaffold_5,scaffold_6,scaffold_7,scaffold_8     layout设定画板上如何绘制。整体上canvas绘图的x,y轴的定位在0-1之间，前三行设定了染色体轨道的位置，循环，颜色，标签，垂直排列以及对应的BED文件。下方的track0就是🍇，track1就是🍑。下一节就是设定在哪些tracks之间绘制共线性关系。e,0,1代表着绘制track0与track1之间的共线性，使用grape.peach.anchors.simple文件中的详细信息。\n1 2 3 4 5  # y, xstart, xend, rotation, color, label, va, bed .6, .1, .8, 0, , Grape, top, grape.bed .4, .1, .8, 0, , Peach, top, peach.bed # edges e, 0, 1, grape.peach.anchors.simple     注意，这两个设定文件末尾不能够存在多余的空行，否则会报错#56\n1  LayoutLine\u0026#39; object has no attribute \u0026#39;color\u0026#39;   从.anchors文件中产生更简要的.simple文件  1  python -m jcvi.compara.synteny screen --minspan=30 --simple grape.peach.anchors grape.peach.anchors.new   绘图\n1  python -m jcvi.graphics.karyotype seqids layout   局部可视化 首先从基因水平提取匹配的区块\n1  python -m jcvi.compara.synteny mcscan grape.bed grape.peach.lifted.anchors --iter=1 -o grape.peach.i1.blocks   参数--iter=1表明从每个🍇区域中提取1个最佳匹配区域。若将--iter设置为2，那么每个🍇区域就会有2个对应的🍑区域。这一点对于基因组重复区域的绘制很有帮助。\n目前grape.peach.i1.blocks中包含有很多局部区域，我们可以手动选择需要绘制的部分。\n1  head -50 grape.peach.i1.blocks \u0026gt; blocks   最终，我们仍然需要一个layout文件进行绘图，blocks.layout文件如下\n1 2 3 4 5  # x, y, rotation, ha, va, color, ratio, label 0.5, 0.6, 0, left, center, m, 1, grape Chr1 0.5, 0.4, 0, left, center, #fc8d62, 1, peach scaffold_1 # edges e, 0, 1   然后，我们就可以进行绘制了\n1 2  cat grape.bed peach.bed \u0026gt; grape_peach.bed python -m jcvi.graphics.synteny blocks grape_peach.bed blocks.layout   参考来源 https://github.com/tanghaibao/jcvi/wiki/MCscan-%28Python-version%29\nhttps://www.jianshu.com/p/39448b970287\n","permalink":"https://sr-c.github.io/2019/01/11/jcvi-MCscan/","tags":["jcvi","MCscan"],"title":"使用jcvi进行MCscan可视化"},{"categories":null,"contents":"统计变换   What is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function?\n统计变换函数stat_summary()的默认几何对象函数是什么？如何使用几何对象函数重绘上述图片？\ngeom_pointrange()由于其stat的默认值为identity，需要数据中包含ymin与ymax。但数据中没有，简便起见，我们通过修改stat为summary实现。\n1 2 3 4 5 6  ggplot(data = diamonds) + geom_pointrange( mapping = aes(x = cut, y = depth), stat = \u0026#34;summary\u0026#34; ) \u0026gt;# No summary function supplied, defaulting to `mean_se()   结果出图如下\n这是由于summary的默认输出使用mean与sd绘制点线图，所以还需要手动设置fun.ymin，fun.ymax与fun.y的值：\n1 2 3 4 5 6 7 8  ggplot(data = diamonds) + geom_pointrange( mapping = aes(x=cut, y=depth), stat = \u0026#34;summary\u0026#34;, fun.ymin = min, fun.ymax = max, fun.y = median )     What does geom_col() do? How is it different to geom_bar()?\ngeom_col()函数有什么功能？它与geom_bar()函数的区别是什么？\ngeom_bar()函数使用每个group中的数量来计算条形图的高度。geom_col()默认使用stat_identity()作为统计变换函数，geom_bar()则默认使用stat_count()作为统计变换函数。\n  Most geoms and stats come in pairs that are almost always used in concert. Read through the documentation and make a list of all the pairs. What do they have in common?\n大多数的几何对象函数与统计变换函数都是成对出现的。阅读说明文档，给出这些列表，它们有哪些共同之处？ ggplot2::geom_abline\tReference lines: horizontal, vertical, and diagonal ggplot2::geom_bar\tBar charts ggplot2::geom_bin2d\tHeatmap of 2d bin counts ggplot2::geom_blank\tDraw nothing ggplot2::geom_boxplot\tA box and whiskers plot (in the style of Tukey) ggplot2::geom_contour\t2d contours of a 3d surface ggplot2::geom_count\tCount overlapping points ggplot2::geom_density\tSmoothed density estimates ggplot2::geom_density_2d\tContours of a 2d density estimate ggplot2::geom_dotplot\tDot plot ggplot2::geom_errorbarh\tHorizontal error bars ggplot2::geom_hex\tHexagonal heatmap of 2d bin counts ggplot2::geom_freqpoly\tHistograms and frequency polygons ggplot2::geom_jitter\tJittered points ggplot2::geom_crossbar\tVertical intervals: lines, crossbars \u0026amp; errorbars ggplot2::geom_map\tPolygons from a reference map ggplot2::geom_path\tConnect observations ggplot2::geom_point\tPoints ggplot2::geom_polygon\tPolygons ggplot2::geom_qq_line\tA quantile-quantile plot ggplot2::geom_quantile\tQuantile regression ggplot2::geom_ribbon\tRibbons and area plots ggplot2::geom_rug\tRug plots in the margins ggplot2::geom_segment\tLine segments and curves ggplot2::geom_smooth\tSmoothed conditional means ggplot2::geom_spoke\tLine segments parameterised by location, direction and distance ggplot2::geom_label\tText ggplot2::geom_raster\tRectangles ggplot2::geom_violin\tViolin plot\n  What variables does stat_smooth() compute? What parameters control its behaviour?\nstat_smooth()函数计算了什么变量？什么参数控制它的行为？ |Computed variables| | | \u0026mdash;- | \u0026mdash;- | | y | predicted value | | ymin | lower pointwise confidence interval around the mean | | ymax | upper pointwise confidence interval around the mean | | se | standard error |\n  In our proportion bar chart, we need to set group = 1. Why? In other words what is the problem with these two graphs?\n在我们部分的条形图中，需要设置group = 1, 这是为什么呢？\n   参考来源 https://r4ds.had.co.nz/data-visualisation.html#exercises-4\nhttps://jrnold.github.io/r4ds-exercise-solutions/data-visualisation.html#statistical-transformations\n","permalink":"https://sr-c.github.io/2019/01/08/R4ds-exercises-1/","tags":["R","ggplot2"],"title":"R4ds-exercises-1"},{"categories":null,"contents":"使用基本的pie()函数能够直接画出饼状图\n1  pie(x, labels, radius, main, col, clockwise)   简单方案 1 2 3 4 5 6 7 8 9 10 11 12  # Create data for the graph. x \u0026lt;- c(11, 30, 39, 20) labels \u0026lt;- c(\u0026#34;70后\u0026#34;, \u0026#34;80后\u0026#34;, \u0026#34;90后\u0026#34;, \u0026#34;00后\u0026#34;) # Give the chart file a name. png(file = \u0026#34;birth_of_age.jpg\u0026#34;) # Plot the chart. pie(x,labels) # Save the file. dev.off()   ggplot2实现 1 2 3 4 5  library(ggplot2) dt = data.frame(A = c(2, 7, 4, 10, 1), B = c(\u0026#39;B\u0026#39;,\u0026#39;A\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;D\u0026#39;,\u0026#39;E\u0026#39;)) dt = dt[order(dt$A, decreasing = TRUE),] myLabel = as.vector(dt$B) myLabel = paste(myLabel, \u0026#34;(\u0026#34;, round(dt$A / sum(dt$A) * 100, 2), \u0026#34;%)\u0026#34;, sep = \u0026#34;\u0026#34;) p = ggplot(dt, aes(x = \u0026#34;\u0026#34;, y = A, fill = B)) + geom_bar(stat = \u0026#34;identity\u0026#34;, width = 1) + coord_polar(theta = \u0026#34;y\u0026#34;) + labs(x = \u0026#34;\u0026#34;, y = \u0026#34;\u0026#34;, title = \u0026#34;\u0026#34;) + theme(axis.ticks = element_blank()) + theme(legend.title = element_blank(), legend.position = \u0026#34;top\u0026#34;) + scale_fill_discrete(breaks = dt$B, labels = myLabel) + theme(axis.text.x = element_blank()) + geom_text(aes(y = A/2 + c(0, cumsum(A)[-length(A)]), x = sum(A)/20, label = myLabel), size = 5) ## 在图中加上百分比：x 调节标签到圆心的距离, y 调节标签的左右位置  p   待解决 如何将需重点突出的扇形区块位移，做出如下的形式\n参考来源 https://www.yiibai.com/r/r_pie_charts.html\nhttps://blog.csdn.net/Bone_ACE/article/details/47455363\nhttps://zhuanlan.zhihu.com/p/26745812\n","permalink":"https://sr-c.github.io/2019/01/02/R-pie-chart/","tags":["R"],"title":"【R】饼图"},{"categories":null,"contents":"treemap 安装 1  install.packages(\u0026#34;treemap\u0026#34;)   读取数据 1  contig_length \u0026lt;- read.table(\u0026#34;genome_length.txt\u0026#34;, header = FALSE, sep = \u0026#39;\\t\u0026#39;)   画图 1  treemap(contig_length, index = \u0026#34;V1\u0026#34;, vSize = \u0026#34;V2\u0026#34;, palette = \u0026#34;Reds\u0026#34;, title = \u0026#34;Contig length of xxx genome\u0026#34;)   指定每格的颜色与边框宽度\n1 2 3 4  #添加一列作为颜色列，为每一行指定颜色 OS_contig_length$V3 \u0026lt;- rep(\u0026#34;#ffffff\u0026#34;, 23) #指定type为color,不显示index treemap(OS_contig_length, index = \u0026#34;V1\u0026#34;, vSize = \u0026#34;V2\u0026#34;, vColor = \u0026#34;V3\u0026#34;, type = \u0026#34;color\u0026#34;, border.lwds = 1, title = \u0026#34;Contig length of XXX genome\u0026#34;, fontsize.labels = 0)   treemapify 使用treemap绘图产生的图形不是ggplot对象，而trrmapify则解决了这一问题。\n安装 1  install.packages(\u0026#34;treemapify\u0026#34;)   使用 1 2 3 4 5 6 7  library(ggplot2) library(treemapify) #使用treemapify自带的示例数据G20 ggplot(data = G20, aes(area = gdp_mil_usd) # 输入的映射中，只有area为必须 ） + geom_treemap()   调整 默认的输出为黑色背景，这一点可以手动赋值给fill映射修改\n1 2  ggplot(G20, aes(area = gdp_mil_usd)) + geom_treemap(fill=\u0026#34;white\u0026#34;)   还可以向fill映射另一个变量\n1 2 3  ggplot(G20, aes(area = gdp_mil_usd, fill = hdi)) + geom_treemap() + scale_fill_distiller(palette=\u0026#34;Greens\u0026#34;) #使用内置的配色方案   使用geom_treemap_text添加标签，不支出ggplot通用的geom_text()函数\n1 2 3 4  ggplot(G20, aes(area = gdp_mil_usd, fill = hdi, label = country)) + geom_treemap() + geom_treemap_text(fontface = \u0026#34;italic\u0026#34;, colour = \u0026#34;red\u0026#34;, place = \u0026#34;centre\u0026#34;,grow = TRUE,alpha=.6)+ scale_fill_distiller(palette=\u0026#34;Greens\u0026#34;)   参考来源 https://www.r-bloggers.com/simple-steps-to-create-treemap-in-r/\nhttps://rdrr.io/cran/treemap/man/treemap.html\nhttps://rpubs.com/brandonkopp/creating-a-treemap-in-r\nhttps://cran.r-project.org/web/packages/treemap/treemap.pdf\nhttps://github.com/wilkox/treemapify\nhttps://zhuanlan.zhihu.com/p/29834340\n","permalink":"https://sr-c.github.io/2018/12/31/R-treemap/","tags":["treemap","treemapify","R"],"title":"绘制treemap"},{"categories":null,"contents":"  参考来源 ","permalink":"https://sr-c.github.io/2018/12/30/StatQuest-Logs/","tags":[],"title":"StatQuest-Logs"},{"categories":null,"contents":"allele 等位基因，对偶基因\na role of tumble 习惯法则\nstandard diversion\nnormal distribution 正态分布/高斯分布\nCentral Limit Theorem 中心极限定理\nEven if you are not normal, the average is normal.\nethnicity 种族特点，种族渊源\nget more bang for your buck 花最少的钱得到最大的效果\nevery now and then 有时会发生，但很少发生\nbe characterized by 表征；有\u0026hellip;特征\n","permalink":"https://sr-c.github.io/2018/12/29/StatQuest-cihui/","tags":["StatQuest"],"title":"StatQuest词汇表"},{"categories":null,"contents":"LTRpred实际是调用GenomeTools中封装的LTRharvest对基因组序列进行de novo 预测，并使用LTRdigest开展分析，并包含了一些可视化的函数。此外，LTRpred还包括了LTRpred.meta()函数，方便调用多个基因组进行meta分析。\n软件安装 环境依赖 1 2 3 4 5 6 7  #conda安装GenomeTools依赖2.7版本的python conda create -n LTRpred python=2.7 source activate LTRpred conda install hmmer genometools vsearch dfam conda install -c bioconda blast #Install USEARCH from git or direct download   程序安装 此时R的默认版本仍为3.4.3，仍然使用bioconductor 3.7的方法安装\n1 2 3 4 5 6 7 8 9 10  # install Bioconductor ## try https:// if http:// URLs are not supported source(\u0026#34;http://bioconductor.org/biocLite.R\u0026#34;) biocLite() ## install prerequisite Bioconductor packages biocLite(c(\u0026#34;GenomicFeatures\u0026#34;,\u0026#34;IRanges\u0026#34;, \u0026#34;AnnotationDbi\u0026#34;, \u0026#34;Biostrings\u0026#34;)) ## install LTRpred as follows source(\u0026#34;http://bioconductor.org/biocLite.R\u0026#34;) biocLite(\u0026#34;devtools\u0026#34;) biocLite(\u0026#34;HajkD/LTRpred\u0026#34;)   Quick Start LTRpred自带人Y染色体序列作为输入，实验一下程序的运行\n1 2 3 4 5 6 7 8  # load LTRpred package library(LTRpred) # de novo LTR transposon prediction for the Human Y chromosome LTRpred( genome.file = system.file(\u0026#34;Hsapiens_ChrY.fa\u0026#34;, package = \u0026#34;LTRpred\u0026#34;), cluster = TRUE, cores = 4 )   程序输出 LTRperd()会将结果输出在*_ltrpred文件夹中\n参考来源 https://hajkd.github.io/LTRpred/\nhttps://hajkd.github.io/LTRpred/articles/Introduction.html\n","permalink":"https://sr-c.github.io/2018/12/22/LTRpred/","tags":["LTRpred"],"title":"LTRpred"},{"categories":null,"contents":"Aging LTR insertion 使用LTRharvest与c计算LTR-RTs在基因组中的插入时间。\nLTRharvest LTRharvest是Genome tools中进行de novo识别LTR的组件。不依赖于序列相似性，根据LTR-RT的结构特点进行识别。\n1 2 3 4 5 6  #LTRharvest并不会根据输入的序列名进行命名，而根据序列在fasta文件中的顺序在输出中命名为seq0，seq1... #因此可手动将输入文件中的scaffold整合为一条，方面后续根据gff文件输出LTR序列 gt suffix -db \u0026lt;path-to-your-genome\u0026gt; -index \u0026lt;genome-index\u0026gt; gt ltrharvest -index \u0026lt;genome-index\u0026gt; -motif TGCA -motifmis 1 -gff3 out/genome.harvest.gff -out out/genome.harvest.out \u0026gt; genome.harvest.scn gt ltrharvest -index \u0026lt;genome-index\u0026gt; -gff3 out/genome.harvest.nonTGCA.gff -out out/genome.harvest.nonTGCA.out \u0026gt; genome.harvest.nonTGCA.scn   得到输出的结果中往往存在着数千条LTR-RTs，但其中存在着许多的假阳性。距离相近的其他类型重复序列，若其相似性较高，往往也会被识别为LTR-RT。LTR-retriever能够过滤这一结果，降低假阳性，得到完整的LTR-RT。\nLTR-retriever LTR-retriever能够识别LTRharvest, LTR_Finder或MGEScan_LTR的输出，对结果进行过滤，得到完整的LTR-RT。 LTR-retriever基于的假设是，LTR-RT在新插入基因组序列时，两端的LTR序列是相同的，同时，LTR序列两端的序列是不同的，以此来判断LTR序列的边界。 LTR-retriever计算LTR序列间的距离，基于物种的突变速率（默认使用水稻的）计算LTR的插入时间。基于bin/LTR.indefinder.pl使用JK法计算。\n计算DNA序列间的距离 最简单的算法是Jukes-Cantor 模型，默认ACGT四种碱基之间的替换速率相同。 $$ d_{AB}=-\\frac{3}{4}\\ln\\left( 1-\\frac{4}{3}f_{AB}\\right) $$\n where fABis the dissimilarity (fraction of observed differences) between sequences A and B, and dABis the estimated evolutionary distance (fraction of expected substitutions) between sequences A and B.\n 实际上，由于嘧啶与嘌呤之间的替换比同种碱基之间的替换更为更为困难，将transitions与transversions分开考虑，则使用的是Kimura 2-parameter 模型 $$ d_{AB}=-\\frac{1}{2}\\ln \\left [ \\left ( 1-2P-Q \\right )\\sqrt{1-2Q} \\right ] $$\n where P is the fraction of sequence positions differing by a transition and Q is the fraction of sequence positions differing by a transversion.\n 计算LTR插入时间 基于LTR-RT两端5'LTR与3'LTR序列之间的距离K，我们可以计算该LTR-RT在插入基因组后所经历的时间T。 $$ K=-\\frac{1}{2}\\ln \\left [ \\left ( 1-2P-Q \\right )\\sqrt{1-2Q} \\right ] $$\n其中，P为transitions的占比，Q为transversions的占比。\n基于\\(T=\\frac{K}{2\\mu} \\)我们可以很容易地由K值计算得到T，其中μ为目标物种的自然变异速率（水稻的变异速率为1.3e-8 bp-1·year-1）。\n参考来源 https://github.com/SIWLab/Lab_Info/wiki/Ageing-LTR-insertions\nhttp://bioinformatics.psb.ugent.be/downloads/psb/Userman/treecon_distance.html\n","permalink":"https://sr-c.github.io/2018/12/22/Aging-LTR-insert-time/","tags":["LTR","LTRharvest","LTR-retriever"],"title":"计算基因组中LTR的插入时间"},{"categories":null,"contents":"转座因子 几乎所有基因组内，都存在着转座因子(transposable element)或称转座子(transposon)。它们是基因组中可移动的DNA序列，可由基因组内的一个位置移至另一个位置。\n转座子不利用其他元件（如噬菌体或质粒DNA）就可完成基因组内的移动。在真核生物和原核生物中，都已返现以DNA形式移动的转座子。每种转座子都携带棉麻其滋生转座所需的酶活性基因。\n转座子可分为两大类：\n  一类因子，或称反转录因子\n1.1这类转座子在总体结构和转座机制上都通常与反转录病毒高度相似，称为LTR反转录转座子(LTR retrotransposon)\n1.2另一类成为非LTR反转录转座子(non-LTR retrotransposon)\n  二类因子，或称DNA型因子\n  酵母Ty因子 酵母中存在5中类型的Ty因子，所有因子都是反转录转座子，具有特征性的LTR与gag和pol基因，可以分为Ty1/copia因子类和Ty3/gypsy因子类，每一类在简化遗传上是独特的，并含有特征性顺序的可读框。\n在大多数情况下，Ty1因子的转座效率大大低于细菌转座子，约为10-7~10-8。许多压力因素，如诱变剂和营养缺乏，可提高转座发生率。\nTy1因子的一般组织结构如图所示，每个因子长5.9 kb，末端的334 bp组成了LTR，历史上称为δ因子。\n尽管单一Ty1因子的两个重复序列可能是相同或至少是非常相关的，但LTR序列也表现出很大的异源性。与Ty1因子相连的LTR序列保守型远远大于单一的LTR因子。这是因为Ty1因子的转座就像反转录病毒的复制一样，它包括LTR的倍增。所有，近期掺入的因子携带同样的LTR，而随着随机突变，在很长时间后，独立的LTR就会发生趋异。\nLTR-RTs的结构 LTR-RT (LTR retrotransposon) 结构的主要特征是75 ~ 5000 bp的长末端重复区域。在5\u0026rsquo;端LTR和3‘端LTR中间的区域被称为中间区域(the internal region)，其中编码了转座需要的蛋白。每一个LTR的末端都有双碱基的motif，大多数情况下都是5\u0026rsquo;-TG..CA-3\u0026rsquo;, 在其他非典型的motif也有在一些物种中被发现。LTR末端motif的侧翼就是TSD(target site duplication)，这是由整合酶(integrase)剪切得到的交错切口产生的。TSDs的长度在3-6 bp之间，在植物中一般为5 bp。根据TSD产生的机制，5’和3‘端的TSD应当完全一致。\nLTR-retriever LTR-RT是大多数植物基因组中转座子的最大组成部分。由于LTR-RT序列的多样性，基于序列同源性的方法来识别LTR-RT不是一个有效的方式。但是，LTR-RTs在不同物种之间其元件的结构是保守的。基于这些结构的特性，很多软件都能够识别LTR-RTs。这些软件识别LTR-RT的效率都很敏感，但是其准确性并不高。作者认为5’LTR与3‘LTR的前后区域不应该相同，以此来界定LTR的边界。\n参考来源 基因X\n","permalink":"https://sr-c.github.io/2018/12/13/LTR-RT/","tags":["LTR","LTR-retriever"],"title":"LTR-RT"},{"categories":null,"contents":"许多人认为p值代表了可能性/概率(probability)，p-value确实与probability相关，但并不是一个意思。举例如下\n抛硬币 试验：已有一枚均匀的硬币，抛硬币两次。\n提问：试验中我们得到两次正面的概率是多少，得到两次正面的p值是多少？\n每次抛硬币，我们得到正面(head, H)与反面(tail, T)的概率都是0.5, 因此两次抛硬币我们有4种可能性，而且是等概率的。\n HH, HT, TH, TT\n 因此，我们计算两次正面(HH)概率的方法就是 $$ \\frac{HH发生的次数}{总事件数} = \\frac{HH}{HH, HT, TH, TT} = \\frac{1}{4} = 0.25 $$ 两次反面(TT)的概率同理，但一次正面一次反面的概率如何呢？\n$$ \\frac{HT或TH发生的总次数}{总事件数} = \\frac{HT, TH}{HH, HT, TH, TT} = \\frac{2}{4} = 0.5 $$\n注意：这种情况下是我们并不在意事件发生的顺序，这在试验抽样时也常常如此。我们并不在意抽样时的顺序。\n试验的例子 试验：假设H代表一个基因的等位基因(allele), T代表另一个等位基因。若试验中父本与母本的这个基因都是是杂合子(heterozygote)\n提问：\n  求后代的基因型。\n  若母本是H的纯合子，求后代的基因型。\n  这个事件的概率与抛两次硬币的概率类似。\np-value  A p-value is the probability that random chance generated the data, or something else that is equal or rarer.\n 根据上述定义，p-value 由三部分组成，随机过程产生目标数据的概率，等同于该事件概率的其他事件，以及比目标事件更小概率发生的事件。这三者之和构成了该事件的p-value\n Part1: A p-value is the probability that random chance generated the data Part2: or something else that is equal Part3: or rarer  计算HH发生的p值时，这三个部分如下：\nPart1：P(HH) = 0.25\nPart2: P(TT) = 0.25\nPart3: 0\n因此，p-value = 0.25 + 0.25 + 0 = 0.5\nBAM!\n抛5次硬币 HHHHH的概率为1/32 = 0.0375，而p-value则是$1/32 + 1/32 + 0 = 1/16 = 0.0625 \u0026gt; 0.05$. 此例中，虽然发生的概率小于0.05, 但p-value大于0.05, 因此我们认为事件HHHHH的发生并不是unusual\nHHHHT的概率为5/32,（不考虑事件发生的顺序）, p-value为$5/32 + 5/32 + (1/32 + 1/32) = 3/8 = 0.375$\n测量身高 身高的密度分布为正态分布，举例如下\n![019-03-16 (2)](C:\\Users\\Yaman\\OneDrive\\图片\\屏幕快照\\2019-03-16 (1).png)\n测量一个人的身高在142 cm的p-value是0.025(half of the \u0026ldquo;equal to or rarer\u0026rdquo;) + 0.025(the other half of the \u0026ldquo;equal to or rarer\u0026rdquo;) = 0.05\n测量一个人的身高在155.4~156 cm之间的p-value是 0.04 + 0.48 + 0.48 = 1, 因此即使测量出这个事件的概率很低(0.04)，但其p-value为1，意味着发生这件事并不特殊。\n There\u0026rsquo;s nothing special about measuring someone who has the average height even though that particular event is relatively rare.\n DOUBLE BAM!\n参考来源 https://www.youtube.com/watch?v=5Z9OIYA8He8\u0026amp;list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9\u0026amp;index=16\n","permalink":"https://sr-c.github.io/2018/12/08/StatQuest-p-value/","tags":["StatQuest"],"title":"【StatQuest】P值"},{"categories":null,"contents":"DNA单碱基的替换有两种类型：\n Transition是双环结构的嘌呤(A↔G) 或单环结构的嘧啶(C↔T)之间的替换 Transversion则是嘌呤与嘧啶之间的替换  尽管transversions的替换类型数目两倍于transitions，但由于两种类型变异的分子机制不同，transitions出现的频率高于transversions。\n两个同源DNA序列之间transition/transversion的比值总体近似于2，但在编码区，这一比值往往更高。这是由于transitions不太可能导致氨基酸的替换（参见密码子表）出现致死突变，因此更可能作为SNPs而保留下来。\n参考来源 https://www.mun.ca/biology/scarr/Transitions_vs_Transversions.html\nhttp://rosalind.info/glossary/transitiontransversion-ratio/\n","permalink":"https://sr-c.github.io/2018/12/07/Transition-vs-Transversion/","tags":["transition","transversion"],"title":"Transition vs Transversion"},{"categories":null,"contents":"默认方案 使用cowplot提供的draw_image函数读入图片\n1 2  library(cowplot) p1 \u0026lt;- ggdraw() + draw_image(\u0026#34;myFirstAlignment.pdf\u0026#34;)   存在问题 读入的图片存在太多的空白，需要手动切割，或者使用paperWidth和paperHeight来调整，此外，draw_image不指出指定分辨率，读图出来的分辨率太低。\n解决方案 使用ggimage::image_read2函数导入外部图片，保证图片分辨率并使用ggplotify::as.ggplot转化为ggplot对象，再使用cowplot组图。\n1 2 3 4 5  #安装ggimage最新版 setRepositories(ind=1:2) devtools::install_github(\u0026#34;GuangchuangYu/ggimage\u0026#34;) #安装ggplotify devtools::install_github(\u0026#34;GuangchuangYu/ggplotify\u0026#34;)   安装依赖 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  Package Magick++ was not found in the pkg-config search path. Perhaps you should add the directory containing `Magick++.pc\u0026#39; to the PKG_CONFIG_PATH environment variable No package \u0026#39;Magick++\u0026#39; found Using PKG_CFLAGS= Using PKG_LIBS=-lMagick++-6.Q16 ------------------------- ANTICONF ERROR --------------------------- Configuration failed because Magick++ was not found. Try installing: - deb: \u0026#39;libmagick++-dev\u0026#39; (Debian, Ubuntu) - rpm: \u0026#39;ImageMagick-c++-devel\u0026#39; (Fedora, CentOS, RHEL) - csw: \u0026#39;imagemagick_dev\u0026#39; (Solaris) On MacOS it is recommended to use install ImageMagick-6 from homebrew with extra support for fontconfig and rsvg rendering: brew reinstall imagemagick@6 --with-fontconfig --with-librsvg For older Ubuntu versions Trusty (14.04) and Xenial (16.04) use our PPA: sudo add-apt-repository -y ppa:opencpu/imagemagick sudo apt-get update sudo apt-get install -y libmagick++-dev If Magick++ is already installed, check that \u0026#39;pkg-config\u0026#39; is in your PATH and PKG_CONFIG_PATH contains a Magick++.pc file. If pkg-config is unavailable you can set INCLUDE_DIR and LIB_DIR manually via: R CMD INSTALL --configure-vars=\u0026#39;INCLUDE_DIR=... LIB_DIR=...\u0026#39; -------------------------------------------------------------------- ERROR: configuration failed for package ‘magick’ * removing ‘/Storage/data002/shurh/miniconda3/envs/R-3.5.1/lib/R/library/magick’ Error in i.p(...) : (converted from warning) installation of package ‘magick’ had non-zero exit status   根据报错信息，环境中缺少安装需求的Magick++.pc，此时可通过yum查找哪些软件包可以提供\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  #yum查找提供Magick++.pc的软件包 yum provides \u0026#39;*/Magick++.pc\u0026#39; #结果发现所需的两个包 ImageMagick-c++-devel-6.7.2.7-6.el6.x86_64 : C++ bindings for the ImageMagick library Repo : base Matched from: Filename : /usr/lib64/pkgconfig/Magick++.pc ImageMagick-c++-devel-6.7.2.7-6.el6.i686 : C++ bindings for the ImageMagick library Repo : base Matched from: Filename : /usr/lib/pkgconfig/Magick++.pc   安装完成后仍然报错\n1 2 3 4 5 6 7 8 9  Error: package or namespace load failed for ‘magick’ in dyn.load(file, DLLpath = DLLpath, ...): unable to load shared object \u0026#39;/Storage/data002/shurh/miniconda3/envs/R-3.5.1/lib/R/library/magick/libs/magick.so\u0026#39;: /Storage/data002/shurh/miniconda3/envs/R-3.5.1/lib/R/library/magick/libs/magick.so: undefined symbol: _ZN6Magick5Image5writeEPNS_4BlobERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEm Error: loading failed Execution halted ERROR: loading failed * removing ‘/Storage/data002/shurh/miniconda3/envs/R-3.5.1/lib/R/library/magick’ Error in i.p(...) : (converted from warning) installation of package ‘magick’ had non-zero exit status   一阵搜索后无果，但明显是magick仍然未能安装完成。最终，通过conda安装r-magick从而完成magick的安装。\n1  conda install -c conda-forge r-magick   此时安装的r-magick为1.9版本，而ggimage要求从源码安装的magick则为2.0版本，若按要求升级则同样会报错，此时可选择不升级，保持1.9版本仍然可完成ggimage的安装\n实例演示 1 2 3  plot_grid( plot_grid(p1, p2$gtable, ncol=2, rel_widths=c(.6, 1), labels=LETTERS[1:2]), p3, ncol=1, labels=c(\u0026#34;\u0026#34;, \u0026#34;C\u0026#34;))   参考来源 https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html\nhttps://cran.r-project.org/web/packages/ggimage/vignettes/ggimage.html\nhttps://guangchuangyu.github.io/cn/2017/04/ggimage/\nhttps://stackoverflow.com/questions/20416857/how-do-i-install-imagemagick-devel-libraries-on-fedora\n","permalink":"https://sr-c.github.io/2018/12/06/arranging-graphs-into-a-grid-by-cowplot/","tags":["R","cowplot","ggimage"],"title":"使用cowplot完成图片排版"},{"categories":null,"contents":"构建物种数据库 准备注释数据 GO注释 由功能注释得到的数据录入，主要分为两类方法：\n 序列相似性比对（BLAST, diamond） 结构域相似性比对（InterProScan）  待注释序列经由比对，得到公共蛋白数据库（nr, swiss-prot , eggNOG或其他）的ID，而这些ID已经与GO编号相关联（ID对应关系可由idmapping.tb.gz得到），由此可以得到待注释序列的GO注释。\n将基因序列与swiss-prot数据库进行blast/diamond比对（outfmt 6），将结果中的UniProtKB ID经由idmapping.tb.gz对应到GO编号。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  import sys import gzip USAGE = \u0026#34;\\nusage: python %sidmapping.tb.gz blastout outputfile\\n\u0026#34; % sys.argv[0] if len(sys.argv) != 4: print USAGE sys.exit() def parseIDmapping(filename): UniProt_GO = {} with gzip.open(filename, \u0026#39;r\u0026#39;) as f: for line in f: lsplit = line.rstrip().split(\u0026#34;\\t\u0026#34;) if lsplit[7]: UniProt_GO[lsplit[1]] = lsplit[7] return UniProt_GO def parseBlastOut(filename): tab_res = {} with open(filename, \u0026#39;r\u0026#39;) as f: for line in f: lsplit = line.split() tab_res.setdefault(lsplit[0], set()).add(lsplit[1]) return tab_res UniProtKB_GO = parseIDmapping(sys.argv[1]) BlastOut = parseBlastOut(sys.argv[2]) OUT = open(sys.argv[3], \u0026#39;w\u0026#39;) for i in BlastOut: temp = [] for j in BlastOut[i]: if j in UniProtKB_GO: go = UniProtKB_GO[j].split(\u0026#34;; \u0026#34;) temp = temp + go else: continue if temp: OUT.write(i + \u0026#34;\\t\u0026#34; + \u0026#34;,\u0026#34;.join(set(temp)) + \u0026#34;\\n\u0026#34;) OUT.close()   KEGG注释 KEGG注释可由官方提供的自动注释服务KAAS进行，使用blast比对方法，在Gene data set中选择Eukaryotes（真核生物），再加上关注的门类，选择BBH(bi-directional best hit)打分方法，提交注释服务，30~60 分钟即可得到注释结果。\neggNOG注释 使用eggNOG-mapper可以得到目标基因序列的注释信息，其中包括GO与KEGG注释，但其注释到的基因数目往往少于直接通过blast序列比对直接得到的结果。\n清洗注释数据 使用dplyr清洗数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  library(tidyverse) library(stringr) #读入数据 ko_anno \u0026lt;- read_delim(\u0026#34;input/query.ko\u0026#34;,\u0026#34;\\t\u0026#34;, escape_double = FALSE, trim_ws = TRUE) go_anno \u0026lt;- read_delim(\u0026#34;input/go_anno.tsv\u0026#34;,\u0026#34;\\t\u0026#34;, escape_double = FALSE, trim_ws = TRUE) emapper \u0026lt;- read_delim(\u0026#34;input/sesame.modified.annotations\u0026#34;,\u0026#34;\\t\u0026#34;, escape_double = FALSE, trim_ws = TRUE) # extract gene name from emapper gene_info \u0026lt;- emapper %\u0026gt;% dplyr::select(query_name = query_name, `eggNOG annot` = `eggNOG annot`) %\u0026gt;% na.omit() #以KEGG注释列表为基础，合并GO注释与eggNOG注释数据 gene_anno \u0026lt;- dplyr::left_join(ko_anno, go_anno, by = \u0026#34;GID\u0026#34;) %\u0026gt;% dplyr::left_join(go_info, by = \u0026#34;GID\u0026#34;) #重命名数据集的列名 names(gene_anno) \u0026lt;- c(\u0026#34;query_name\u0026#34;,\u0026#34;GO_terms\u0026#34;,\u0026#34;KEGG_KOs\u0026#34;,\u0026#34;eggNOG annot\u0026#34;) #输出数据集 write.table(gene_anno, file=\u0026#34;input/sesame.modified.annotations\u0026#34;, sep=\u0026#34;\\t\u0026#34;)   构建OrgDB 使用AnnotationForge构建待注释物种的OrgDB\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92  library(tidyverse) library(stringr) library(KEGGREST) library(AnnotationForge) #\u0026#39; Title #\u0026#39; #\u0026#39; @param f_emapper_anno eggnog-mapper annotation result #\u0026#39; @param author Who is the creator of this package? like \u0026#34;xxx \u0026lt;xxx@xxx.xx\u0026gt;\u0026#34; #\u0026#39; @param tax_id The Taxonomy ID that represents your organism. (NCBI has a nice online browser for finding the one you need) #\u0026#39; @param genus Single string indicating the genus #\u0026#39; @param species Single string indicating the species #\u0026#39; #\u0026#39; @return OrgDb name #\u0026#39; @export #\u0026#39; #\u0026#39; @examples makeOrgPackageFromEmapper \u0026lt;- function(f_emapper_anno, author, tax_id = \u0026#34;0\u0026#34;, genus = \u0026#34;default\u0026#34;, species = \u0026#34;default\u0026#34;) { # read emapper result emapper \u0026lt;- read_delim(f_emapper_anno, \u0026#34;\\t\u0026#34;, escape_double = FALSE, trim_ws = TRUE) # extract gene name from emapper gene_info \u0026lt;- emapper %\u0026gt;% dplyr::select(GID = query_name, GENENAME = `eggNOG annot`) %\u0026gt;% na.omit() # extract go annotation from emapper gos \u0026lt;- emapper %\u0026gt;% dplyr::select(query_name, GO_terms) %\u0026gt;% na.omit() gene2go = data.frame(GID = character(), GO = character(), EVIDENCE = character()) for (row in 1:nrow(gos)) { the_gid \u0026lt;- gos[row, \u0026#34;query_name\u0026#34;][[1]] the_gos \u0026lt;- str_split(gos[row,\u0026#34;GO_terms\u0026#34;], \u0026#34;,\u0026#34;, simplify = FALSE)[[1]] df_temp \u0026lt;- data_frame(GID = rep(the_gid, length(the_gos)), GO = the_gos, EVIDENCE = rep(\u0026#34;IEA\u0026#34;, length(the_gos))) gene2go \u0026lt;- rbind(gene2go, df_temp) } # extract kegg pathway annotation from emapper gene2ko \u0026lt;- emapper %\u0026gt;% dplyr::select(GID = query_name, Ko = KEGG_KOs) %\u0026gt;% na.omit() load(file = \u0026#34;kegg_info.RData\u0026#34;) gene2pathway \u0026lt;- gene2ko %\u0026gt;% left_join(ko2pathway, by = \u0026#34;Ko\u0026#34;) %\u0026gt;% dplyr::select(GID, Pathway) %\u0026gt;% na.omit() # make OrgDb makeOrgPackage(gene_info=gene_info, go=gene2go, ko=gene2ko, pathway=gene2pathway, # gene2pathway=gene2pathway, version=\u0026#34;0.0.2\u0026#34;, maintainer=author, author=author, outputDir = \u0026#34;.\u0026#34;, tax_id=tax_id, genus=genus, species=species, goTable=\u0026#34;go\u0026#34;) my_orgdb \u0026lt;- str_c(\u0026#34;org.\u0026#34;, str_to_upper(str_sub(genus, 1, 1)) , species, \u0026#34;.eg.db\u0026#34;, sep = \u0026#34;\u0026#34;) return(my_orgdb) } my_orgdb \u0026lt;- makeOrgPackageFromEmapper(\u0026#34;input/sesame.modified.annotations\u0026#34;, \u0026#34;zhangxudong \u0026lt;zhangxudong@genek.tv\u0026gt;\u0026#34;, tax_id = \u0026#34;4182\u0026#34;, genus = \u0026#34;Sesamum\u0026#34;, species = \u0026#34;indicum\u0026#34;) if (requireNamespace(my_orgdb, quietly = TRUE)) remove.packages(my_orgdb) install.packages(my_orgdb, repos = NULL) library(org.Sindicum.eg.db) columns(org.Sindicum.eg.db)   富集分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  library(readr) library(clusterProfiler) library(DOSE) ############################################################################# #OrgDb 是 Bioconductor 中存储不同数据库基因ID之间对应关系，以及基因与GO等注释的对应关系的 R 软件包。 #支持19个物种，可以到这里查询： #http://bioconductor.org/packages/release/BiocViews.html#___OrgDb #不支持的物种可以通过 AnnotationForge 自己构建。 ############################################################################# library(org.Sindicum.eg.db) columns(org.Sindicum.eg.db) ############################################################################# # 导入需要进行富集分析的基因列表，并转换为向量 ############################################################################# gene_list \u0026lt;- read_csv(\u0026#34;input/gene.list\u0026#34;, col_names = FALSE) gene_list \u0026lt;- as.character(gene_list$X1) ############################################################################# #clusterProfiler 可能是目前最优秀的富集分析软件，参考网站： #https://www.bioconductor.org/packages/release/bioc/vignettes/clusterProfiler/inst/doc/clusterProfiler.html ############################################################################# #GO 富集,基于基因数目 ego \u0026lt;- enrichGO(gene = gene_list, #差异基因 vector keyType = \u0026#34;GID\u0026#34;, #差异基因的 ID 类型，需要是 OrgDb 支持的 OrgDb = org.Sindicum.eg.db, #对应的OrgDb ont = \u0026#34;CC\u0026#34;, #GO 分类名称，CC BP MF, 也可选择all pvalueCutoff = 1, #Pvalue 阈值，默认值为0.05 qvalueCutoff = 1, #qvalue 阈值，默认值为0.2 pAdjustMethod = \u0026#34;BH\u0026#34;, #Pvalue 校正方法 readable = FALSE) #TRUE 则展示SYMBOL，FALSE 则展示原来的ID #将 ego 对象转换为dataframe，新版本可以用as.data.frame(ego) ego_results\u0026lt;-as.data.frame(ego) write.table(ego_results, file = \u0026#34;ego_results.txt\u0026#34;, quote = F)   可视化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  #以gplot2重新实现的clusterProfiler可视化工具 library(enrichplot) pdf(file = \u0026#34;ego_barplot.pdf\u0026#34;) barplot(ego, showCategory=20, x = \u0026#34;GeneRatio\u0026#34;) dev.off() dotplot(ego, showCategory=30) emapplot(ego) goplot(ego) #分面绘图 go \u0026lt;- enrichGO(de, OrgDb = \u0026#34;org.Sindicum.eg.db\u0026#34;, ont=\u0026#34;all\u0026#34;) dotplot(go, split=\u0026#34;ONTOLOGY\u0026#34;) + facet_grid(ONTOLOGY~., scale=\u0026#34;free\u0026#34;) #Gene-Concept Network ## remove redundent GO terms ego2 \u0026lt;- simplify(ego) cnetplot(ego2, foldChange=geneList)   参考来源 https://zhuanlan.zhihu.com/p/43651419\nhttp://www.genek.tv/course/223\nhttps://mp.weixin.qq.com/s?__biz=MzI1MjU5MjMzNA==\u0026amp;mid=2247486147\u0026amp;idx=1\u0026amp;sn=081fa71939afc04965179d526a5ef018\u0026amp;chksm=e9e02362de97aa7494b581bc33454f6d2cfdf30796fb70a15501b3fe3bcfe68e27b8cdedfc15\nhttps://mp.weixin.qq.com/s?__biz=MzI4NjMxOTA3OA==\u0026amp;mid=2247484154\u0026amp;idx=1\u0026amp;sn=2a992ed151f0a746cdc6997bef9e29ce\u0026amp;chksm=ebdf8a73dca80365ff29497dc73c78f9597e7b739241d15752b178e32b9e05c2ae916a2a3edb\u0026amp;mpshare=1\u0026amp;scene=23\u0026amp;srcid=1113eHRotxw8k1JU2hZomTu7#rd\nhttps://mp.weixin.qq.com/s?__biz=MzI5NjUyNzkxMg==\u0026amp;mid=2247485611\u0026amp;idx=1\u0026amp;sn=69990a569c623730583b56e54d55b58b\u0026amp;scene=21#wechat_redirect\n","permalink":"https://sr-c.github.io/2018/12/01/clusterProfiler/","tags":["R","clusterProfiler","AnnotationForge","OrgDB"],"title":"使用clusterProfiler进行富集分析"},{"categories":null,"contents":"conda安装 1  conda create -n R-3.5.1 R=3.5.1   但在安装完成后，运行R出现如下报错：\n1 2  (R-3.5.1) [shurh@cluster EggNOG]$ R /Storage/data002/shurh/miniconda3/envs/R-3.5.1/lib/R/bin/exec/R: error while loading shared libraries: libbz2.so.1.0: cannot open shared object file: No such file or directory   此时，覆盖安装bzip2可解决问题\n1  conda install -c conda-forge -c bioconda bzip2   安装软件包 安装clusterProfiler 在R 3.5版本后，bioconductor采用了新的安装方法，直接调用BioManager中的intall进行安装\n1 2 3  if (!requireNamespace(\u0026#34;BiocManager\u0026#34;, quietly = TRUE)) install.packages(\u0026#34;BiocManager\u0026#34;) BiocManager::install(\u0026#34;clusterProfiler\u0026#34;, version = \u0026#34;3.8\u0026#34;)   但在编译结果提示，部分包没有编译完成。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  * installing *source* package ‘units’ ... ** package ‘units’ successfully unpacked and MD5 sums checked configure: units: 0.6-1 checking whether the C++ compiler works... yes checking for C++ compiler default output file name... a.out checking for suffix of executables... checking whether we are cross compiling... no checking for suffix of object files... o checking whether we are using the GNU C++ compiler... yes checking whether g++ accepts -g... yes checking how to run the C++ preprocessor... g++ -E checking for grep that handles long lines and -e... /bin/grep checking for egrep... /bin/grep -E checking for ANSI C header files... yes checking for sys/types.h... yes checking for sys/stat.h... yes checking for stdlib.h... yes checking for string.h... yes checking for memory.h... yes checking for strings.h... yes checking for inttypes.h... yes checking for stdint.h... yes checking for unistd.h... yes checking for stdbool.h that conforms to C99... no checking for _Bool... no checking for error_at_line... yes checking for gcc... gcc -std=gnu99 checking whether we are using the GNU C compiler... yes checking whether gcc -std=gnu99 accepts -g... yes checking for gcc -std=gnu99 option to accept ISO C89... none needed checking for XML_ParserCreate in -lexpat... no checking udunits2.h usability... no checking udunits2.h presence... no checking for udunits2.h... no checking udunits2/udunits2.h usability... no checking udunits2/udunits2.h presence... no checking for udunits2/udunits2.h... no checking for ut_read_xml in -ludunits2... no configure: error: in `/tmp/RtmpmHkkL2/R.INSTALLf29a648ed03a/units\u0026#39;: configure: error: -------------------------------------------------------------------------------- Configuration failed because libudunits2.so was not found. Try installing: * deb: libudunits2-dev (Debian, Ubuntu, ...) * rpm: udunits2-devel (Fedora, EPEL, ...) * brew: udunits (OSX) If udunits2 is already installed in a non-standard location, use: --configure-args=\u0026#39;--with-udunits2-lib=/usr/local/lib\u0026#39; if the library was not found, and/or: --configure-args=\u0026#39;--with-udunits2-include=/usr/include/udunits2\u0026#39; if the header was not found, replacing paths with appropriate values. You can alternatively set UDUNITS2_INCLUDE and UDUNITS2_LIBS manually. -------------------------------------------------------------------------------- See `config.log\u0026#39; for more details ERROR: configuration failed for package ‘units’ * removing ‘/Storage/data002/shurh/miniconda3/envs/R-3.5.1/lib/R/library/units’ ERROR: dependency ‘units’ is not available for package ‘ggforce’ * removing ‘/Storage/data002/shurh/miniconda3/envs/R-3.5.1/lib/R/library/ggforce’ ERROR: dependency ‘ggforce’ is not available for package ‘ggraph’ * removing ‘/Storage/data002/shurh/miniconda3/envs/R-3.5.1/lib/R/library/ggraph’ The downloaded source packages are in ‘/tmp/RtmpYqu9Z7/downloaded_packages’ Updating HTML index of packages in \u0026#39;.Library\u0026#39; Making \u0026#39;packages.html\u0026#39;... done Warning messages: 1: In install.packages(\u0026#34;ggraph\u0026#34;) : installation of package ‘units’ had non-zero exit status 2: In install.packages(\u0026#34;ggraph\u0026#34;) : installation of package ‘ggforce’ had non-zero exit status 3: In install.packages(\u0026#34;ggraph\u0026#34;) : installation of package ‘ggraph’ had non-zero exit status   此时选择先安装ggraph\n1  BiocManager::install(\u0026#34;graph\u0026#34;, version = \u0026#34;3.8\u0026#34;)   出现提示，需要安装udunits2，此时同样选择conda安装\n1  conda install -c conda-forge udunits2   完成后，即可重新安装clusterProfiler\n参考来源 https://stackoverflow.com/questions/27893230/installation-of-package-file-path-had-non-zero-exit-status-in-r\n","permalink":"https://sr-c.github.io/2018/11/24/R-3-5-1-install/","tags":["R","clusterProfiler"],"title":"R-3.5.1安装"},{"categories":null,"contents":"问题描述 BUG1 运行maker3.01.02时，部分scaffold运行失败，其报错信息如下\n1  substr outside of string at ../lib/PhatHit_utils.pm line 850.   本机perl版本为5.10.1，经过查询，问题似乎能够通过较高版本的perl解决\nBUG2 使用tRNAcscan-SE-2.0.0版本用于检测tRNA时，maker默认调用的-p参数在新版本中并不支持，如此API的改变使得maker程序停滞在tRNAscan阶段，程序无法进行。\n解决方案 使用perlbrew安装5.16.3版本perl 1 2  perlbrew install 5.16.3 --thread --multi perlbrew use 5.16.3   重新运行maker3 1 2 3 4 5  mv genome.maker.output/ genome.maker.failed.output/ maker 2\u0026gt; maker_n1.error \u0026amp; maker 2\u0026gt; maker_n2.error \u0026amp; maker 2\u0026gt; maker_n3.error \u0026amp; ...   使用1.3.1版本的tRNAscan-SE，保证其与maker的兼容 1 2 3 4 5 6 7 8 9 10  wget http://lowelab.ucsc.edu/software/tRNAscan-SE.tar.gz tar zxf tRNAscan-SE.tar.gz cd tRNAscan-SE-1.3.1 vi Makerfile #将其中的$(HOME)变量修改为需要安装的路径 make \u0026amp;\u0026amp; make install echo \u0026#39;PATH=$PATH:/your/path/to/tRNAscanSE/bin/\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;PERL5LIB=$PERL5LIB:/your/path/to/tRNAscanSE/bin/\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc   参考来源 https://groups.google.com/forum/#!topic/maker-devel/at8neEnjbYk\nhttps://blog.csdn.net/herokoking/article/details/77853662\n","permalink":"https://sr-c.github.io/2018/11/12/debug-maker3/","tags":["maker","debug","annotation"],"title":"【debug】maker3"},{"categories":null,"contents":"计算后代显性表型的期望 http://rosalind.info/problems/iev/\n1 2 3 4 5 6 7  import os with open(\u0026#39;rosalind_iev.txt\u0026#39;,\u0026#39;r\u0026#39;) as f: couplesNumb = f.readline().strip() [p1, p2, p3, p4, p5, p6] = couplesNumb.split() dominantNumb = (int(p1)+ int(p2)+ int(p3))*2 + int(p4)*1.5 + int(p5) print(dominantNumb)   计算蛋白对应的mRNA序列数量 http://rosalind.info/problems/mrna/\n1 2 3 4 5 6 7 8 9  aa_dict = {\u0026#39;A\u0026#39;:4,\u0026#39;R\u0026#39;:6,\u0026#39;N\u0026#39;:2,\u0026#39;D\u0026#39;:2,\u0026#39;C\u0026#39;:2,\u0026#39;Q\u0026#39;:2,\u0026#39;E\u0026#39;:2,\u0026#39;G\u0026#39;:4,\u0026#39;H\u0026#39;:2,\u0026#39;L\u0026#39;:6,\u0026#39;I\u0026#39;:3,\u0026#39;K\u0026#39;:2,\u0026#39;M\u0026#39;:1,\u0026#39;P\u0026#39;:4,\u0026#39;F\u0026#39;:2,\u0026#39;S\u0026#39;:6,\u0026#39;T\u0026#39;:4,\u0026#39;W\u0026#39;:1,\u0026#39;Y\u0026#39;:2,\u0026#39;V\u0026#39;:4} total_mrna = 1 with open(\u0026#39;rosalind_mrna.txt\u0026#39;) as f: strand = f.readline().strip() for i in range(len(strand)): total_mrna *= aa_dict[strand[i]] print((total_mrna * 3) % 1000000)   参考来源 ","permalink":"https://sr-c.github.io/2018/11/07/python-exercise-4/","tags":["Python","Rosalind"],"title":"Python练习4"},{"categories":null,"contents":"安装 1 2 3 4 5 6 7 8 9 10  # Get latest STAR source from releases wget https://github.com/alexdobin/STAR/archive/2.6.1c.tar.gz tar -xzf 2.6.1c.tar.gz cd STAR-2.6.1c # Compile cd STAR/source make STAR #在bin/目录下就已存在对应平台的已编译版本，可直接使用   使用 构建索引 1 2 3 4 5  STAR --runMode genomeGenerate \\ --genomeDir ~/reference/STAR_index/ \\ --genomeFastaFiles ~/reference/genome/hg38/Homo_sapiens.GRCh38.dna.toplevel.fa \\ --sjdbGTFfile ~/reference/genome/hg38/Homo_sapiens.GRCh38.90.chr_patch_hapl_scaff.gtf \\ --sjdbOverhang 149   进行比对 1 2 3 4  STAR --runThreadN 20 --genomeDir ~/reference/index/STAR/mm10/ \\ --readFilesIn SRR3589959_1.fastq SRR3589959_2.fastq \\ --outSAMtype BAM SortedByCoordinate \\ --outFileNamePrefix ./SRR3589959   注意事项 比对时候需要注意内存限制，否则可能得到报错\n1 2 3 4 5 6  Nov 07 16:46:03 ..... started sorting BAM Max memory needed for sorting = 2566682989 EXITING because of FATAL ERROR: number of bytes expected from the BAM bin does not agree with the actual size on disk: 1613446932 0 45 Nov 07 16:46:03 ...... FATAL ERROR, exiting   查看比对结果 使用IGV查看结果\n向IGV中导入参考基因组  Genomes -\u0026gt; Creat a .genome file\u0026hellip;\n 在弹出窗口中加入基因组序列fasta文件与基因组结构注释gff/gtf文件\n导入比对文件  File -\u0026gt; Load from File\u0026hellip;\n 比对的bam/sam文件需要具有索引\n检验转录组文库是否具有链特异性 首先，需要辨明正反链，正义/反义链，编码链，模板链的概念。\nDNA 的正链和负链，就是那两条反向互补的链。参考基因组给出的那个链就是所谓的正链（forword），另一条链是反链（reverse）。但是这正反一定不能和正义链（sense strand）反义链（antisense strand）混淆，两条互补的DNA链其中一条携带编码蛋白质信息的链称为正义链，另一条与之互补的称为反义链。但是携带编码信息的正义链不是模板，只是因为它的序列和RNA相同，正义链也是编码链。而反义链虽然和RNA反向互补，但它可是真正给RNA当模板的链，因此反义链也是模板链。\n总结两点\n 正义链（sense strand）= 编码链（coding strand）= 非模板链 forword strand 上可以同时有sense strand 和 antisense strand。因为这完全是两个不同的概念。  在IGV的Read strand模式中，显示的reads分为红蓝两色，其中红色代表read方向与DNA正链方向相同(5\u0026rsquo; -\u0026gt; 3\u0026rsquo;)，蓝色代表read方向与DNA正链方向相反\n在First-of-pair strand模式中，红色代表成对的reads中，第一链的方向与正链相同(5\u0026rsquo; -\u0026gt; 3\u0026rsquo;)，蓝色代表成对的reads中，第一链的方向与正链相反(5\u0026rsquo; -\u0026gt; 3\u0026rsquo;)。这对于展示链特异性的文库特别有帮助。\n For a given transcript, non-directional libraries will show a mix of red and blue reads aligning to the locus.\nDirectional libraries will show reads of one color in the direction matching the transcript orientation.\n 对于非链特异性的文库，匹配到同一个基因的reads会表现出红蓝混合的情况；对于链特异性的文库，匹配到同一个基因的reads则会表现出与转录本方向相匹配的颜色。\n判断测序文库是否链特异性的其他方法\n参考来源 https://github.com/alexdobin/STAR\nhttps://www.jianshu.com/p/eca16bf2824e\nhttps://www.jianshu.com/p/a63595a41bed\nhttp://kaopubear.top/2017-10-23-ssrnaseqbasic.html\n","permalink":"https://sr-c.github.io/2018/11/06/STAR/","tags":["STAR","transcript","dUTP","IGV"],"title":"使用STAR判断测序文库的链特异性"},{"categories":null,"contents":"问题描述 1  UnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x96 in position 4575: invalid start byte   解决方案 1 2 3 4  import codecs with open(\u0026#39;pepunit.idlist\u0026#39;,\u0026#39;r\u0026#39;,encoding=\u0026#39;unicode-escape\u0026#39;) as f: #或忽略错误 with open(\u0026#39;pepunit.idlist\u0026#39;,\u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, errors=\u0026#39;ignore\u0026#39;) as f:   参考来源 https://www.crifan.com/summary_python_unicodedecode_error_possible_reasons_and_solutions/\n","permalink":"https://sr-c.github.io/2018/11/02/python-DecodeError/","tags":["Python","debug"],"title":"读取文件UnicodeDecodeError编码错误"},{"categories":null,"contents":"删除文件夹软链接 通过ln -s方法建立的文件夹软链接，只会将该文件夹链接过来，该文件夹下的文件仍然为原文件，直接删除就是删除原文件。\n1 2 3 4  #删除软链接的文件夹 rm -rf symbolic_name #注意不能有/，否则会删除该软链接文件夹下的所有文件 rm -rf symbolic_name/   grep使用多个查询条件 1 2 3 4  #使用-E参数兼容正则表达式，注意一定要加引号 grep -E \u0026#34;ESTABLISHED|WAIT\u0026#34; #或者并列使用多个-e参数 grep -e EST -e WAIT   echo输出换行 1  echo -e \u0026#34;Hello, Welcome\\ntimes\u0026#34;   sort按列去重 1 2  sort -t \u0026#39;,\u0026#39; -k 1,1 -u #-t指定分隔符，-k指定依据第几列到第几列去重   参考来源 https://www.cnblogs.com/xiaochaohuashengmi/archive/2011/10/05/2199534.html\nhttp://blog.sina.com.cn/s/blog_5ceb51480102wli8.html\nhttp://www.blogjava.net/zhyiwww/archive/2009/01/21/252170.html\nhttps://blog.csdn.net/wqfhenanxc/article/details/81937584\nhttps://segmentfault.com/q/1010000000665713\n","permalink":"https://sr-c.github.io/2018/11/02/grep-echo-ln-tips/","tags":["linux"],"title":"文本处理小技巧"},{"categories":null,"contents":"使用re模块 在Python中使用正则表达式时，re模块内部会干两件事情：\n 编译正则表达式，如果正则表达式的字符串本身不合法，会报错； 用编译后的正则表达式去匹配字符串。  1 2 3 4 5  import re pattern = re.compile(r\u0026#39;[.{1,4}.\u0026#39;) pattern.findall(line) m = pattern.search(line) m.group()   编译后生成Regular Expression对象，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。\n参考来源 http://funhacks.net/2016/12/27/regular_expression/\nhttps://www.jianshu.com/p/2d27ed7fb5cd\nhttps://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386832260566c26442c671fa489ebc6fe85badda25cd000\n","permalink":"https://sr-c.github.io/2018/10/31/python-regex/","tags":["Python","regex"],"title":"【Python】使用正则表达式搜索"},{"categories":null,"contents":"斐波那契数列 http://rosalind.info/problems/fib/\n数列的循环数为n(≤40)，扩大级数为k(≤5)\nF~1~ = F~2~ = 1\nFn=Fn−1+Fn−2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  def fib(i,k): i = int(i) if i in [1,2]: return 1 elif i \u0026gt; 2: return k * fib(i-2,k) + fib(i-1,k) else: print(\u0026#39;Not a positive integer.\u0026#39;) with open(\u0026#39;rosalind_fib.txt\u0026#39;) as f: (n, k) = f.readline().strip().split(\u0026#39; \u0026#39;) n = int(n) k = int(k) print(fib(n,k))   孟德尔第一定律 http://rosalind.info/problems/iprb/\n1 2 3 4 5 6 7 8 9 10  def c2(num): num = int(num) return num * (num - 1) / 2 with open(\u0026#39;rosalind_iprb.txt\u0026#39;) as f: (k, m, n) = f.readline().strip().split(\u0026#39; \u0026#39;) total_num = int(k) + int(m) + int(n) total_count = c2(total_num) recessive_count = c2(m) / 4 + int(m) * int(n) / 2 + c2(n) print(1- recessive_count/total_count)   参考来源 ","permalink":"https://sr-c.github.io/2018/10/31/python-exercise-3/","tags":["Python","Rosalind"],"title":"Python练习-3"},{"categories":null,"contents":"查找核酸序列的子集 http://rosalind.info/problems/subs/\n1 2 3 4 5 6 7 8 9 10 11 12  with open(\u0026#39;rosalind_subs.txt\u0026#39;) as lines: string_1 = lines.readline().strip() string_2 = lines.readline().strip() note = [] length_2 = len(string_2) for i in range(0, len(string_1)+1): if string_1[i:i+length_2] == string_2: note.append(str(i+1)) notes = \u0026#39; \u0026#39;.join(note) print(notes)   计算两条序列间的汉明距离 http://rosalind.info/problems/hamm/\n1 2 3 4 5 6 7 8 9  with open(\u0026#39;rosalind_hamm.txt\u0026#39;) as f: s,t = f.read().split() count_n = 0 for i in range(len(t)): if not s[i] == t[i]: count_n += 1 print(count_n)   将核酸序列转换为蛋白序列 http://rosalind.info/problems/prot/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  #!/usr/bin/env python def prot(condon): if condon in [\u0026#39;GCU\u0026#39;, \u0026#39;GCC\u0026#39;, \u0026#39;GCA\u0026#39;, \u0026#39;GCG\u0026#39;]: return \u0026#39;A\u0026#39; elif condon in [\u0026#39;CGU\u0026#39;, \u0026#39;CGC\u0026#39;, \u0026#39;CGA\u0026#39;, \u0026#39;CGG\u0026#39;, \u0026#39;AGA\u0026#39;, \u0026#39;AGG\u0026#39;]: return \u0026#39;R\u0026#39; elif condon in [\u0026#39;AAU\u0026#39;, \u0026#39;AAC\u0026#39;]: return \u0026#39;N\u0026#39; elif condon in [\u0026#39;GAU\u0026#39;, \u0026#39;GAC\u0026#39;]: return \u0026#39;D\u0026#39; elif condon in [\u0026#39;UGU\u0026#39;, \u0026#39;UGC\u0026#39;]: return \u0026#39;C\u0026#39; elif condon in [\u0026#39;CAA\u0026#39;, \u0026#39;CAG\u0026#39;]: return \u0026#39;Q\u0026#39; elif condon in [\u0026#39;GAA\u0026#39;, \u0026#39;GAG\u0026#39;]: return \u0026#39;E\u0026#39; elif condon in [\u0026#39;GGU\u0026#39;, \u0026#39;GGC\u0026#39;, \u0026#39;GGA\u0026#39;, \u0026#39;GGG\u0026#39;]: return \u0026#39;G\u0026#39; elif condon in [\u0026#39;CAU\u0026#39;, \u0026#39;CAC\u0026#39;]: return \u0026#39;H\u0026#39; elif condon in [\u0026#39;UUA\u0026#39;, \u0026#39;UUG\u0026#39;, \u0026#39;CUU\u0026#39;, \u0026#39;CUC\u0026#39;, \u0026#39;CUA\u0026#39;, \u0026#39;CUG\u0026#39;]: return \u0026#39;L\u0026#39; elif condon in [\u0026#39;AUU\u0026#39;, \u0026#39;AUC\u0026#39;, \u0026#39;AUA\u0026#39;]: return \u0026#39;I\u0026#39; elif condon in [\u0026#39;AAA\u0026#39;, \u0026#39;AAG\u0026#39;]: return \u0026#39;K\u0026#39; elif condon in [\u0026#39;AUG\u0026#39;]: return \u0026#39;M\u0026#39; elif condon in [\u0026#39;UUU\u0026#39;, \u0026#39;UUC\u0026#39;]: return \u0026#39;F\u0026#39; elif condon in [\u0026#39;CCU\u0026#39;, \u0026#39;CCC\u0026#39;, \u0026#39;CCA\u0026#39;, \u0026#39;CCG\u0026#39;]: return \u0026#39;P\u0026#39; elif condon in [\u0026#39;UCU\u0026#39;, \u0026#39;UCC\u0026#39;, \u0026#39;UCA\u0026#39;, \u0026#39;UCG\u0026#39;, \u0026#39;AGU\u0026#39;, \u0026#39;AGC\u0026#39;]: return \u0026#39;S\u0026#39; elif condon in [\u0026#39;ACU\u0026#39;, \u0026#39;ACC\u0026#39;, \u0026#39;ACA\u0026#39;, \u0026#39;ACG\u0026#39;]: return \u0026#39;T\u0026#39; elif condon in [\u0026#39;UGG\u0026#39;]: return \u0026#39;W\u0026#39; elif condon in [\u0026#39;UAU\u0026#39;, \u0026#39;UAC\u0026#39;]: return \u0026#39;Y\u0026#39; elif condon in [\u0026#39;GUU\u0026#39;, \u0026#39;GUC\u0026#39;, \u0026#39;GUA\u0026#39;, \u0026#39;GUG\u0026#39;]: return \u0026#39;V\u0026#39; elif condon in [\u0026#39;AUG\u0026#39;, \u0026#39;UAA\u0026#39;, \u0026#39;UGA\u0026#39;, \u0026#39;UAG\u0026#39;]: return \u0026#39;\u0026#39; else: print(\u0026#39;Not proper condon input.\u0026#39;) with open(\u0026#39;rosalind_prot.txt\u0026#39;) as f: RNA_string = f.readline().strip() p_len = int(len(RNA_string)/3) prots = [prot(RNA_string[i*3:i*3+3]) for i in range(p_len)] prot_string = \u0026#39;\u0026#39;.join(prots) print(prot_string)   判断一个能否被整除 1 2 3 4  if x % 3 == 0: print \u0026#34;x能够被3整除\u0026#34; else: print \u0026#34;x不能够被3整除\u0026#34;   查找列表 1 2 3  \u0026#39;a\u0026#39; in a_list a_list.index(\u0026#39;a\u0026#39;) a_list.find(\u0026#39;a\u0026#39;)   参考来源 https://en.wikipedia.org/wiki/Hamming_distance\nhttps://blog.csdn.net/qq_31747765/article/details/80944227\nhttps://blog.csdn.net/qq842977873/article/details/79372844\n","permalink":"https://sr-c.github.io/2018/10/30/python-exercise-2/","tags":["Python","Rosalind"],"title":"Python练习-2"},{"categories":null,"contents":"获得一条序列的反义链 http://rosalind.info/problems/revc/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  with open(\u0026#39;rosalind_revc.txt\u0026#39;) as string_dna: string_dna = string_dna.readline().strip() new_sting = [] for i in string_dna: if i == \u0026#39;A\u0026#39;: new_sting.append(\u0026#39;T\u0026#39;) elif i == \u0026#39;C\u0026#39;: new_sting.append(\u0026#39;G\u0026#39;) elif i == \u0026#39;T\u0026#39;: new_sting.append(\u0026#39;A\u0026#39;) elif i == \u0026#39;G\u0026#39;: new_sting.append(\u0026#39;C\u0026#39;) #将列表倒序排列，revesre()函数进行原为改变，此时不能够将其赋值给另一个列表 new_sting.reverse() #将列表转换为字符串 rev_string = \u0026#39;\u0026#39;.join(new_sting) print(rev_string)   倒置列表的三种方法 使用reverse()方法 1  list.reverse()   注意，reverse()方法对列表进行原位改变，此时不能够对另一个函数赋值，否则会得到空值。\n1 2 3 4  \u0026gt;\u0026gt;\u0026gt; a = [5,7,6,3,4,1,2] \u0026gt;\u0026gt;\u0026gt; b = a.reverse() \u0026gt;\u0026gt;\u0026gt; print b None   类似地，sort()方法也会出现如此情况，此时，应当使用sorted()函数。\n使用reversed()函数 1 2 3  a=[1,2,3,4,5,6,7,8,9] b=list(reversed(a)) print b   注意：reversed()函数返回的是一个迭代器，而不是一个列表，需要再使用List函数转换一下。\n使用切片 1  L[::-1]   将列表转换为字符串 使用join()方法\n1  \u0026lt;str\u0026gt; = \u0026lt;separator\u0026gt;.join(\u0026lt;list\u0026gt;)   join方法是字符串的方法，因此要求.之前必须为str类型\n计算GC含量 http://rosalind.info/problems/gc/\n输入含有10条核算序列的FATSA格式文件，每条序列约长1kbp.\n输出GC含量最高的序列ID与其对应的GC含量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  try: fasta = {} with open(\u0026#39;rosalind_gc.txt\u0026#39;) as lines: for line in lines: line = line.strip() if line[0] == \u0026#39;\u0026gt;\u0026#39;: name = line[1:] else: fasta[name] = fasta.get(name, \u0026#39;\u0026#39;) + line except IOError as err: print(\u0026#39;File error: \u0026#39; + str(err)) max_gc = 0 max_id = \u0026#39;\u0026#39; #使用for循环取得最大的GC含量 for read_id in fasta.keys(): rate_gc = (fasta[read_id].count(\u0026#39;G\u0026#39;) + fasta[read_id].count(\u0026#39;C\u0026#39;)) / len(fasta[read_id]) if rate_gc \u0026gt; max_gc: max_gc = rate_gc max_id = read_id print(max_id) print(max_gc * 100)   上述程序要求输入的数据中，不含有N碱基，也不含有小写碱基。\n参考来源 http://www.iplaypy.com/jinjie/jj114.html\nhttps://www.jianshu.com/p/c61279736a03\nhttps://blog.csdn.net/akisayaka/article/details/50042175\nhttps://blog.csdn.net/u013810296/article/details/57082163?utm_source=blogxgwz1\n","permalink":"https://sr-c.github.io/2018/10/29/python-exercise-1/","tags":["Python","Rosalind"],"title":"Python练习-1"},{"categories":null,"contents":"问题描述 在函数中调用了外部的变量，并改变变量的值，却不声明这是全局变量，会导致Python报错。因为一旦要在函数中改变这个变量的值，就会使得这个变量成为局部变量。\n1 2 3 4 5  count = 0 def function(): count = count + 1 print(count)    UnboundLocalError: local variable \u0026lsquo;count\u0026rsquo; referenced before assignment.\n本地变量\u0026rsquo;count\u0026rsquo;在之前已有定义\n 解决方案 在函数中以globlal声明调用的这个变量是全局变量\n1 2 3 4 5 6  count = 0 def function(): global count #declare \u0026#39;count\u0026#39; used in this function is the global one  count = count + 1 print(count)   Best solution Don\u0026rsquo;t use global variable in a function.\n参考来源 https://blog.csdn.net/yuli_dai/article/details/9326773\nhttps://stackoverflow.com/questions/11904981/local-variable-referenced-before-assignment\nhttp://blog.sina.com.cn/s/blog_4b9eab320100q2l5.html\n","permalink":"https://sr-c.github.io/2018/10/28/debug-python-UnboundLocalError/","tags":["debug","Python"],"title":"【debug】local variable referenced before assignment"},{"categories":null,"contents":"方法串链与函数串链 方法串链从左向右读\n1  f.readline().strip().split(\u0026#39;.\u0026#39;)   函数串链从右向左读\n1  print(sorted(james))   排序有两种方式 原地排序（In-place sorting）是指按照指定的顺序排列数据，然后用排序后的数据替换原来的数据。原来的顺序会丢失。sort()方法提供原地排序。\n复制排序（Copied sorting）是指按照指定的顺序排列顺序，然后返回原数据的一个有序副本。原数据的顺序依然保留，只是对一个副本排序。sorted() BIF支持复制排序。\n数据清洗 由于输入的数据常常并不是理想的标准格式，因此我们需要对其进行清洗。\n例如，教练记录的选手跑步成绩记录成了如下三种形式\n 2-15, 3:04, 2.44\n 需要先将这些数据统一为同一种计时方式，再进行后续处理。\n 创建函数sanitize()，从各个选手的列表接收一个字符串作为输入，然后处理这个字符串，将找到的所有短横线或冒号转换为一个点号，并返回清理过的字符串。\n 1 2 3 4 5 6 7 8 9  def sanitize(time_string): if \u0026#39;-\u0026#39; in time_string: splitter = \u0026#39;-\u0026#39; elif \u0026#39;:\u0026#39; in time_string: splitter = \u0026#39;:\u0026#39; else: return(time_string) (mins, secs) = time_string.split(splitter) return(mins + \u0026#39;.\u0026#39; + secs)   列表推导 列表推导（list comprehension）是Python提供的工具，用于方便地转换列表，减少代码量。\n1 2 3 4  clean_mikey = [] for each_t in mikey: clean_mikey.append(sanitize(each_t))   1  clean_mikey = [sanitize(each_t) for each_t in mikey]   迭代删除重复项 1 2 3 4 5 6 7  james = sorted([sanitize(t) for t in james]) unique_james = [] for i in james: if i not in unique_james: unique_james.append(i) print(unique_james[0:3])   使用集合删除重复项 工厂函数set() BIF创建集合，其中的数据项是无序的，而且不允许重复。\n1  print(sorted(set([sanitized(i) for i in james]))[0:3])   参考来源 《Head First Python》\n","permalink":"https://sr-c.github.io/2018/10/28/python-learning-5/","tags":["Python","Head First"],"title":"【Python】理解数据"},{"categories":null,"contents":"用with处理文件 使用try/except处理文件时，若在文件处理过程出现错误，其后的代码都不会运行。此时，应当将关闭文件的语句加入finally语句中。\n更简便的方法是，使用with语句保证关闭打开的数据文件。\n1 2 3 4 5 6 7 8  try: data = open(\u0026#39;its.txt\u0026#39;, \u0026#39;w\u0026#39;) print(\u0026#34;It\u0026#39;s...\u0026#34;, file=data) except IOError as err: print(\u0026#39;File error: \u0026#39; + str(err)) finally: if \u0026#39;data\u0026#39; in locals(): data.close()   等价于\n1 2 3 4 5  try: with open(\u0026#39;its.txt\u0026#39;, \u0026#39;w\u0026#39;) as data: print(\u0026#34;It\u0026#39;s...\u0026#34;, file=data) except IOError as err: print(\u0026#34;File error: \u0026#34; + str(err))   \u0026ldquo;腌制\u0026quot;数据 使用标准库pickle保存和加载Python数据对象。\n使用dump保存，使用load恢复 1 2 3 4 5 6 7 8 9  import pickle with open(\u0026#39;mydata.pickle\u0026#39;,\u0026#39;wb\u0026#39;) as mysavedata: pickle.dump([1,2,\u0026#39;three\u0026#39;], mysavedata) ... with open(\u0026#39;mydata.pickle\u0026#39;,\u0026#39;rb\u0026#39;) as myrestoredata: a_list = pickle.load(myrestoredata) print(a_list)   1 2 3 4 5 6 7 8 9 10  import pickle try: with open(\u0026#39;man_data.txt\u0026#39;, \u0026#39;wb\u0026#39;) as man_file, open(\u0026#39;other_data.txt\u0026#39;, \u0026#39;wb\u0026#39;) as other_file: pickle.dump(man, man_file) pickle.dump(other, other_file) except IOError as err: print(\u0026#39;File error: \u0026#39; + str(err)) except pickle.PickleError as perr: print(\u0026#39;Pickling error: \u0026#39; + str(perr))   使用pickle的通用文件I/O才是上策\n参考来源 《Head First Python》\n","permalink":"https://sr-c.github.io/2018/10/28/python-learning-4/","tags":["Python","Head First"],"title":"【Python】持久存储"},{"categories":null,"contents":"输入 Python中的基本输入机制是基于行的：从文本文件向程序读入数据时，一次会到达一个数据行。\n基本操作 使用open() BIF处理文件中的数据时，会创建一个迭代器从文件向你的代码输入数据行，一次传入一行数据。\n1 2 3 4  the_file = open(\u0026#39;sketch.txt\u0026#39;) # Do something with the data # in \u0026#34;the_file\u0026#34;. the_file.close()   指定文件读取的指针seek(0)回到文件起始位置\n使用split()根据需要抽取数据行中的各个部分。split()方法返回一个字符串列表，这回赋值至一个目标标识符列表。这成为多重赋值（multiple assignment）:\n1  (role, line_spoken) = each_line.split(\u0026#34;:\u0026#34;)   查看帮助 1  help(each_line.split)   处理异常的两种方法 增加额外的逻辑 1 2 3 4 5 6 7 8 9 10 11 12  import os if os.path.exists(\u0026#39;sketch.txt\u0026#39;): data = open(\u0026#39;sketch.txt\u0026#39;) for each_line in data: if not each_line.find(\u0026#39;:\u0026#39;) == -1: (role, line_spoken) = each_line.split(\u0026#39;:\u0026#39;, 1) ... else: print (\u0026#39;The data file does not exist!\u0026#39;)   随着必须考虑的错误越来越多，“额外增加的代码和逻辑”方案的复杂性也随之增加，直到最后可能会严格程序的本来作用。\npython的基本理念是要重点关注你的代码需要做什么。\n先尝试，然后恢复 try/except机制\n1 2 3 4  try: your codes except: error handle codes   放过错误，可以使用pass语句，忽略这个错误。\n但是，过于放松条件，会使得程序悄无声息地忽略关键的错误。因此，应当注意以一种不那么一般化的方式使用except。\n指定要处理的错误类型\n一般，我们真正需要考虑的只是之前开发程序时出现的异常类型，比如IOError和ValueError。因此，我们推荐在except语句中指定要处理的运行时错误类型。\n参考来源 《Head First Python》\n","permalink":"https://sr-c.github.io/2018/10/26/python-learning-3/","tags":["Python","Head First"],"title":"【Python】文件与异常"},{"categories":null,"contents":"命名空间  主Python程序中的代码与一个名为__main__的命名空间关联。将代码放在其单独的模块中时，Python会自动创建一个与模块同名的命名空间。\n 如果在程序开头通过import导入了某些模块，这些模块中的函数都会属于其特定的命名空间，而不是主程序默认的命名空间__main__，所以在调用时需要特殊指定。\n使用可选参数 将一个函数的必要参数改为可选参数，需要为这个参数提供一个缺省值。同时，为要给参数提供缺省值，也使得这个参数成为了可选参数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  #!/usr/bin/env python \u0026#39;\u0026#39;\u0026#39;This is the model \u0026#39;nester.py\u0026#39;, server a fuction print_lol() to print a list,which would have lists in it.\u0026#39;\u0026#39;\u0026#39; def print_lol(the_list, indent=False, level=0): \u0026#39;\u0026#39;\u0026#39;This function take one positional argument called \u0026#39;the_list\u0026#39;, which is any Python list(possibly nested lists).\u0026#39;\u0026#39;\u0026#39; for each_item in the_list: if isinstance(each_item,list): print_lol(each_item, level+1) else: if indent: for tab_stop in range(level): print(\u0026#39;\\t\u0026#39;, end=\u0026#39;\u0026#39;) print(each_item)   构建发布 向函数文本中补充了注释之后，将该模块保存。准备发布(distribution)，其实是一个文件集合，这些文件联合起来使得用户能够构建、打包和发布自己的模块。\n  为模块创建一个文件夹\n文件夹中放置模块源文件，\n  在文件夹中创建一个名为setup.py的文件\nsetup.py文件中包含了此发布的元数据，形式如下：\n1 2 3 4 5 6 7 8 9 10 11  from distutils.core import setup setup( name = \u0026#39;nester\u0026#39;, version = \u0026#39;1.0.0\u0026#39;, py_modules = [\u0026#39;nester\u0026#39;], author = \u0026#39;hfpython\u0026#39;, author_email = \u0026#39;\u0026#39;, url = \u0026#39;\u0026#39;, description = \u0026#39;\u0026#39;, )     构建一个发布文件\n1  python setup.py sdist     将发布安装到自己的Python本地副本中\n1  sudo python setup.py install   模块安装后，会放置在特定的路径中，因此，解释器就能够找到该模块。用户可以使用import sys; sys.path来查看当前解释器的模块列表。\n1 2  \u0026gt;\u0026gt;\u0026gt; import sys; sys.path [\u0026#39;C:\\\\Users\\\\YourUserName\\\\PycharmProjects\\\\hfpython\u0026#39;, \u0026#39;C:\\\\Program Files\\\\Python37\\\\Lib\\\\idlelib\u0026#39;, \u0026#39;C:\\\\Program Files\\\\Python37\\\\python37.zip\u0026#39;, \u0026#39;C:\\\\Program Files\\\\Python37\\\\DLLs\u0026#39;, \u0026#39;C:\\\\Program Files\\\\Python37\\\\lib\u0026#39;, \u0026#39;C:\\\\Program Files\\\\Python37\u0026#39;, \u0026#39;C:\\\\Program Files\\\\Python37\\\\lib\\\\site-packages\u0026#39;]     向PyPI上传代码 注册PyPI https://pypi.org/\n准备上传 1 2 3  # 注册信息，在其中使用之前已注册的信息登陆 python setup.py register python setup.py sdist upload   更新模块 在向模块添加了新的API(Application Programming Interface, API)后，需要更新该模块。更新了模块文件后，还需要修改setup.py中的version，重新本地安装或提交\n1 2  sudo python setup.py install python setup.py sdist upload   参考来源 《Head First Python》\n","permalink":"https://sr-c.github.io/2018/10/26/python-learning-2/","tags":["Python","PyPI","Head First"],"title":"【Python】函数模块"},{"categories":null,"contents":"数据类型 变量 变量的动态类型：整型（int），浮点型（float），字符型（str）\n基本运算 +，-，*，/，次方（**），整除（//），取余（%）\n四舍五入 round()\n取整 int()\n高级功能 1 2 3 4 5 6 7 8 9 10 11 12  import math trunc() ceil() floot() fabs() sqrt() import random uniform(a,b) randint(a,b)   字符型变量 转义字符\n\\t \\n \\\u0026quot; \\'\n忽略多个转义字符，可在字符串前加r\n常用操作\n连接 +\n重复 s*5\n取值/切片\n 字符型变量可被视作数组处理\n 函数/方法 1 2 3 4 5 6 7 8  len(s) s.upper() s.lower() s.strip() s.replace() s.index() s.split() s.count()   数组/序列 列表（list） 定义：chr_len = [10, 20, 30]\n元祖（tuple） 定义：chr_len = (10, 20, 30)\n 元祖不支持原位改变\n 列表特有操作 1 2 3 4 5 6 7 8 9 10 11 12 13  #增加元素 append() extend() insert() #删除元素 del remove() pop() #原位改变 reverse() sort() #拷贝列表 copy()   列表生成式\n1 2  [\u0026#39;Chr\u0026#39;+str(i) for i in chr if isinstance(i,int)] #操作 + 循环 + 条件   哈希/字典 定义： chr = {\u0026lsquo;chr1\u0026rsquo;:10, \u0026lsquo;chr2\u0026rsquo;:20, \u0026lsquo;chr3\u0026rsquo;:30}\n特征： 无序，键唯一\n调用\n1 2  chr[\u0026#39;chr1\u0026#39;] chr.get(\u0026#39;chr1\u0026#39;,\u0026#39;\u0026#39;)   键，值和项目\n1 2 3  chr.keys() chr.values() chr.items()   常用操作 1 2 3 4 5 6 7 8 9  #添加项目 chr.update() #删除项目 del chr[\u0026#39;chr1\u0026#39;] chr.pop() chr.popitem() chr.clear() #求长度 len(chr)   输入输出 1 2 3 4 5 6 7 8 9 10 11 12  #打开句柄 f = open(f_fasta,\u0026#39;r\u0026#39;) f.read() #读取所有内容，返回字符串 f.readlines() #读取所有内容，按行分割，返回列表 f.readline() #读取一行内容，返回字符串 #关闭文件 f.close() #写入一个字符串 f.write() #写入一个序列型变量 f.writelines()   更简洁写法\n1 2 3 4 5 6 7  with open(f_fasta,\u0026#39;r\u0026#39;) as lines: for line in lines: ... with open(args.out,\u0026#39;w\u0026#39;) as f: f.write() ...   argparse 更专业的参数输入 1 2 3 4 5 6 7 8 9 10 11  import argparse parser = argparse.ArgumentParaser() parser.add_argument(\u0026#39;...\u0026#39;, type=str, nargs=\u0026#39;+\u0026#39;, #action=\u0026#39;store_true\u0026#39;, #default=\u0026#39;\u0026#39;, required=True, help=\u0026#39;\u0026#39;) args = parser.parse_args()   格式化字符串 1  print(\u0026#34;...{2:^16s}...{0:0\u0026gt;6.2f}...{}...\u0026#34;.format(\u0026#34;total length:\u0026#34;,\u0026#34;total_len\u0026#34;))   格式化输出指定输出格式的方式：\n 元素下标/名：填充符，对齐方式，字符宽度，[浮点精度]，变量类型\n 参考来源 http://www.genek.tv/my/course/194\n","permalink":"https://sr-c.github.io/2018/10/25/python-learning-1/","tags":["Python"],"title":"Python语言基础"},{"categories":null,"contents":"Infernal 直接下载安装Infernal1.1.2的已编译版本\n1 2 3  wget http://eddylab.org/infernal/infernal-1.1.2-linux-intel-gcc.tar.gz tar xf infernal-1.1.2-linux-intel-gcc.tar.gz echo \u0026#39;PATH=$PATH:/your/path/to/infernal-1.1.2-linux-intel-gcc/binaries\u0026#39; \u0026gt;\u0026gt; ~/.bashrc   Rfam12.0是第一个基于Infernal1.1构建的版本，更新了注释流程。因此，更早版本的Rfam数据库只能通过Infernal1.0.2版本进行注释。\n下载安装Rfam数据库 1 2 3 4 5  wget ftp://ftp.ebi.ac.uk/pub/databases/Rfam/CURRENT/Rfam.cm.gz gunzip Rfam.cm.gz wget ftp://ftp.ebi.ac.uk/pub/databases/Rfam/CURRENT/Rfam.clanin # index the Rfam.cm file cmpress Rfam.cm   计算目标基因组大小 1 2  esl-seqstat tutorial/mrum-genome.fa $ Total # of residues: 2937203   Inferal需要根据输入数据的总碱基数设定E-value的阈值。因此，在执行cmscan之前需要先行计算总碱基数，乘以2（因为序列的两条链都会被搜索），再除以1,000,000得到以兆为单位的碱基数，作为cmscan的参数输入。\n运行程序 1  cmscan -Z 5.874406 --cut_ga --rfam --nohmmonly --tblout mrum-genome.tblout --fmt 2 --cpu 8 --clanin Rfam.clanin Rfam.cm tutorial/mrum-genome.fa \u0026gt; mrum-genome.cmscan   参考来源 https://rfam.readthedocs.io/en/latest/genome-annotation.html\nhttp://eddylab.org/infernal/Userguide.pdf\n","permalink":"https://sr-c.github.io/2018/10/19/rfam-genome-annotation/","tags":["Rfam","annotation","ncRNA","non-coding RNA"],"title":"使用Rfam进行ncRNA注释"},{"categories":null,"contents":"安装 RIPCAL最新版应为2.0版本，但其软件主页默认下载项却仍为1.0.4版本，应当注意下载时的选择。\n1 2 3 4 5 6 7 8 9 10 11  wget http://nchc.dl.sourceforge.net/project/ripcal/RIPCAL/RIPCAL_2.0/ripcal2_install.zip unzip ripcal2_install.zip mkdir ~/opt/ripcal2/ mv ripcal2_intall/perl/* ~/opt/ripcal2/ chmod 755 ~/opt/ripcal2/* mv ripcal2_install/RIPCAL_manual_v1_0.pdf ~/opt/ripcal2/ echo \u0026#39;PATH=$PATH:~/opt/ripcal2/\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc #安装需求的模块 cpanm Math::Round Tk   配置 RIPCAL默认调用clustalw进行多序列比对，若目前使用的是clustalw2，则需要做相应的修改。\n1 2  vi ~/opt/ripcal2/ripcal # 将system()括号内的clustalw修改为clustalw2   输入文件 FASTA RIPCAL接受标准的FASTA/Pearson序列格式，或FASTA多序列比对格式。\nGFF 在分析多于一个重复序列家族时，RIPCAL要求输入GFF文件。输入的GFF文件第1列为序列ID，第3列为Repeat_Region，第9列中以Target标签标注重复序列的种类。\n运行 1 2 3 4 5 6 7  #Alignment-based mode ripcal -c --type align --seq input.fasta --gff input.gff --model consensus ripcal_summarise *_RIPALIGN.TXT \u0026gt; outputfile #Di-nucleotide mode ripcal -c -t index -s input.fasta -g input.gff # Index scan mode ripcal -c -t scan -s input.fasta   参考来源 https://sourceforge.net/projects/ripcal/files/RIPCAL/RIPCAL_2.0/RIPCAL_manual_v1_0.pdf\n","permalink":"https://sr-c.github.io/2018/10/18/ripcal/","tags":["ripcal","RIP"],"title":"真菌RIP分析"},{"categories":null,"contents":"问题描述 在求一些向量的交集时，发现肉眼可见的交集却得不到结果。\n一阵折腾过后，在notepad++中打开，发现在换行时两组数据的差异。CR LF (\\r\\n) 与 LF (\\n) ，也就是一组数据以\\r\\n换行，另一组数据以\\n换行。\n这一差异在vim中:set list打开list mode仍然不能区分\n解决方案 将\\r\\n替换为\\n\n 由于历史遗留问题，在Unix系统中，每行结尾只有\u0026quot;\u0026lt;换行\u0026gt;\u0026rdquo;，即\u0026rdquo;\\n\u0026rdquo;；Windows系统里面，每行结尾是\u0026quot;\u0026lt;回车\u0026gt;\u0026lt;换行\u0026gt;\u0026rdquo;，即\u0026rdquo;\\r\\n\u0026rdquo;；Mac系统里，每行结尾是\u0026quot;\u0026lt;回车\u0026gt;\u0026rdquo;。一个直接后果是，Unix/Mac系统下的文件在Windows里打开的话，所有文字会变成一行；而Windows里的文件在Unix/Mac下打开的话，在每行的结尾可能会多出一个^M符号。\n 参考来源 http://www.ruanyifeng.com/blog/2006/04/post_213.html\nhttps://www.cnblogs.com/xiaotiannet/p/3510586.html\n","permalink":"https://sr-c.github.io/2018/10/18/CR-and-LF/","tags":["vim","Unix"],"title":"回车与换行"},{"categories":null,"contents":"练习 4765-8910之间的所有自然数\n1  476[5-9]|47[7-9]\\d|4[8-9]\\d\\d|[5-7]\\d{3}|8[0-8]\\d\\d|890\\d|8910   参考来源 https://www.jb51.net/tools/zhengze.html\n","permalink":"https://sr-c.github.io/2018/10/16/Regex/","tags":["regex"],"title":"正则表达式"},{"categories":null,"contents":"集合运算 1 2 3 4 5 6 7  x \u0026lt;- c(1,3,4) y \u0026lt;- c(2,4,8) union(x,y) #取向量x与向量y的交集，并去冗余 intersect(x,y) #取向量x与向量y的并集 setdiff(x,y) #找出向量x中与向量y不同的元素 setequal(x,y) #判断x,y是否相同 x %in% y #判断x中的每个元素是否在y中，返回逻辑向量   简便方案 对于简单的两组列表，使用sort与uniq联用就可达到目的\n1 2 3 4  sort a.txt b.txt | uniq -d #求交集 sort a.txt b.txt | uniq #求并集 sort a.txt b.txt b.txt | uniq -u #求差集(A-B) sort a.txt b.txt | uniq -u #求对称差，即A,B中只出现一次的项   参考来源 https://blog.csdn.net/sinat_26917383/article/details/51277581\nhttps://blog.csdn.net/woodcorpse/article/details/80494605\nhttps://blog.csdn.net/yinxusen/article/details/7450213\n","permalink":"https://sr-c.github.io/2018/10/14/R-intersect/","tags":["R","集合运算","交集","并集"],"title":"【R】集合运算"},{"categories":null,"contents":"安装 使用conda安装\n1 2 3  conda create -n pfam_scan ##可新建一个环境，用于安装pfam-scan source activate pfam_scan conda install pfam_scan   pfam_scan依赖bioperl，因此，通过conda安装简单快捷\n使用 首先，使用hmmer给数据库建立索引\n1  hmmpress Pfam-A.hmm   执行程序\n1  pfam_scan.pl -fasta \u0026lt;fasta_file\u0026gt; -dir \u0026lt;directory location of Pfam files\u0026gt; -outfile \u0026lt;file\u0026gt; -cpu \u0026lt;n\u0026gt; -as   为什么不直接使用hmmscan呢？ 使用pfam-scan和直接使用hmmscan进行比对得到的输出有一定的不同。直接使用hmmscan比对得到的结果会允许不同上级分类的序列重叠，而pfam_scan.pl的输出指挥输出同一级分类下最显著（E-value最小）的结果。\n Q: Why are the results from running hmmscan different from those I get when I run pfam_scan.pl? A: We group together families which we believe to have a common evolutionary ancestor in clans. Where there are overlapping matches within a clan, pfam_scan.pl will only show the most significant (the lowest E-value) match within the clan. We perform the same clan filtering step on the Pfam website. If you do want the script to report all the overlapping clan matches, you can use the -clan_overlap option.\n 参考来源 https://www.jianshu.com/p/396eaf5c1a83\nhttp://www.oebiotech.com/Article/dbbmnlycrj.html\n","permalink":"https://sr-c.github.io/2018/10/14/pfam-scan/","tags":["Pfam","HMMER","pfam-scan","annotation"],"title":"使用pfam-scan进行Pfam注释"},{"categories":null,"contents":"dbCAN2 meta server是dbCAN web server的升级版本，使用HMMER, DIAMOND与Hotpep三种方式进行比对，并且允许提交核算序列（原核生物）。2018年8月25日，dbCAN 数据库升级至7.0版本。\n数据准备 输入的序列应当为FASTA格式。对于原核生物，dbCAN2支持直接输入核酸序列（便于宏基因组数据输入）；对于真核生物，用户还是应当自行预测出蛋白序列作为输入。\n使用CGCFinder预测CAZyme 基因簇(CAZyme gene clusters, CGCs)，需要提供基因的位置信息（BED/GFF格式）。其要求的示例格式如下。\n Scaffold1\tprot_00001\t3174\t3575\t+ Scaffold1\tprot_00002\t6297\t7073\t- Scaffold1\tprot_00003\t7066\t8793\t- Scaffold1\tprot_00004\t8793\t10658\t- Scaffold1\tprot_00005\t10782\t10907\t- Scaffold1\tprot_00006\t11206\t11871\t-\n 如提交GFF文件，其中包含\u0026rsquo;CDS\u0026rsquo;字符的列才会被识别。基因ID应当包含在Name或ID标签中。\n结果输出 结果页首先会展示3种比对工具注释到的CAZyme，整合3种方法的结果，提高准确率。\n若提供了基因位置信息，结果中还有CGCFinder预测出的CAZyme基因簇。结果页中还可调整CGCFinder的阈值，方便筛选预测结果。\n参考来源 http://cys.bios.niu.edu/dbCAN2/help.php\n","permalink":"https://sr-c.github.io/2018/10/12/dbCAN2/","tags":["CAZyme","dbCAN","annotation"],"title":"使用dbCAN2进行CAZyme注释"},{"categories":null,"contents":"在线工具 The Galaxy Project提供了这方面的在线工具，可以方便地将gff格式文件转换为bed格式。\n但是由于gff格式文件标签太多，导致输出的bed文件可能会确实关键信息（如基因ID）。工具会将以type列作为基因名称，得到错误的ID信息。\n使用gtf格式文件转换为bed格式文件能够得到更有用的输出。\n本地工具 gtf2Bed.pl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111  #!/usr/bin/perl # Copyright (c) 2011 Erik Aronesty (erik@q32.com) #  # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the \u0026#34;Software\u0026#34;), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: #  # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. #  # THE SOFTWARE IS PROVIDED \u0026#34;AS IS\u0026#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE. #  # ALSO, IT WOULD BE NICE IF YOU LET ME KNOW YOU USED IT. use Data::Dumper; $in = shift @ARGV; open IN, ($in =~/\\.gz$/ ? \u0026#34;gunzip -c $in\u0026#34; : $in =~/\\.zip$/ ? \u0026#34;unzip -p $in\u0026#34; : \u0026#34;$in\u0026#34;); while (\u0026lt;IN\u0026gt;) { $gff = 2 if /^##gff-version 2/; $gff = 3 if /^##gff-version 3/; next if /^#/ \u0026amp;\u0026amp; $gff; s/\\s+$//; # 0-chr 1-src 2-feat 3-beg 4-end 5-scor 6-dir 7-fram 8-attr my @f = split /\\t/; if ($gff) { # most ver 2\u0026#39;s stick gene names in the id field ($id) = $f[8]=~/\\bID=\u0026#34;([^\u0026#34;]+)\u0026#34;/; # most ver 3\u0026#39;s stick unquoted names in the name field ($id) = $f[8]=~/\\bName=([^\u0026#34;;]+)/ if !$id \u0026amp;\u0026amp; $gff == 3; } else { ($id) = $f[8]=~/transcript_id \u0026#34;([^\u0026#34;]+)\u0026#34;/; } next unless $id \u0026amp;\u0026amp; $f[0]; if ($f[2] eq \u0026#39;exon\u0026#39;) { die \u0026#34;no position at exon on line $.\u0026#34; if ! $f[3]; # gff3 puts :\\d in exons sometimes $id =~ s/:\\d+$// if $gff == 3; push @{$exons{$id}}, \\@f; # save lowest start $trans{$id} = \\@f if !$trans{$id}; } elsif ($f[2] eq \u0026#39;start_codon\u0026#39;) { #optional, output codon start/stop as \u0026#34;thick\u0026#34; region in bed $sc{$id}-\u0026gt;[0] = $f[3]; } elsif ($f[2] eq \u0026#39;stop_codon\u0026#39;) { $sc{$id}-\u0026gt;[1] = $f[4]; } elsif ($f[2] eq \u0026#39;miRNA\u0026#39; ) { $trans{$id} = \\@f if !$trans{$id}; push @{$exons{$id}}, \\@f; } } for $id ( # sort by chr then pos sort { $trans{$a}-\u0026gt;[0] eq $trans{$b}-\u0026gt;[0] ? $trans{$a}-\u0026gt;[3] \u0026lt;=\u0026gt; $trans{$b}-\u0026gt;[3] : $trans{$a}-\u0026gt;[0] cmp $trans{$b}-\u0026gt;[0] } (keys(%trans)) ) { my ($chr, undef, undef, undef, undef, undef, $dir, undef, $attr, undef, $cds, $cde) = @{$trans{$id}}; my ($cds, $cde); ($cds, $cde) = @{$sc{$id}} if $sc{$id}; # sort by pos my @ex = sort { $a-\u0026gt;[3] \u0026lt;=\u0026gt; $b-\u0026gt;[3] } @{$exons{$id}}; my $beg = $ex[0][3]; my $end = $ex[-1][4]; if ($dir eq \u0026#39;-\u0026#39;) { # swap $tmp=$cds; $cds=$cde; $cde=$tmp; $cds -= 2 if $cds; $cde += 2 if $cde; } # not specified, just use exons $cds = $beg if !$cds; $cde = $end if !$cde; # adjust start for bed --$beg; --$cds; my $exn = @ex;\t# exon count my $exst = join \u0026#34;,\u0026#34;, map {$_-\u0026gt;[3]-$beg-1} @ex;\t# exon start my $exsz = join \u0026#34;,\u0026#34;, map {$_-\u0026gt;[4]-$_-\u0026gt;[3]+1} @ex;\t# exon size # added an extra comma to make it look exactly like ucsc\u0026#39;s beds print \u0026#34;$chr\\t$beg\\t$end\\t$id\\t0\\t$dir\\t$cds\\t$cde\\t0\\t$exn\\t$exsz,\\t$exst,\\n\u0026#34;; } close IN;   去冗余 由于输入的gff/gtf文件中含有可变剪切信息，转换得到的bed文件会将每个基因的可变剪切列出。如果目标文件中不需要包含可变剪切，只需要对应到基因水平，则还需要进行手动调整。\n选取目标列 我们只需要目标列Chrom Start End Name Strand\n1 2  # 使用awk，将第4列提前到第2列，并以制表符分隔 awk \u0026#39;BEGIN {OFS=\u0026#34;\\t\u0026#34;} {print $1, $4, $2, $3, $6}\u0026#39; old.bed \u0026gt; step1.bed   删除可变剪切信息 1 2 3  vi step1.bed # :%s/\\.t.//g # 删除Name列中基因ID后.t1,.t2,.t3等可变剪切信息   去重 1  sort -k 2 -u step1.bed \u0026gt; final.bed   参考来源 https://anjingwd.github.io/AnJingwd.github.io/2017/08/15/NGS%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8FBED-GFF-GTF%E4%B9%8B%E4%BB%8B%E7%BB%8D-%E6%AF%94%E8%BE%83-%E8%BD%AC%E6%8D%A2/\nhttps://lvs071103.gitbooks.io/awk/content/awk_syntax_and_basic_commands/ofs-.html\nhttps://blog.csdn.net/github_36669230/article/details/64503589\nhttps://zhuanlan.zhihu.com/p/20197581\n","permalink":"https://sr-c.github.io/2018/10/12/convert-gff-to-bed/","tags":["gff","gtf","bed"],"title":"bed格式转换"},{"categories":null,"contents":"数据格式 Wide format为每列一个变量，也是常见的SPSS，excel等存储数据的格式。例如在基因表达数据中，每行一个基因，每列一个样品，构成一个数据矩阵，具有较强的人类可读性。\nLong format的最简格式为两列，第一列为变量名，第二列为数值。基因表达矩阵转换为long format后，由三列构成。第一列为基因ID，第二列为样品名，第三列则为基因表达量。此格式更方便用于数据分析。\n wide format     基因 分生组织 根 花     gene1 582 91 495   gene2 305 3505 33     long format     基因 组织 表达量     gene1 分生组织 582   gene2 分生组织 305   gene1 根 91   gene2 根 3503   gene1 花 492   gene2 花 33    大多数情况下，人类的实验原始记录常以wide format形式存储，而进行数据分析时，需要将其转换为更易操作的long format形式。\n解决方案 R的tidyr包就是数据清理的好帮手。tidyr使用gather()与spread()函数来改变数据格式。\n1 2 3 4 5 6 7 8 9  olddata_wide \u0026lt;- read.table(header=TRUE, text=\u0026#39; subject sex control cond1 cond2 1 M 7.9 12.3 10.7 2 F 6.3 10.6 11.1 3 F 9.5 13.1 13.8 4 M 11.5 13.4 12.9 \u0026#39;) # Make sure the subject column is a factor olddata_wide$subject \u0026lt;- factor(olddata_wide$subject)   From wide to long Use gather:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  olddata_wide #\u0026gt; subject sex control cond1 cond2 #\u0026gt; 1 1 M 7.9 12.3 10.7 #\u0026gt; 2 2 F 6.3 10.6 11.1 #\u0026gt; 3 3 F 9.5 13.1 13.8 #\u0026gt; 4 4 M 11.5 13.4 12.9 library(tidyr) # The arguments to gather(): # - data: Data object # - key: Name of new key column (made from names of data columns) # - value: Name of new value column # - ...: Names of source columns that contain values # - factor_key: Treat the new key column as a factor (instead of character vector) data_long \u0026lt;- gather(olddata_wide, condition, measurement, control:cond2, factor_key=TRUE) data_long #\u0026gt; subject sex condition measurement #\u0026gt; 1 1 M control 7.9 #\u0026gt; 2 2 F control 6.3 #\u0026gt; 3 3 F control 9.5 #\u0026gt; 4 4 M control 11.5 #\u0026gt; 5 1 M cond1 12.3 #\u0026gt; 6 2 F cond1 10.6 #\u0026gt; 7 3 F cond1 13.1 #\u0026gt; 8 4 M cond1 13.4 #\u0026gt; 9 1 M cond2 10.7 #\u0026gt; 10 2 F cond2 11.1 #\u0026gt; 11 3 F cond2 13.8 #\u0026gt; 12 4 M cond2 12.9   参考来源 http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/\nhttps://www.jianshu.com/p/e3eaea18650d\n","permalink":"https://sr-c.github.io/2018/10/11/convert-wide-formatted-data/","tags":["R","tidyr","data format"],"title":"【R】转换数据格式"},{"categories":null,"contents":"数据库下载 nr数据库下载前应确认其序列ID中包含有gi信息。推荐通过NCBI提供的脚本进行下载。\n1  perl update_blastdb.pl nr   下载完成后解压\n1  tar -zxvf *.tar.gz   建库 由于下载的是nr数据库建库后的文件，因此不需要再通过makeblastdb进行建库。\n此外，如果想要在blast结果中添加物种名称，则需要手动下载nr数据(FASTA格式)以及gi与taxid的映射文件，在makeblastdb时通过-taxid_map gi_taxid_nucl.dmp参数将映射信息包含在库中。\n其他 同时，使用blastdbcmd可以从库文件中提取序列\n1  blastdbcmd -db nr -dbtype prot -entry all -outfmt \u0026#34;%f\u0026#34; -out nr.fa   使用blast_formatter可以将asn格式(outfmt 11)转换为其他格式\n1  blast_formatter -archive \u0026#34;test.blastn@nr.asn\u0026#34; -outfmt \u0026#34;7 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore staxids salltitles\u0026#34; \u0026gt; \u0026#34;test.blastn@nr.tab\u0026#34;   参考来源 http://blog.shenwei.me/local-blast-installation/\nhttps://segmentfault.com/a/1190000012055972\n","permalink":"https://sr-c.github.io/2018/10/02/local-nr-database/","tags":["nr","ncbi","blast","gi","taxid"],"title":"本地化nr数据库"},{"categories":null,"contents":"问题描述 新建数据表时出现报错\n1  ERROR 1101 (42000): BLOB, TEXT, GEOMETRY or JSON column \u0026#39;description\u0026#39; can\u0026#39;t have a default value   这是由于mysql处于strict mode, 不允许text或blod字段有缺省值。因此，我们需要修改mysql为非严格模式。\n1 2 3 4  \u0026gt;show variables like \u0026#39;sql_mode\u0026#39;; | Variable_name | Value | sql_mode ONLY_FULL_GROUP_BY, STRICT_TRANS_TABLES, NO_ZERO_IN_DATE, NO_ZERO_DATE, ERROR_FOR_DIVISION_BY_ZERO, NO_AUTO_CREATE_USER, NO_ENGINE_SUBSTITUTION   解决方案 一般建议修改mysql的配置文件my.ini,将其中sql-mode这一行注释掉即可。 但在LinuxMint中，mysql的默认配置文件中没找到这一行。因此，手动在mysql中修改sql_mode\n1  \u0026gt;set session sql_mode=\u0026#39;ONLY_FULL_GROUP_BY,NO_ZERO_IN_DATE,NO_ZERO_DATE, ERROR_FOR_DIVISION_BY_ZERO\u0026#39;;   参考来源 https://yq.aliyun.com/sqlarticle/35671 https://segmentfault.com/a/1190000004512672\n","permalink":"https://sr-c.github.io/2018/09/29/mysql-ERROR-1101-42000/","tags":["debug","mysql"],"title":"mysql ERROR 1101 (42000)"},{"categories":null,"contents":"官方描述 alienness需要输入蛋白序列比对到nr数据库的结果文件作为输入\n其对blastp的需求如下\n   -option value Description     -outfmt 7 7 = tabular with comment lines !MANDATORY   -db nr BLAST database name For a better coverage of the biodiversity, NCBI\u0026rsquo;s nr library is recommanded but not necessary. The protein library must have gi or accession numbers that exist in the NCBI database.   -seg no The SEG program is used to mask or filter low complexity regions in amino acid queries   -evalue 1e-3 Expect value (E) for saving hits    记录 实际上，alienness需要结果文件第2列subject id中含有gi 或 accession numbers 使其识别subject id的物种来源。且其格式应当如下\n1  gi|831779554|ref|XP_012755758.1|   但有时blast输出的结果第二列为subject acc.ver，其内容只是accession number的一部分\n1  XP_018661062.1   如此，并不能得到我们需求的结果\n参考来源 http://alienness.sophia.inra.fr/cgi/faq.cgi\n","permalink":"https://sr-c.github.io/2018/09/27/HGT-alienness/","tags":["HGT","alienness"],"title":"使用alienness识别基因水平转移"},{"categories":null,"contents":"解决方案 目前找到最简便的解决方案\n其他方案都依赖使用Bioperl, BioPython, BioRuby等解决方案，不够灵活\n网页版解决方案，其中还有许多丰富功能，有待发现\n使用方法 1  python blastxml_to_tabular.py -o output.tabular -c std input.xml   其中std代表标准的12列格式，ext代表具有额外信息的25列格式\n参考来源 http://seqanswers.com/forums/showthread.php?t=29799\nhttps://www.biostars.org/p/7290/\n","permalink":"https://sr-c.github.io/2018/09/27/blastxml-to-tabular/","tags":["blast","python"],"title":"blast结果格式转换"},{"categories":null,"contents":"问题描述 对某个特定下载的数据库使用makeblastdb建库时，出现No volumes were created because no sequences were found.报错\n使用环境 BLAST 2.2.28+\n需要建库的fasta文件内容格式如下\n1 2 3 4  \u0026gt;gnl|TC-DB|1001796365|4.F.1.1.5 CDP-alcohol phosphatidyltransferase [Marinobacter excellens] xxxxxxxxxxxxxxxxxxxx \u0026gt;gnl|TC-DB|YP_009506345.1|9.B.308.4.1 p4 [Cordyline virus 1] xxxxxxxxxxxxxxxxxxxx   解决方案 删除序列名中重复的部分，删去gnl|即可运行makeblastdb\n似乎此bug在BLAST 2.2.30+版本后被修正，目前还未验证。\n参考来源 https://github.com/wurmlab/sequenceserver/issues/74\nhttps://www.biostars.org/p/115984/\n","permalink":"https://sr-c.github.io/2018/09/25/makeblastdb-fails/","tags":["debug","blast"],"title":"makeblastdb-fails"},{"categories":null,"contents":"导入错误 导入第一个数据库go_month-assocdb-data之后，再载入第二个数据库，却报告不存在b2gdb.gene2accession这个表\n1 2 3 4 5  ~/database/blast2go$ mysql -s -u root -p123456 b2gdb \u0026lt; go_monthly-assocdb-data mysql: [Warning] Using a password on the command line interface can be insecure. ~/database/blast2go$ mysql -u root -p123456 b2gdb -e \u0026#34;LOAD DATA LOCAL INFILE \u0026#39;/home/train/database/blast2go/gene2accession\u0026#39; INTO TABLE gene2accession FIELDS TERMINATED BY \u0026#39;\\t\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39;;\u0026#34; mysql: [Warning] Using a password on the command line interface can be insecure. ERROR 1146 (42S02) at line 1: Table \u0026#39;b2gdb.gene2accession\u0026#39; doesn\u0026#39;t exist   无奈之下，手动新建这个table\n1 2 3  CREATE TABLE gene2accession( `test_id` INT NOT NULL AUTO_INCREMENT ); CREATE TABLE gene2accession( `test_id` INT NOT NULL ); LOAD DATA LOCAL INFILE \u0026#39;/home/train/database/blast2go/gene2accession\u0026#39; INTO TABLE gene2accession FIELDS TERMINATED BY \u0026#39;\\t\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39;;   再查看这个表，却发现只导入了一列\n1 2 3 4 5 6 7 8 9 10 11 12 13  select * from gene2accession; ## 结果如下 | 2291892 | | 2291892 | | 2291892 | | 2301732 | | 2301732 | | 2301732 | | 2301732 | | 2301732 | | 2301732 | +---------+ 76783167 rows in set (2 min 49.13 sec)   参考来源 http://www.biotrainee.com/thread-1773-1-1.html http://blog.shenwei.me/local-blast2go-installation/ www.chenlianfu.com/?tag=blast2go\n","permalink":"https://sr-c.github.io/2018/09/22/blast2go-local-failed/","tags":["blast2go"],"title":"blast2go-local-failed"},{"categories":null,"contents":"安装 使用conda安装简单方便\n1 2 3  conda creat -n jupyter python=3.6 source activate jupyter conda install jupyter notebook   配置 1 2 3  jupyter notebook --generate-config\t##生成配置文件 jupyter notebook password\t##设置jupyter密码 vi ~/.jupyter/jupyter_notebook_config.py   设置如下\n1 2 3 4  c.NotebookApp.ip = \u0026#39;*\u0026#39; c.NotebookApp.open_browser = False c.NotebookApp.port = 8889\t##手动设置一个空闲的端口 c.NotebookApp.notebook_dir = u\u0026#39;/home/user1/jupyter\u0026#39; ##设置jupyter的工作目录   启动 后台启动Jupyter Notebook\n1  nohup jupyter notebook \u0026gt; ~/jupyter/jupyter.log 2\u0026gt;\u0026amp;1 \u0026amp;   其他 服务器的防火墙没有开放我们所需要的端口，不能够直接远程访问服务器的目的端口\n方法一 此时，我们可以在本地建立ssh隧道，进行连接\n1  ssh -N -f -L localhost:8000:localhost:8889 -p 22 username@host_ip   连接成功后，在本机浏览器访问\n1  localhost:8000   在网页输入之前设置的jupyter密码即可访问服务器端的Jupyter Notebook\n方法二 此外，我们也可以手动开放防火墙的指定端口\n1 2  iptables -I INPUT -p tcp --dport 8889 -j ACCEPT iptables save   参考来源 https://jupyter-notebook.readthedocs.io/en/latest/public_server.html\nhttp://blog.biochen.com/archives/1169\nhttps://blog.csdn.net/tuzixini/article/details/79105482\nhttps://zhuanlan.zhihu.com/p/20226040\n","permalink":"https://sr-c.github.io/2018/09/19/jupyter-notebook/","tags":["jupyter"],"title":"在服务器安装Jupyter Notebook"},{"categories":null,"contents":"安装 1  git clone https://github.com/apetkau/orthomcl-pipeline.git   Perl模块 通过cpanm安装\n1  cpanm BioPerl DBD::mysql DBI Parallel::ForkManager YAML::Tiny Set::Scalar Text::Table Exception::Class Test::Most Test::Warn Test::Exception Test::Deep Moose SVG Algorithm::Combinatorics   依赖软件  OrthoMCL 或 OrthoMCL Custom （能够自定义序列识别符的修改版本） BLAST (blastall, formatdb) 注意不是NCBI-blast+, 推荐使用2.2.26版本 MCL  依赖环境解决后能够顺利运行设置脚本\n1 2 3 4 5 6 7 8 9  $ perl scripts/orthomcl-pipeline-setup.pl Checking for Software dependencies... Checking for OthoMCL ... OK Checking for formatdb ... OK Checking for blastall ... OK Checking for mcl ... OK Wrote new configuration to orthomcl-pipeline/scripts/../etc/orthomcl-pipeline.conf Wrote executable file to orthomcl-pipeline/scripts/../bin/orthomcl-pipeline Please add directory orthomcl-pipeline/scripts/../bin to PATH   数据库设置 创建orthomcl用户 1 2 3 4 5  $ mysql -u root -p Enter password: mysql\u0026gt; GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, CREATE VIEW, INDEX, DROP on . to orthomcl; #创建用户并授权 mysql\u0026gt; set password for orthomcl@localhost = password(\u0026#39;orthomcl\u0026#39;); #设置用户密码 mysql\u0026gt; quit;   创建数据库并生成配置脚本 1 2 3 4  $ perl scripts/orthomcl-setup-database.pl --user orthomcl --password orthomcl --host localhost --database orthomcl --outfile orthomcl.conf Connecting to mysql and creating database **orthmcldb** on host orthodb with user orthomcl ...OK database orthmcl created ...OK Config file **orthomcl.conf** created.   若已有orthomcl数据库，则可添加--no-create-database参数，不新建数据库。\n测试数据 1 2 3 4 5 6 7 8 9  $ perl t/test_pipeline.pl -m orthomcl.conf -s fork -t /tmp Test using scheduler fork TESTING NON-COMPLIANT INPUT TESTING FULL PIPELINE RUN 3 README: Tests case of one gene (in 1.fasta and 2.fasta) not present in other files. ok 1 - Expected matched returned groups file ...   运行 1 2 3 4  $ ./bin/orthomcl-pipeline Error: no input-dir defined Usage: orthomcl-pipeline -i [input dir] -o [output dir] -m [orthmcl config] [Options] ...   参考来源 https://github.com/apetkau/orthomcl-pipeline/blob/master/INSTALL.md\nhttps://www.jianshu.com/p/449a51fa3d18\n","permalink":"https://sr-c.github.io/2018/09/18/OrthoMCL-Pipeline/","tags":["OrthoMCL"],"title":"自动化运行OrthoMCL"},{"categories":null,"contents":"R中的因子存在着有序和无序两种，默认按照ASCII顺序排序。\n对于无序因子 使用levels函数指定顺序，如下\n1 2 3 4 5 6 7 8 9 10  # 创建一个错误次序的因子  sizes \u0026lt;- factor(c(\u0026#34;small\u0026#34;, \u0026#34;large\u0026#34;, \u0026#34;large\u0026#34;, \u0026#34;small\u0026#34;, \u0026#34;medium\u0026#34;)) sizes #\u0026gt; [1] small large large small medium  #\u0026gt; Levels: large medium small # 顺序被直接指定 sizes \u0026lt;- factor(sizes, levels = c(\u0026#34;small\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;large\u0026#34;)) sizes #\u0026gt; [1] small large large small medium  #\u0026gt; Levels: small medium large   对于有序因子 1 2 3 4 5  sizes \u0026lt;- ordered(c(\u0026#34;small\u0026#34;, \u0026#34;large\u0026#34;, \u0026#34;large\u0026#34;, \u0026#34;small\u0026#34;, \u0026#34;medium\u0026#34;)) sizes \u0026lt;- ordered(sizes, levels = c(\u0026#34;small\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;large\u0026#34;)) sizes #\u0026gt; [1] small large large small medium  #\u0026gt; Levels: small \u0026lt; medium \u0026lt; large   小tips 快速逆序排列\n1  sizes \u0026lt;- factor(sizes, levels=rev(levels(sizes)))   参考来源 http://www.cookbook-r.com/Manipulating_data/Changing_the_order_of_levels_of_a_factor/\nhttps://www.jianshu.com/p/87ae057ae557\n","permalink":"https://sr-c.github.io/2018/09/16/Changing-the-order-of-levels-of-a-factor/","tags":["R"],"title":"【R】改变因子的排列顺序"},{"categories":null,"contents":"安装要求  64-bit Linux Perl 5 (default on most Linux distributions) Python 3 (InterProScan 5.30-69.0 onwards) Oracle\u0026rsquo;s Java JDK/JRE version 8 (InterProScan 5.17-56.0 onwards) Environment variables set  $JAVA_HOME should point to the location of the JVM $JAVA_HOME/bin should be added to the $PATH    下载与安装 1 2 3 4 5 6 7 8 9 10 11 12 13  # 下载核心程序 mkdir my_interproscan cd my_interproscan wget ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.30-69.0/interproscan-5.30-69.0-64-bit.tar.gz wget ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.30-69.0/interproscan-5.30-69.0-64-bit.tar.gz.md5 # 由于程序很大，推荐进行MD5校验，保证数据准确: md5sum -c interproscan-5.30-69.0-64-bit.tar.gz.md5 # Must return *interproscan-5.30-69.0-64-bit.tar.gz: OK* # If not - try downloading the file again as it may be a corrupted copy. # 解压程序，注意使用参数p保留文件的权限信息 tar -pxvzf interproscan-5.30-69.0-*-bit.tar.gz   1 2 3 4 5 6 7 8 9 10 11  # 下载Panther Models cd [InterProScan5 home]/data/ wget ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/data/panther-data-12.0.tar.gz wget ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/data/panther-data-12.0.tar.gz.md5 # 下载完成后进行MD5校验 md5sum -c panther-data-12.0.tar.gz.md5 # This must return *panther-data-12.0.tar.gz: OK* # If not - try downloading the file again as it may be a corrupted copy. cd [InterProScan5 home]/data # 在data文件夹中解压数据库 tar -pxvzf panther-data-12.0.tar.gz   运行测试数据 1 2 3 4 5 6  ./interproscan.sh # 短暂等待后，即会输出帮助信息 # 使用程序自带的测试数据进行测试 ./interproscan.sh -i test_proteins.fasta -f tsv # 程序不联网，使用本地的Lookup Service ./interproscan.sh -i test_proteins.fasta -f tsv -dp   提升程序运行效率 多线程设置 在单机运行InterProScan时，可以在配置文件interproscan.properties中设置运行时的线程数，其默认设置如下\n1 2 3 4 5  # Set the number of embedded workers to the number of processors that you would like to employ # on the machine you are using to run InterProScan. #number of embedded workers a master process can have number.of.embedded.workers=6 maxnumber.of.embedded.workers=8   对于一个40核的机器，推荐的设置如下\n1 2 3 4  #Number of embedded workers at start time number.of.embedded.workers=1 #Maximum number of embedded workers maxnumber.of.embedded.workers=35   将输入数据分组 推荐单次输入蛋白序列10000条，核酸序列1000条。若输入数据过多，建议将其分组运行。\n精简输出结果 使用-appl参数设置比对的项目，减少程序耗时\n常见问题 RPSBlast 错误 1  bin/blast/ncbi-blast-2.6.0+/rpsblast: error while loading shared libraries: libgnutls.so.28: cannot open shared object file: No such file or directory   这是由于InterProScan自带的blast+中rpsblast的编译环境与本机环境不同。我们可以通过替换这支程序来解决。下载ncbi提供的已编译版本，使用其中的rpsblast替换InterProScan自带的rpsblast即可。\n1 2 3 4 5 6 7 8  wget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/2.6.0/ncbi-blast-2.6.0+-x64-linux.tar.gz tar xvf ncbi-blast-2.6.0+-x64-linux.tar.gz # backup original version cp bin/blast/ncbi-blast-2.6.0+/rpsblast bin/blast/ncbi-blast-2.6.0+/rpsblast.bak # copy the new version to the binary folder cp ncbi-blast-2.6.0+/bin/rpsblast bin/blast/ncbi-blast-2.6.0+/rpsblast   序列输入错误 进行比对的蛋白序列中不应含有星号，否则将有如下报错。将序列中的星号删除即可。\n You have submitted a protein sequence which contains an asterix (*). This may be from an ORF prediction program. \u0026lsquo;*\u0026rsquo; is not a valid IUPAC amino acid character and amino acid sequences which go through our pipeline should not contain it. Please strip out all asterix characters from your sequence and resubmit your search.\n 参考来源 https://github.com/ebi-pf-team/interproscan/wiki/HowToDownload\nhttps://github.com/ebi-pf-team/interproscan/wiki/HowToRun\nhttps://github.com/ebi-pf-team/interproscan/issues/44\nhttps://github.com/ebi-pf-team/interproscan/wiki/FAQ\nhttps://github.com/ebi-pf-team/interproscan/wiki/ImprovingPerformance\n","permalink":"https://sr-c.github.io/2018/09/13/interproscan-config/","tags":["InterPro"],"title":"InterProScan的安装与使用"},{"categories":null,"contents":"重新安装mysql, 将其回复默认的配置\n删除mysql 1  sudo apt-get remove --purge mysql-*   清理残留数据 1  dpkg -l |grep ^rc|awk \u0026#39;{print $2}\u0026#39; |sudo xargs dpkg -P   或者更为手动的方式，首先搜索含有mysql文件名的路径\n1  sudo find / -name mysql -print   然后，使用sudo rm -rf执行删除\n安装mysql 1  sudo apt-get install mysql-server   安装过程中未提示设置管理员密码，因此，可通过如下方式直接登陆mysql\n1  sudo mysql -uroot   但在某些条件下，我们希望设置mysql管理员的密码\n1  ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;test\u0026#39;;    此处，test是为管理员账户设置的新密码 如此，mysql即可通过mysql -uroot -ptest方式登陆。\n 参考目录 https://www.jianshu.com/p/c76b31df5d09 https://blog.csdn.net/chudongfang2015/article/details/52154903 https://askubuntu.com/questions/1029177/error-1698-28000-access-denied-for-user-rootlocalhost-at-ubuntu-18-04 https://stackoverflow.com/questions/39281594/error-1698-28000-access-denied-for-user-rootlocalhost https://www.percona.com/blog/2016/03/16/change-user-password-in-mysql-5-7-with-plugin-auth_socket/\n","permalink":"https://sr-c.github.io/2018/09/11/mysql-reinstall/","tags":["mysql"],"title":"重装mysql"},{"categories":null,"contents":"查看mysql文件存放目录 1  show variables like \u0026#39;%dir%\u0026#39;;   能够查看到目前的数据库文件配置信息，其中的datadir值为/var/lib/mysql/\n停止mysql服务 1  sudo service mysql stop   迁移数据库文件 直接移动目录 1  sudo mv /var/lib/mysql /mnt/data/   拷贝目录（同时复制文件权限信息） 1  sudo cp -a /var/lib/mysql /mnt/data/   修改配置文件 my.cnf mysql数据库会按照优先级顺序从/etc/my.cnf, /etc/mysql/my.cnf, /usr/etc/my.cnf, ~/.my.cnf四个位置找my.cnf配置文件。Ubuntu默认将my.cnf配置问价放置在/etc/mysql/my.cnf位置，没有/etc/my.cnf文件。 在基于Ubuntu的LinuxMint19中，其/etc/mysql/my.cnf文件内容如下\n1 2  !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/   将配置指向了同级目录另两个文件夹中的配置文件，其中/etc/mysql/mysql.conf.d/mysqld.cnf设置了关键内容，在其[mysql]标签下，将datadir值改为新的路径，保存修改后退出。\nusr.bin.mysqld 由于Ubuntu使用的apparmor安全模块，还需修改apparmor的配置文件/etc/apparmor.d/usr.sbin.mysqld找到其中 /var/lib/mysql/ r, /var/lib/mysql/** rwk, 两行权限声明，修改为： /mnt/data/mysql/ r, /mnt/data/mysql/** rwk,\nabstractions/mysql 由于usr.bin.mysqld文件中引用了abstractions/mysql文件，因此还需要修改其中的设置。但在LinuxMint 19 中，并没有这个文件，因此，我们也就不做修改。\n重启数据库 重新载入apparmor配置文件\n1  sudo /etc/init.d/apparmor restart   再重启mysql即可\n1  sudo /etc/init.d/mysql start   参考来源 https://blog.csdn.net/qinxiandiqi/article/details/43270147\n","permalink":"https://sr-c.github.io/2018/09/11/change-mysql-datadir/","tags":["mysql","datadir"],"title":"更改mysql数据库路径"},{"categories":null,"contents":"这一情况使得用户使用mysql -u root -p登陆时出现MySql Error 1698(28000)错误。这与之前出现MySql Error 1698(28000)错误的原因不同，因此很多网上MySql Error 1698(28000)问题的解决方案不奏效。\n使用linuxMint安装MySQL Server 5.7, 在导入本地blast2go数据库时出现错误。\nMySql Error 1698(28000) 表征 1 2 3  mysql -u root -p Enter password: ERROR 1698 (28000): Access denied for user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39;(using password: NO)   进一步确认 默认root用户的plugin项为auth_socker，不是mysql_native_password，因此不能够输入密码进入。同时，也可以看到root用户的密码为空。\n1  sudo mysql -u root\t##如此方可进入刚安装的MySQL   1 2 3 4 5 6 7 8 9 10 11 12 13 14  mysql\u0026gt; USE mysql; mysql\u0026gt; SELECT User, Host, plugin FROM mysql.user; +------------------+-----------------------+ | User | plugin | +------------------+-----------------------+ | root | auth_socket | | mysql.sys | mysql_native_password | | debian-sys-maint | mysql_native_password | +------------------+-----------------------+ mysql\u0026gt; SELECT User, Host, HEX(authentication_string) FROM mysql.user; +-----------+-----------+-------------------------------------------------------------+ | User | Host | HEX(authentication_string) | +-----------+-----------+-------------------------------------------------------------+ | root | localhost | |    As you can see in the query, the root user is using the auth_socket plugin.And the password is empty.\n 解决方案 Step 1 Option 1: set the root user to use the mysql_native_password plugin 1 2 3 4 5 6 7 8  $ sudo mysql -u root # I had to use \u0026#34;sudo\u0026#34; since is new installation mysql\u0026gt; USE mysql; mysql\u0026gt; UPDATE user SET plugin=\u0026#39;mysql_native_password\u0026#39; WHERE User=\u0026#39;root\u0026#39;; mysql\u0026gt; FLUSH PRIVILEGES; mysql\u0026gt; exit; $ service mysql restart   Option 2: replace YOUR_SYSTEM_USER with the username you have 1 2 3 4 5 6 7 8 9 10  $ sudo mysql -u root # I had to use \u0026#34;sudo\u0026#34; since is new installation mysql\u0026gt; USE mysql; mysql\u0026gt; CREATE USER \u0026#39;YOUR_SYSTEM_USER\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;\u0026#39;; mysql\u0026gt; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;YOUR_SYSTEM_USER\u0026#39;@\u0026#39;localhost\u0026#39;; mysql\u0026gt; UPDATE user SET plugin=\u0026#39;auth_socket\u0026#39; WHERE User=\u0026#39;YOUR_SYSTEM_USER\u0026#39;; mysql\u0026gt; FLUSH PRIVILEGES; mysql\u0026gt; exit; $ service mysql restart   Remember that if you use option #2 you\u0026rsquo;ll have to connect to mysql as your system username (mysql -u YOUR_SYSTEM_USER)\nNote: On some systems (e.g., Debian stretch) \u0026lsquo;auth_socket\u0026rsquo; plugin is called \u0026lsquo;unix_socket\u0026rsquo;, so the corresponding SQL command should be: UPDATE user SET plugin='unix_socket\u0026rsquo; WHERE User='YOUR_SYSTEM_USER\u0026rsquo;;\nStep 2 向root用户添加密码 1 2  mysql\u0026gt; ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;your_password\u0026#39;; mysql\u0026gt; SELECT User, Host, HEX(authentication_string) FROM mysql.user; ##查看密码是否添加成功   终极一键解决方案 1  mysql\u0026gt; ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;your_password\u0026#39;;   And now, it works 🙂\n1 2 3 4 5  mysql\u0026gt; SELECT User, Host, HEX(authentication_string) FROM mysql.user; +-----------+-----------+------------------------------------------------------------------------------------+ | User | Host | HEX(authentication_string) | +-----------+-----------+------------------------------------------------------------------------------------+ | root | localhost | 2A39344244434542453139303833434532413146393539464430324639363443374146344346433239 |   重新安装mysql 对于5.5版本生效，但对于5.7版本MySQL，重新安装的过程中仍然不会向用户要求设置root密码，因此并不奏效。\n1 2 3 4  sudo apt-get purge mysql-server mysql-client mysql-common mysql-server-core-5.5 mysql-client-core-5.5 sudo rm -rf /etc/mysql /var/lib/mysql sudo apt-get autoremove sudo apt-get autoclean   再重新安装mysql-server，此时可在安装时设置root密码\n1 2  sudo apt-get update sudo apt-get install mysql-server   参考来源 https://www.percona.com/blog/2016/03/16/change-user-password-in-mysql-5-7-with-plugin-auth_socket/ https://stackoverflow.com/questions/39281594/error-1698-28000-access-denied-for-user-rootlocalhost\n","permalink":"https://sr-c.github.io/2018/09/09/mysql-config/","tags":["mysql","debug"],"title":"配置MySQL Server"},{"categories":null,"contents":"数据导入 安装软件包 1 2  install.packages(\u0026#34;rafalib\u0026#34;) install.packages(\u0026#34;downloader\u0026#34;)   下载课程地址中的femaleMiceWeights.csv，并导入数据到R中。\n方法一 直接下载文件到工作目录下，然后使用read.csv导入数据\n1  dat \u0026lt;- read.csv(\u0026#34;femaleMiceWeights.csv\u0026#34;)   方法二 1 2 3 4 5 6  library(downloader) ##use install.packages to install url \u0026lt;- \u0026#34;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv\u0026#34; filename \u0026lt;- \u0026#34;femaleMiceWeights.csv\u0026#34; download(url, destfile=filename) dat \u0026lt;- read.csv(filename)   数据清理dplyr 读入femaleMiceWeights.csv的数据后，筛选我们想要的数据\n1 2 3 4 5  library(dplyr) chow \u0026lt;- filter(dat, Diet==\u0026#34;chow\u0026#34;) #keep only the ones with chow diet head(chow) chowVals \u0026lt;- select(chow,Bodyweight) #选择chow中Bodyweight这一列   在dplyr中可以使用管道符号%\u0026gt;%来构建连续任务，上述步骤可以在一步完成\n1  chowVals \u0026lt;- filter(dat, Diet==\u0026#34;chow\u0026#34;) %\u0026gt;% select(Bodyweight)   上述命令不会改变数据的性质，若想要将列表转变为向量，则应使用unlist函数\n1  chowVals \u0026lt;- filter(dat, Diet==\u0026#34;chow\u0026#34;) %\u0026gt;% select(Bodyweight) %\u0026gt;% unlist   summarize函数 该函数的作用是将多个数值降维至单一数值，用法如下\nsummarise(.data, \u0026hellip;)\n Useful functions\nCenter: mean(), median()\nSpread: sd(), IQR(), mad()\nRange: min(), max(), quantile()\nPosition: first(), last(), nth(),\nCount: n(), n_distinct()\nLogical: any(), all()\n 1  dat %\u0026gt;% summarise(mean = mean(sleep_total)) %\u0026gt;% unlist   数学符号 编号，求和，希腊字母\n希腊字母 μ , the Greek letter for m (m is for mean)\nσ , the Greek letter for s(s is for standard deviation )\nε , the Greek letter for e(e is for measurement error )\nβ , Effect sizes\n∞  In the text we often talk about asymptotic results. Typically, this refers to an approximation that gets better and better as the number of data points we consider gets larger and larger, with perfect approximations occurring when the number of data points is ∞. In practice, there is no such thing as ∞, but it is a convenient concept to understand. One way to think about asymptotic results is as results that become better and better as some number increases and we can pick a number so that a computer can’t tell the difference between the approximation and the real number. Here is a very simple example that approximates 1/3 with decimals:\n 1 2 3 4 5 6 7 8 9  onethird \u0026lt;- function(n) sum( 3/10^c(1:n)) 1/3 - onethird(4) ## [1] 3.333333e-05 1/3 - onethird(10) ## [1] 3.333334e-11 1/3 - onethird(16) ## [1] 0   此时，16即是无穷大\n积分 ∫42f(x)dx\n1 2 3 4 5 6  width \u0026lt;- 0.01 x \u0026lt;- seq(2,4,width) areaofbars \u0026lt;- f(x)*width sum( areaofbars ) ## [1] 0.02298998   参考来源 http://genomicsclass.github.io/book/pages/dplyr_intro.html\nhttp://genomicsclass.github.io/book/pages/dplyr_intro_exercises.html\nhttp://genomicsclass.github.io/book/pages/math_notation.html\n","permalink":"https://sr-c.github.io/2018/09/06/study-notes-PH525x-Chapter0/","tags":["PH525x","笔记"],"title":"PH525x学习笔记Ⅰ"},{"categories":null,"contents":"直接编译AUGUSTUS 1 2 3 4 5 6 7 8 9 10 11 12  make[2]: Entering directory `$HOME/software/augustus-3.3.1/auxprogs/bam2hints\u0026#39; make[2]: warning: jobserver unavailable: using -j1. Add `+\u0026#39; to parent make rule. g++ -Wall -O2 -c bam2hints.cc -o bam2hints.o -I/usr/include/bamtools bam2hints.cc:16:10: fatal error: api/BamReader.h: No such file or directory #include \u0026lt;api/BamReader.h\u0026gt; ^~~~~~~~~~~~~~~~~ compilation terminated. make[2]: *** [bam2hints.o] Error 1 make[2]: Leaving directory `$HOME/software/augustus-3.3.1/auxprogs/bam2hints\u0026#39; make[1]: *** [all] Error 2 make[1]: Leaving directory `$HOME/software/augustus-3.3.1/auxprogs\u0026#39; make: *** [all] Error 2   直接编译AUGUSTUS会在编译bam2hints时出错。因此，我们需要安装bamtools。\n安装bamtools 一键解决方案 1  sudo apt-get install bamtools libbamtools-dev   但是，我们没有root权限，只能手动编译。\n普通用户安装bamtools需要指定安装目录 1 2 3 4 5 6 7 8  wget https://github.com/pezmaster31/bamtools/archive/v2.4.1.tar.gz -O bamtools-2.4.1.tar.gz tar zxf bamtools-2.4.1.tar.gz cd bamtools-2.4.1 mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/your/path/to/bamtools .. make make install   我目前的服务器环境比较复杂。CentOS 6.9，其默认的GCC版本是4.4.7，在编译很多软件时版本过低。目前将其升级到了7.3.0，但未保留原本的4.4.7环境。因此，在编译bamtools过程中有些挑剔软件版本。\n对于≤2.4.0版本的bamtools，编译时都会出现如下报错。≥2.4.2版本的bamtools则要求cmake版本高于3.0，而本机的cmake版本是2.8.12.2。因此，在尽量少折腾的原则下，我们选择2.4.1版本。\n编译bamtools-2.4.1 安装完成后，运行bamtools程序，提示缺少libbamtools.so.2.4.1\n1 2  $ ~/opt/bamtools-2.4.1/bin/bamtools $HOME/opt/bamtools-2.4.1/bin/bamtools: error while loading shared libraries: libbamtools.so.2.4.1: cannot open shared object file: No such file or directory   find ~/opt -name libbamtools.so.2.4.1搜寻该文件。然后将libbamtools.so.2.4.1软链接到~/opt/bamtools-2.4.1/lib/目录下，即可正常启动bamtools\n修改bam2hints 与filterBam的Makefile，使其与手动编译的bamtools配合 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  First, go to the “./auxprogs/bam2hints” directory and make the following changes for the Makefile: Add: BAMTOOLS = $HOME/opt/bamtools-2.4.1 Replace: INCLUDES = /usr/include/bamtools By: INCLUDES = $(BAMTOOLS)/include Replace: LIBS = -lbamtools -lz By: LIBS = $(BAMTOOLS)/lib/bamtools/libbamtools.a -lz Then, go to the “augustus-3.2.3/auxprogs/filterBam/src” directory and make the following changes for the Makefile: Replace: BAMTOOLS = /usr/include/bamtools By: BAMTOOLS = $HOME/opt/bamtools-2.4.1 Replace: INCLUDES = -I$(BAMTOOLS) -Iheaders -I./bamtools By: INCLUDES = -I$(BAMTOOLS)/include -Iheaders -I./bamtools Replace: LIBS = -lbamtools -lz By: LIBS = $(BAMTOOLS)/lib64/libbamtools.a -lz   编译bam2wig 编译好bamtools后，应当向系统目录拷贝一些库文件，但限于我们没有root权限，只能修改Makefile，手动指向这些库文件。\n编译bam2wig依赖bcftools, htslib, samtools, tabix四款软件 从其github站点，或官网下载，解压后直接编译。其中，tabix已经并入了htslib\n1 2 3  ./configure --prefix=/where/to/install make make install   修改bam2wig的Makefile，使其与上述软件配合 1 2 3 4 5 6 7 8 9 10 11  # SAMTOOLS=$(TOOLDIR)/samtools/ SAMTOOLS=$HOME/software/samtools-1.9 #HTSLIB=$(TOOLDIR)/htslib/ HTSLIB=$HOME/software/htslib-1.9 # BCFTOOLS=$(TOOLDIR)/bcftools/ BCFTOOLS=$HOME/software/bcftools-1.9 # TABIX=$(TOOLDIR)/tabix/ TABIX=$HOME/software/htslib-1.9    注意，此处的路径$HOME/software/其实是解压后的软件包编译的路径，并不是软件最终安装的路径。否则，会出现如下报错。\n 1 2 3 4 5 6 7  make[2]: Entering directory `$HOME/software/augustus-3.3.1/auxprogs/bam2wig\u0026#39; make[2]: warning: jobserver unavailable: using -j1. Add `+\u0026#39; to parent make rule. gcc -Wall -O2 -I$HOME/opt/samtools-1.8/ -I. -I$HOME/opt/htslib-1.9/ -I$HOME/opt/bcftools-1.9/ -I$HOME/opt/htslib-1.9/ -c bam2wig.c -o bam2wig.o bam2wig.c:18:10: fatal error: sam.h: No such file or directory #include \u0026#34;sam.h\u0026#34; ^~~~~~~ compilation terminated.   编译bam2wig过程中报错 1 2 3  hfile_s3.c:(.text+0x8d7): undefined reference to `EVP_sha1\u0026#39; hfile_s3.c:(.text+0x8fd): undefined reference to `HMAC\u0026#39; collect2: error: ld returned 1 exit status   解决方案\n1 2  hfile_libcurl.c:(.text+0x12aa): undefined reference to `curl_global_cleanup\u0026#39; collect2: error: ld returned 1 exit status   解决方案\n因此，我们还需修改bam2wig路径中的Makefile，向其中的LIBS项末尾添加-lcrypto -lcurl两个字段补充缺少的库文件。\n再次编译AUGUSTUS 1 2 3 4 5 6 7  ##清除之前未成功编译的中间文件，回复初始状态 make clean ##多线程编译缩短时间 make -j 8 echo \u0026#39;PATH=$PATH:~/opt/augustus/bin:~/opt/augustus/scripts\u0026#39; \u0026gt;\u0026gt; ~/.bashrc ##可选步骤，手动设置配置文件夹的路径。如未设置，默认会搜寻可执行文件相对路径../config中的文件。 echo \u0026#39;AUGUSTUS_CONFIG_PATH=/my_path_to_AUGUSTUS/augustus/config/\u0026#39; \u0026gt;\u0026gt; ~/.bashrc   总结 经过上述步骤，在普通用户权限下解决好bam2hints与bam2wig的依赖，并修改其Makefile其实适应手动安装的配置环境，从而完成普通用户的AUGUSTUS安装。\n参考来源 https://github.com/Gaius-Augustus/Augustus\nhttps://genehub.wordpress.com/2018/05/16/augustus-3-3%E7%9A%84%E5%AE%89%E8%A3%85/\nhttps://iamphioxus.org/2017/05/08/installing-augustus-with-manual-bamtools-installation/\n","permalink":"https://sr-c.github.io/2018/09/04/augustus-install/","tags":["AUGUSTUS","bamtools","debug"],"title":"AUGUSTUS的普通用户安装"},{"categories":null,"contents":"软件下载 虽然在Korf Lab的官网上提供最新版本(latest release 11/29/2013)的下载，但此版本在本机(CENTOS 6.9, GCC )编译时出现错误，改而从github站点下载，成功完成编译。\n1 2 3 4 5  git clone https://github.com/KorfLab/SNAP.git cd SNAP make -j 4 echo \u0026#39;PATH=$PATH:~/opt/SNAP\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc   SNAP Training 需要旧版本EVM提供的一支程序将GFF3文件转换为ZFF格式。\n1 2 3  cd ~/software wget https://ayera.dl.sourceforge.net/project/evidencemodeler/OLD_CONTENT/evidencemodeler/EVM_r2012-06-25.tgz tar zxf EVM_r2012-06-25.tgz -C ~/opt/   SNAP Training 转换ZFF文件\n1 2  export PERL5LIB=$PERL5LIB:~/opt/EVM_r2012-06-25/PerlLib/ ~/opt/EVM_r2012-06-25/OtherGeneFinderTrainingGuide/SNAP/gff3_to_SNAP_train.pl species.gff3 species_genome.fasta   参考来源 https://github.com/KorfLab/SNAP\n","permalink":"https://sr-c.github.io/2018/09/02/SNAP-install/","tags":["SNAP","ab initio"],"title":"SNAP软件安装"},{"categories":null,"contents":"软件下载 1 2  wget http://www.ebi.ac.uk/~birney/wise2/wise2.4.1.tar.gz tar zxf ~/software/wise2.4.1.tar.gz -C ~/opt/   软件编译 1 2 3 4 5 6 7 8  cd /opt/biosoft/wise2.4.1/src/ cd HMMer2 sed \u0026#39;s/getline/getline_new/\u0026#39; sqio.c \u0026gt; a \u0026amp;\u0026amp; mv a sqio.c cd .. perl -p -i -e \u0026#39;s/isnumber/isdigit/\u0026#39; models/phasemodel.c make all echo \u0026#39;PATH=$PATH:~/opt/wise2.4.1/src/bin/\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;WISECONFIGDIR=~/opt/wise2.4.1/wisecfg/\u0026#39; \u0026gt;\u0026gt; ~/.bashrc   这里修改了两个文件HMMer2/sqio.c与models/phasemodel.c，否则在编译过程中会有相应报错。\n注意事项 安装前应判断环境中是否存在glib或glib-2.0。\n若环境中存在glib，则直接按照上述步骤即可完成安装。\n若现环境中使用的不是glib，而是glib2，我们可以不安装glib库，修改软件的makefile文件，直接使用glib2编译。但Genewise不止含有一个makefile文件，故通过如下方法进行批量修改。\n1 2  find ./ -name makefile | xargs sed -i \u0026#39;s/glib-config/pkg-config --libs glib-2.0/\u0026#39; export C_INCLUDE_PATH=/usr/include/glib-2.0/:/usr/lib64/glib-2.0/include/:$C_INCLUDE_PATH   系统版本 1  Linux version 2.6.32-696.16.1.el6.centos.plus.x86_64 (mockbuild@c1bl.rdu2.centos.org) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-18) (GCC) ) #1 SMP Wed Nov 15 18:52:54 UTC 2017   参考来源 https://jingyan.baidu.com/article/e2284b2b5947e7e2e7118d54.html\nhttp://blog.51cto.com/wjpinrain/1077147\n","permalink":"https://sr-c.github.io/2018/09/01/genewise-install/","tags":["debug","install","GeneWise"],"title":"GeneWise的安装"},{"categories":null,"contents":"批量化生成trimmomatic运行脚本 对于paired-end数据，trimmomatic官方推荐的运行方式如下\n java -jar trimmomatic-0.35.jar PE -phred33 input_forward.fq.gz input_reverse.fq.gz output_forward_paired.fq.gz output_forward_unpaired.fq.gz output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n 其中输入输出的六个文件依次在命令行中排列，大量数据处理时，手动编辑的工作量巨大。\n输出运行脚本 1 2  #ls *.fq.gz | xargs -i echo java -jar trimmomatic-0.35.jar PE {}_forward.fq.gz {}_reverse.fq.gz {}_forward_paired.fq.gz {}_forward_unpaired.fq.gz {}_reverse_paired.fq.gz {}_reverse_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:5 TRAILING:5 SLIDINGWINDOW:4:5 MINLEN:25 \u0026gt; trimmomatic.sh awk \u0026#39;{print $2}\u0026#39; ../sample.txt | xargs -i echo java -jar trimmomatic-0.35.jar PE {}_forward.fq.gz {}_reverse.fq.gz {}_forward_paired.fq.gz {}_forward_unpaired.fq.gz {}_reverse_paired.fq.gz {}_reverse_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:5 TRAILING:10 SLIDINGWINDOW:4:5 MINLEN:36 \u0026gt; trimmomatic.sh   参考来源 http://www.genek.tv/article/25\n","permalink":"https://sr-c.github.io/2018/08/31/xargs-tips/","tags":["xargs","awk"],"title":"xargs-tips"},{"categories":null,"contents":"使用Jellyfish 统计k-mer   下载并安装: http://www.genome.umd.edu/jellyfish.html#Release\n  准备数据\n参照陈连福的方法，提前将paired-end数据整合。将read2数据的序列反向重复后与read1文件合并。这一步使用Trinity附带的fastool程序完成转换。\n  1 2 3  $ zcat read_1.clean.fq.gz | $Trinity_Home/trinity-plugins/fastool/fastool --illumina-trinity --to-fasta \u0026gt; reads_1.fasta $ zcat read_2.clean.fq.gz | $Trinity_Home/trinity-plugins/fastool/fastool --rev --illumina-trinity --to-fasta \u0026gt; reads_2.fasta $ cat reads_1.fasta reads_2.fasta \u0026gt; both.fasta   统计kmers  1  $ jellyfish count -C -m 21 -s 1000000000 -t 10 both.fasta -o reads.jf   绘制直方图  1  $ jellyfish histo -t 10 reads.jf \u0026gt; reads.histo    -m是kmer长度；\n-s是预估哈希表的大小，即G+Gce*k。G是Genome Size；c是coverage（genome survey测序通常低于100x）；e是测序错误率（illumina为1%）；k是kmer大小。\n-C表示考虑DNA正义与反义链，遇到反义kmer时，计入正义kmer频数中。\n Running GenomeScope Online 在线作图http://genomescope.org/\nRunning GenomeScope on the Command Line 1  $ Rscript genomescope.R histogram_file k-mer_length read_length output_dir [kmer_max] [verbose]   参考来源 https://github.com/schatzlab/genomescope\nhttps://gif.biotech.iastate.edu/genomescope\nhttp://www.chenlianfu.com/?p=806\nhttps://fengleiblog.wordpress.com/2014/12/17/%E5%9F%BA%E4%BA%8Eillumina%E6%B5%8B%E5%BA%8F%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%BD%BF%E7%94%A8jellyfish%E4%BC%B0%E8%AE%A1%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%A4%A7%E5%B0%8F/\n","permalink":"https://sr-c.github.io/2018/08/30/genome-scope/","tags":["GenomeScope","Jellyfish"],"title":"使用GenomeScope进行基因组评估"},{"categories":null,"contents":"查看单个fq.gz文件中的reads数目 使用zcat命令即可直接查看fq.gz文件的内容。而fastq文件中，每一条read记录占用4行。因此，查看单个文件的reads数目可如下实现\n1  zcat your_raw_data.fastq.gz | grep -c \u0026#39;+\u0026#39;   或者统计文件内容的行数，除以4即为reads数目。\n1  zcat your_raw_data.fastq.gz | wc -l   直接通过fq.gz文件大小进行判断 参考Lablueee同志的研究，gzip的压缩效率对于同一测序平台的数据文件效率相近，因此可通过数据文件的大小与数据量的线性关系推断测序数据量。\n 因为R1和R2数据量相同的原因，我只看R1的真实文件和gz文件大小与数据量之间的关系。\n数据量=FASTQ文件行数/4*151/1000/1000 单位为M\n真实文件大小估计=FASTQ文件行数/4*357/1024/1024 单位为M，预测值，差别不大，因为FASTQ文件中每四行357个字符（和平台和设置有关系），每个字符1byte。\nGZ文件大小通过ll -h查看\n因为FASTQ文件是规范的，每四行字符基本一致，所以FASTQ真实文件大小和数据量成正比。比如我前面提到的每四行有357个字符，其中序列只占151个字符，也就是说FASTQ文件大小大概是测序量的357/151≈2.3倍多。但因为FASTQ文件为文本文件，占用空间较大，所以一般将FASTQ文件压缩成gzip格式文件。\n 根据其线性拟合结果，（对于PE150数据）Miseq R1文件的数据量大概是gzip文件的1.242倍（1/0.35/2.3），Hiseq约在1.89(1/0.23/2.3)倍。\n查看多个fq.gz文件中的reads数目 1  zcat ???.cb_R1.fastq.gz | grep -c \u0026#39;^+\u0026#39; \u0026gt; AAA.txt   或者直接写一个循环：\n1  ls *fastq.gz|while read i;do echo \u0026#34;$i\u0026#34;;zcat $i |wc -l ;done   参考来源 http://www.omicsclass.com/question/145\nhttp://www.zxzyl.com/archives/1011\n","permalink":"https://sr-c.github.io/2018/08/30/count-reads-number/","tags":["fastq","qc"],"title":"快速了解fastq.gz文件中的reads数目"},{"categories":null,"contents":"解决方案 无管理员权限则修改客户端的ssh配置\n1 2 3 4  ##编辑ssh配置文件，无此文件则新建 vi ~/.ssh/config ##在其中添加如下语句 ServerAliveInterval 60   使得客户端在ssh连接时，每隔60秒发送一个keep-alive包\n使用rsync下载 虽然通过上述设置能够避免长时间传输文件时scp异常掉线的问题，但对于大文件传输，仍不能实现断点续传，这时就需要使用同步工具rsync来进行服务器间的同步了。\n1  rsync -Rr 文件夹名 ip::目标模块   选项说明  -v, \u0026ndash;verbose 详细模式输出 -q, \u0026ndash;quiet 精简输出模式 -c, \u0026ndash;checksum 打开校验开关，强制对文件传输进行校验 -a, \u0026ndash;archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD -r, \u0026ndash;recursive 对子目录以递归模式处理 -R, \u0026ndash;relative 使用相对路径信息\n 1 2 3 4  rsync foo/bar/foo.c remote:/tmp/ ##则在/tmp目录下创建foo.c文件，而如果使用-R参数： rsync -R foo/bar/foo.c remote:/tmp/ ##则会创建文件/tmp/foo/bar/foo.c，也就是会保持完全路径信息。   参考来源 https://www.logcg.com/archives/897.html\nhttps://www.cnblogs.com/dudu/archive/2013/02/07/ssh-write-failed-broken-pipe.html\nhttp://blog.51cto.com/1inux/1751255\nhttp://www.dahouduan.com/2014/11/19/rsync-daemon/\nhttp://www.cnblogs.com/MikeZhang/p/rsyncExample_20160818.html\nhttps://blog.csdn.net/hepeng597/article/details/8960885\n","permalink":"https://sr-c.github.io/2018/08/14/Write-failed-Broken-pipe/","tags":["ssh","scp","debug"],"title":"SCP遇到\"Write failed Broken pipe\""},{"categories":null,"contents":"超净工作台的防护罩材质 一般超净台的防护罩采用的都是有机玻璃，聚甲基丙烯酸甲酯（poly(methyl methacrylate)，简称PMMA） ，又称做压克力、亚克力（英文Acrylic）或有机玻璃、Lucite（商品名称） 。根据一些厂商的宣传资料，现在也有改进型的防护罩材质，使用有机玻璃、热塑性聚氨酯TPU胶粘层、无色抗紫外线聚氨酯薄膜层等复合材料。\n一般认为PMMA能够有效滤除波长小于300 nm的紫外光，但对300-400 nm之间的紫外光滤除效果较差。\n超净工作台紫外发生器的波段 一般超净工作台的紫外杀菌灯都是低压汞灯，其发射波长为185nm 与253.7 nm。\n有机玻璃的光谱吸收曲线 https://i.stack.imgur.com/gTY0G.gif\nhttps://physics.stackexchange.com/questions/74412/why-does-nasa-use-gold-foil-on-equipment-and-gold-coated-visors\nhttp://www.hindawi.com/journals/ijp/2009/150389.fig.002.jpg\n钠钙玻璃的光谱吸收曲线 https://pic1.zhimg.com/80/de1df99a97a070b77487cb47513d5814_hd.jpg\n Q: 低压汞灯灯管本身的玻璃是否会吸收紫外波段？\n  A: 汞蒸气灯管都由石英制成，灯头材质多采用胶木、塑料或陶瓷。 石英玻璃几乎不吸收紫外波段，故成为汞蒸气灯管的合适材料。\n 参考来源 https://www.zhihu.com/question/20720208 http://www.shiying.org/news/1059.html https://www.reddit.com/r/askscience/comments/3p88vp/how_is_plexiglass_pmma_made_to_be_uv_and_ir/\n","permalink":"https://sr-c.github.io/2018/08/03/pmma-uv-transmittance/","tags":["pmma","uv"],"title":"超净工作台的防护罩能阻挡紫外辐射么？"},{"categories":null,"contents":"Pilon使用read比对分析识别输入的基因组序列与reads证据之间的不一致，尝试修正基因组序列中的差错，包括：\n  单碱基变异 小的插入缺失 较大的插入缺失或片段替换事件（block substitution events ） 补洞 识别错误的本地组装，包括打开新的gaps   输入数据 Pilon的输入数据有两个，FASTA格式的基因组文件以及将测序数据比对到基因组上的BAM文件。其中，BAM文件需要提前sort并建立索引（be sorted in coordinate order and indexed）。对于Illumina数据，一般使用BWA或Bowtie 2进行序列比对。\n To use Pilon with default arguments, read length is recommended to be 75 bases or longer and total sequence coverage should be 50x or greater, though deeper total coverage of \u0026gt;100x is beneficial.\n 要使用默认参数运行Pilon，推荐输入数据的reads读长大于75 bp，全序列的覆盖度应大于50x，当然覆盖度更高（大于100x）就更好了。Pilon能够使用小片段，大片段或单端测序数据作为输入，并且可以同时输入多个文库的数据。Pilon能够有效地利用reads之间的配对信息，因此强烈建议以双端测序文库数据作为输入。\n输入文件 Pilon产生一个修正后的基因组序列，包括了所有的单碱基变异，Indel等修正。\nPilon能产生一个VCF格式的文件，记录了基因组中每个位点的SNP或InDel信息。经本地重新组装得到的变化记录在相应的标记(SVTYPE=INS and SVTYPE=DEL) 中。Pilon还能够产生一个“changes”文件，以表格方式记录了从输入基因组到输出基因组之间的编辑操作，两者之间的一致性序列，以及两个各自的序列。最后，Pilon还能够产生一系列可视化的记录文件 (“bed” and “wig” files) ，用于诸如IGV(17)和GenomeView(18)基因组浏览器的可视化。记录文件包括了序列的覆盖度（sequence coverage and physical coverage），每个位点匹配到的有效reads的百分比。\nPilon的标准输出也含有许多有用的信息，包括覆盖度，输入基因组的确认度，修正信息的汇总，以及一些特殊标记，却未得到修正的问题。\n资源消耗  Larger genomes will require more memory to process; exactly how much is very data-dependent, but as a rule of thumb, try to allocate 1GB per megabase of input genome to be processed.\n 因此，100M的基因组就约需要100G的内存消耗。105M基因组，输入clean data5G数据量实测内存占用峰值达到82.5G，对于大型基因组来说，消耗惊人。\n参考来源 https://github.com/broadinstitute/pilon/wiki/Requirements-\u0026amp;-Usage\nhttps://github.com/broadinstitute/pilon/wiki/Methods-of-Operation\n","permalink":"https://sr-c.github.io/2018/07/16/pilon/","tags":["Pilon"],"title":"Pilon的使用"},{"categories":null,"contents":"环境变量 系统环境变量 我们知道，我们经常要设置一些环境变量，系统环境变量我们非常容易理解。其实我们在windows中经常容易接触。其实环境变量是一个非常广泛的一个概念，它与web应用程序中的web.config所处的角色很像。什么意思呢？就是说，程序（系统或应用）要运行的时候，它的基本业务逻辑可能是一定的，但是实现业务逻辑的时候有些设置性的东西却可以改变程序很多。如web应用程序，编译之后他的业务逻辑基本不会发生改变，但是如果你更改一些web.config中的参数，程序的运行就会发生相应的改变。这些设置。就像电视机上面调制一样。改变了设置会得到某些不同。\n那么环境变量可以理解成设置的一种，为什么有不直接称为设置呢？因为它处于一种被动的境地。越多说越糊涂。\n最常见的环境变量莫过于PATH，和ClassPATH，这个在设置jdk的时候就需要设置，这里的PATH变量指的是，当系统的接口接收到一个程序启动命令的时候，除了默认在当前目录下寻找那个可执行文件意外，还需要到那些地方寻找。有了这个设置，你就不需要一定要进入那个目录才能执行那个程序了。ClassPATH变量也差不多，它设置的是那些类似于动态库的路径，也就是说，程序在执行的时候，发现要引入动态库，那样就要在这个变量指定的地方去找。\n在linux中，系统也有一个PATH变量。其实系统有一个文件是专门记录那些环境变量的。\n1）/etc/profile，系统登录会执行这个文件在当前环境中引入那些变量。\n2）还有 /home/ali/.bashrc这个文件，简单的来说，/etc/profile是对全局有效的，而./bashrc是对当前用户有效.\n3）还有一种设置方法，就是通过终端命令直接修改，我们知道前面两个文件其作用的方式就是当程序进入状态的时候，他们会被执行引入到当前空间，那么在当前状态下就会有这些变量，程序也就是可以使用它们。那么如果我们直接在内存中修改该他们，就可以起到暂时的作用。\n程序环境变量 根据前面我们说过环境变量的作用和意义，就很容易推出，普通的程序也可以有环境变量。按照前面系统的环境变量起作用的模式。应用程序，也可以有一些配置文件来持久保存这些环境变量，在程序执行的时候，这些变量会通过某种方式进入程序执行的空间，这样程序执行的时候就可以使用这些变量了。而同样，我们可以改变这些变量来“适量”的改变我们的程序。\nGCC就是这样一个程序。很多时候，应用程序需不需要环境变量机制，关键看他是否有很多的选择性，GCC就是一个这样的程序。\n最常用GCC环境变量的就是include搜索路径，以及库搜索路径。他们分别在编译和连接的时候使用。\n他们的使用背景是：\ninclude搜索路径 通常，使用C/C++进行开发程序的时候，会使用头文件，并且有头文件的实现文件，这个时候有三类文件，使用头文件的源文件，头文件，实现头文件的源文件。编译的时候，头文件和源文件一起就可以了。通常他们是在同一目录下的。所以不会有什么问题。\n但是，当你使用到了系统自身的一些头文件的时候，你需要引入一些头文件，而这些文件不在当前目录下，使用绝对地址是一个办法，但是是一个极差的办法。所以GCC就有一个搜索机制。就是在规定的那些文件夹下，搜索你所引入的那个头文件。这样解决了问题。这个环境变量叫着CPLUS_INCLUDE_PATH。属于GCC，与系统无关。我们看看GCC头文件搜索路径\n头文件：\n   #include“headfile.h”\r   搜索顺序为：\n①先搜索当前目录\n②然后搜索-I指定的目录\n③再搜索gcc的环境变量CPLUS_INCLUDE_PATH（C程序使用的是C_INCLUDE_PATH）\n④最后搜索gcc的内定目录\n/usr/include\n/usr/local/include\n/usr/lib/gcc/x86_64-redhat-linux/4.1.1/include\n各目录存在相同文件时，先找到哪个使用哪个。\n  #include\u0026lt;headfile.h\u0026gt;\r   ①先搜索-I指定的目录\n②然后搜索gcc的环境变量CPLUS_INCLUDE_PATH\n③最后搜索gcc的内定目录\n/usr/include\n/usr/local/include\n/usr/lib/gcc/x86_64-redhat-linux/4.1.1/include\n与上面的相同，各目录存在相同文件时，先找到哪个使用哪个。这里要注意，#include\u0026lt;\u0026gt;方式不会搜索当前目录！\n虽然搜索了GCC自定义的环境变量目录之后，下一个的内定目录，就应该是操作系统有关这种头文件的定义。这种推导很正确。事实上就算不是这样的。GCC头文件搜索模式，也是按照先“专”后“宽”的模式，也就是说，大部分都是使用自己的一套，所以基本都能找到，可能真有一些是那些大家共有的头文件。所以，这里的内定目录其实与继承操作系统的目录的意思没有多大区别。\n这里要说下include的内定目录，它不是由$PATH环境变量指定的，而是由g++的配置prefix指定的(知道它在安装g++时可以指定，不知安装后如何修改的，可能是修改配置文件，需要时再研究下)：\n-bash-3.2$ g++ -v\nUsing built-inspecs.\nTarget:x86_64-redhat-linux\nConfigured with:../configure \u0026ndash;prefix=/usr \u0026ndash;mandir=/usr/share/man\u0026ndash;infodir=/usr/share/info \u0026ndash;enable-shared \u0026ndash;enable-threads=posix\u0026ndash;enable-checking=release \u0026ndash;with-system-zlib \u0026ndash;enable-__cxa_atexit\u0026ndash;disable-libunwind-exceptions \u0026ndash;enable-libgcj-multifile\u0026ndash;enable-languages=c,c++,objc,obj-c++,java,fortran,ada\u0026ndash;enable-java-awt=gtk \u0026ndash;disable-dssi \u0026ndash;enable-plugin\u0026ndash;with-java-home=/usr/lib/jvm/java-1.4.2-gcj-1.4.2.0/jre\u0026ndash;with-cpu=generic \u0026ndash;host=x86_64-redhat-linux\nThread model:posix\ngcc version 4.1.2 20080704(Red Hat 4.1.2-46)\n在安装g++时，指定了prefix，那么内定搜索目录就是：\nPrefix/include\nPrefix/local/include\nPrefix/lib/gcc/\u0026ndash;host/\u0026ndash;version/include\n编译时可以通过-nostdinc++选项屏蔽对内定目录搜索头文件。\n库搜索路径 在编译之后，程序要进行链接操作，前面指出，链接不管是动态和是静态，GCC这个程序，必须确认“真的有”那些头文件的实现。于是就需要定位找到那些文件。与include的情景差不多。使用绝对目录是可以的，但不适于管理。于是就出现了lib搜索路径这个环境变量。LIBRARY_PATH。\n他们的搜索路径为：\n库文件：\n编译的时候：\n①gcc会去找-L\n②再找gcc的环境变量LIBRARY_PATH\n③再找内定目录/lib /usr/lib/usr/local/lib 这是当初compile gcc时写在程序内的（不可配置的？）\n运行时动态库的搜索路径\n(不要把这个和库的搜索路径混淆了，这里程序执行的时候有linux系统/usr/bin/ld程序控制的过程，这里只是顺带介绍。以完整程序整个生命周期。编译、链接、启动，装载（包括动态装载)、执行）：\n动态库的搜索路径搜索的先后顺序是：\n①编译目标代码时指定的动态库搜索路径（这是通过gcc 的参数\u0026rdquo;-Wl,-rpath,\u0026ldquo;指定。当指定多个动态库搜索路径时，路径之间用冒号\u0026rdquo;：\u0026ldquo;分隔）\n②环境变量LD_LIBRARY_PATH指定的动态库搜索路径（当通过该环境变量指定多个动态库搜索路径时，路径之间用冒号\u0026rdquo;：\u0026ldquo;分隔）\n③配置文件/etc/ld.so.conf中指定的动态库搜索路径；\n④默认的动态库搜索路径/lib；\n⑤默认的动态库搜索路径/usr/lib。\n我们在来对GCC这个命令的这一方面进行总结一下。\n编译的时候使用-I命令可以装入include搜索路径。\n连接的时候使用-l -L命令可以装入连接搜索路径或文件\n执行的时候。在当初编译连接时候使用-Wl 这个可以装入动态库的搜索路径。\n参考来源 http://blog.sina.com.cn/s/blog_93b45b0f01011nrz.html\n","permalink":"https://sr-c.github.io/2018/07/16/LIBRARY-PATH/","tags":["include","library","path","gcc","linux","环境变量"],"title":"【转载】环境变量，include搜索路径，lib库搜索路径"},{"categories":null,"contents":"安装PASA 1 2 3 4 5 6 7 8  cd ~/software/ wget https://github.com/PASApipeline/PASApipeline/releases/download/pasa-v2.3.3/PASApipeline-v2.3.3.tar.gz tar zxf PASApipeline-v2.3.3.tar.gz -C ~/opt/ mv ~/opt/PASApipeline-pasa-v2.3.3 ~/opt/pasa-v2.3.3 cd ~/opt/pasa-v2.3.3 make echo \u0026#39;$PASAHOME=~/opt/pasa-v2.3.3/\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc   安装perl模块 安装PASA最大的困难就在于perl模块的配置。本文参考使用perlbrew与cpanm配置perl模块\n1  cpanm DBD::SQLite   conda的perl和系统的perl冲突 有一次我遇到这个问题\n1  perl: symbol lookup error: /home/wangjw/perl5/lib/perl5/x86_64-linux-thread-multi/auto/Cwd/Cwd.so: undefined symbol   这个问题显然我用系统perl安装的软件被conda的perl有限查找到导致，用perl -V和perl -e '{print \u0026quot;$_\\n\u0026quot; foreach @INC}'可以发现conda的perl查找路径低于我为系统perl安装的路径，解决方案如下\n1  export PERL5LIB=\u0026#34;\u0026#34;   报错解决 安装过程中由于之前配置的conda环境的干扰，使得在测试环境时出现报错，提示缺少perl模块支持。使用cpanm安装模块，却出现编译错误，提示缺少 db.h，但/usr/include/db.h存在，一时未能排除BUG。\n整理思路后发觉应当在~/.bashrc中屏蔽conda环境后重新配置。\n但仍然出现/usr/bin/perl: xxx symbol lookup error错误，报错原因应当是原conda环境中的perl安装的DBD::SQLite模块与现有环境不兼容。因此，需要先卸载已安装的模块，再重新安装。\n p.s. 其他解决方案，未验证\n perl模块的卸载 cpan工具只管理模块的安装，却不管卸载。使用App::pmuninstall模块来方便地卸载perl模块\n1  cpanm App::pmuninstall   使用很方便\n1  pm-uninstall DBD::SQLite   此外，还可使用 App::pmodinfo模块查看perl模块的相关信息，常用参数说明如下。（安装过程依赖38个模块，较耗时）\n -v \u0026ndash;version\n-f \u0026ndash;full\n-h \u0026ndash;hash\n-l \u0026ndash;local-modules\n-u \u0026ndash;check-updates\n 安装GMAP 安装BLAT 安装FASTA 1 2 3 4 5 6 7 8 9  cd ~/software wget http://faculty.virginia.edu/wrpearson/fasta/fasta36/fasta-36.3.8g.tar.gz tar zxf fasta-36.3.8g.tar.gz -C ~/opt/ cd ~/opt/fasta-36.3.8g/src/ make -f ../make/Makefile.linux_sse2 all cd ../bin/ ln -s fasta36 fasta echo \u0026#39;PATH=$PATH:~/opt/fasta-36.3.8g/bin/\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc    按照《NGS生物信息分析v6.0》，不应当安装fasta36或以上版本，否则运行PASA进行GFF3文件更新时，会调用fasta命令，导致内存耗尽，同时得不到结果。（pasa-v2.2.0）\n不知目前版本（v2.3.3）不知是否修复了此BUG，留此存照。\n 配置UniVec数据库 seqclean在v2.3.3版本中默认位于$PASAHOME/bin/路径中，不需另行安装。因此只需安装其依赖的数据库即可，在此以UniVec数据库为例。（UniVec应当能够满足大部分需求）\n1 2 3 4 5  mkdir ~/database/UniVec cd ~/database/UniVec wget ftp://ftp.ncbi.nlm.nih.gov/pub/UniVec/UniVec formatdb -t UniVec -i UniVec -p F -o T $PASAHOME/bin/seqclean transcripts.fasta -v ~/database/UniVec/UniVec   环境测试 1 2  cd $PATHHOME/sample_data ./runMe.SQLite.sh   测试运行约1h后得到输出结果\n sample_mydb_pasa.assemblies.fasta\nsample_mydb_pasa.pasa_assemblies.gff3\nsample_mydb_pasa.pasa_assemblies_described.txt\n 参考来源 https://github.com/PASApipeline/PASApipeline/wiki/Pasa_installation_instructions\nhttps://blog.csdn.net/u011450367/article/details/41522729\nhttp://stackoverflow.com/questions/7777252/uninstall-all-perl-modules-installed-by-cpan\nconda的perl和系统的perl冲突\n","permalink":"https://sr-c.github.io/2018/07/15/pasa-install/","tags":["pasa","perl"],"title":"PASA的安装"},{"categories":null,"contents":"设置日志文件级别 添加log-level项目\n1 2 3  log-level=warn OR log-level=error   删去设置中的log项 去除aria2.conf文件中的log项，程序运行时即不记录日志文件。\n参考来源 https://www.jianshu.com/p/6adf79d29add\nhttp://www.hostloc.com/thread-446920-1-1.html\n","permalink":"https://sr-c.github.io/2018/07/10/aria2-log-level/","tags":["aria2"],"title":"设置aria2日志级别"},{"categories":null,"contents":"解决方案 设置-更多应用-googleplay服务-管理空间-清除所有数据\n重新打开Google Play，问题解决。\n 前提条件：保证本机能够连接到Google服务。\n 参考来源 http://www.miui.com/thread-4320975-1-1.html\n","permalink":"https://sr-c.github.io/2018/07/10/googleplay-login/","tags":["debug"],"title":"Google Play出现登陆错误"},{"categories":null,"contents":"一、查找 查找命令\n1 2  /pattern\u0026lt;Enter\u0026gt; ：向下查找pattern匹配字符串 ?pattern\u0026lt;Enter\u0026gt;：向上查找pattern匹配字符串   使用了查找命令之后，使用如下两个键快速查找： n：按照同一方向继续查找 N：按照反方向查找 字符串匹配 pattern是需要匹配的字符串，例如：\n1 2  /abc\u0026lt;Enter\u0026gt; #查找abc / abc \u0026lt;Enter\u0026gt; #查找abc单词（注意前后的空格）   除此之外，pattern还可以使用一些特殊字符，包括（/、^、$、*、.），其中前三个这两个是vi与vim通用的，/为转义字符。\n1 2 3  /^abc\u0026lt;Enter\u0026gt; #查找以abc开始的行 /test$\u0026lt;Enter\u0026gt; #查找以abc结束的行 //^test\u0026lt;Enter\u0026gt; #查找^tabc字符串   二、替换 基本替换\n1 2 3 4 5 6 7  :s/vivian/sky/ #替换当前行第一个 vivian 为 sky :s/vivian/sky/g #替换当前行所有 vivian 为 sky :n,$s/vivian/sky/ #替换第 n 行开始到最后一行中每一行的第一个 vivian 为 sky :n,$s/vivian/sky/g #替换第 n 行开始到最后一行中每一行所有 vivian 为 sky （n 为数字，若 n 为 .，表示从当前行开始到最后一行） :%s/vivian/sky/ #（等同于 :g/vivian/s//sky/） 替换每一行的第一个 vivian 为 sky :%s/vivian/sky/g #（等同于 :g/vivian/s//sky/g） 替换每一行中所有 vivian 为 sky   可以使用 #或+ 作为分隔符，此时中间出现的 /不会作为分隔符\n1 2  :s#vivian/#sky/# ##替换当前行第一个 vivian/ 为 sky/ :%s+/oradata/apras/+/user01/apras1+ ##（使用+替换/）/oradata/apras/替换成/user01/apras1/   删除文本中的^M\n 问题描述：对于换行，window下用回车换行（0A0D）来表示，linux下是回车（0A）来表示。这样，将window上的文件拷到unix上用时，总会有个^M，请写个用在unix下的过滤windows文件的换行符（0D）的shell或c程序。\n 使用命令：cat filename1 | tr -d “^V^M” \u0026gt; newfile; 使用命令：sed -e “s/^V^M//” filename \u0026gt; outputfilename 需要注意的是在1、2两种方法中，^V和^M指的是Ctrl+V和Ctrl+M。你必须要手工进行输入，而不是粘贴。 在vi中处理：首先使用vi打开文件，然后按ESC键，接着输入命令：\n1 2  :%s/^V^M// :%s/^M$//g   如果上述方法无用，则正确的解决办法是：\n1 2 3 4  tr -d “/r” \u0026lt; src \u0026gt;dest tr -d “/015″ dest strings A\u0026gt;B   其他用法\n1 2 3 4 5  :s/str1/str2/ #用字符串 str2 替换行中首次出现的字符串 str1 :s/str1/str2/g #用字符串 str2 替换行中所有出现的字符串 str1 :.,$ s/str1/str2/g #用字符串 str2 替换正文当前行到末尾所有出现的字符串 str1 :1,$ s/str1/str2/g #用字符串 str2 替换正文中所有出现的字符串 str1 :g/str1/s//str2/g #功能同上   从上述替换命令可以看到：\n g 放在命令末尾，表示对指定行的搜索字符串的每次出现进行替换；不加 g，表示只对指定行的搜索字符串的首次出现进行替换；\ng 放在命令开头，表示对正文中所有包含搜索字符串的行进行替换操作。\n 即：命令的开始可以添加影响的行，如果为g表示对所有行；命令的结尾可以使用g来表示是否对每一行的所有字符串都有影响。\n三、简单的vim正则表达式规则 在vim中有四种表达式规则：\n magic(/m)：除了$.*^之外其他元字符都要加反斜杠\nnomagic(/M)：除了$^之外其他元字符都要加反斜杠\n/v（即 very magic 之意）：任何元字符都不用加反斜杠\n/V（即 very nomagic 之意）：任何元字符都必须加反斜杠\n vim默认使用magic设置，这个设置也可以在正则表达式中通过 /m /M /v /V开关临时切换。例如：\n1 2 3 4 5 6 7  //m.* # 查找任意字符串 //M.* # 查找字符串 .* （点号后面跟个星号） //v(a.c){3}$ # 查找行尾的abcaccadc //m(a.c){3}$ # 查找行尾的(abc){3} //M(a.c){3}$ # 查找行尾的(a.c){3} //V(a.c){3}$ # 查找任意位置的(a.c){3}$   推荐使用默认的magic设置，在这种情况下，常用的匹配有：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  //\u0026lt;abc #查找以test开始的字符串 /abc/\u0026gt; #查找以test结束的字符串 $ 匹配一行的结束 ^ 匹配一行的开始 //\u0026lt;abc\u0026lt;Enter\u0026gt;:查找以abc开始的字符串 /\u0026gt; 匹配一个单词的结束，例如/abc/\u0026gt;\u0026lt;Enter\u0026gt;:查找以abc结束的字符串 * 匹配0或多次 /+ 匹配1或多次 /= 匹配0或1次 . 匹配除换行符以外任意字符 /a 匹配一个字符 /d 匹配任一数字 /u 匹配任一大写字母 [] 匹配范围，如t[abcd]s 匹配tas tbs tcs tds /{} 重复次数，如a/{3,5} 匹配3~5个a /( /) 定义重复组，如a/(xy/)b 匹配ab axyb axyxyb axyxyxyb ... /| 或，如：for/|bar 表示匹配for或者bar /%20c 匹配第20列 /%20l 匹配第20行   关于正则表达式的详细信息，请参见参考文献。\n参考文献：\nvi中的正则表达式 vim运用正则表达式进行查找替换 vi替换字符串\n参考来源 https://blog.csdn.net/lanxinju/article/details/5731843\nhttp://man.linuxde.net/vi\nhttps://harttle.land/2016/08/08/vim-search-in-file.html\n","permalink":"https://sr-c.github.io/2018/07/06/vim-tips/","tags":["vim","vi","unix"],"title":"vim使用的小技巧"},{"categories":null,"contents":"下载aria2 1  https://github.com/aria2/aria2/releases/download/release-1.34.0/aria2-1.34.0.tar.gz   编译安装 1 2 3  ./configure --prefix=~/opt/aria2 make make install   浏览器附加组件 Firefox https://addons.mozilla.org/zh-CN/firefox/addon/baidu-pan-exporter/\nChrome https://github.com/acgotaku/BaiduExporter\naria2配置文件 1 2 3 4  mkdir download cd download touch aria2.session aria2.log vi aria2.conf   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  ## \u0026#39;#\u0026#39;开头为注释内容, 选项都有相应的注释说明, 根据需要修改 ## ## 被注释的选项填写的是默认值, 建议在需要修改时再取消注释 ## ## 文件保存相关 ## # 文件的保存路径(可使用绝对路径或相对路径), 默认: 当前启动位置 dir=~/download/ # 启用磁盘缓存, 0为禁用缓存, 需1.16以上版本, 默认:16M disk-cache=32M # 文件预分配方式, 能有效降低磁盘碎片, 默认:prealloc # 预分配所需时间: none \u0026lt; falloc ? trunc \u0026lt; prealloc # falloc和trunc则需要文件系统和内核支持 # NTFS建议使用falloc, EXT3/4建议trunc, MAC 下需要注释此项 file-allocation=none # 断点续传 continue=true ## 下载连接相关 ## # 最大同时下载任务数, 运行时可修改, 默认:5 max-concurrent-downloads=10 # 同一服务器连接数, 添加时可指定, 默认:1 max-connection-per-server=5 # 最小文件分片大小, 添加时可指定, 取值范围1M -1024M, 默认:20M # 假定size=10M, 文件为20MiB 则使用两个来源下载; 文件为15MiB 则使用一个来源下载 min-split-size=10M # 单个任务最大线程数, 添加时可指定, 默认:5 split=20 # 整体下载速度限制, 运行时可修改, 默认:0 #max-overall-download-limit=0 # 单个任务下载速度限制, 默认:0 #max-download-limit=0   之后即可使用aria2c --conf-path=\u0026lt;PATH\u0026gt;启动，其中PATH必须为绝对路径。\n参考来源 https://github.com/acgotaku/BaiduExporter\nhttps://blog.csdn.net/gatieme/article/details/44782801\nhttp://www.bioinfo-scrounger.com/archives/543\n","permalink":"https://sr-c.github.io/2018/07/04/aria2-baiduyun/","tags":["aria2"],"title":"百度云的aria2下载"},{"categories":null,"contents":"下载测试数据 1 2  curl -L -o pacbio.fastq http://gembox.cbcb.umd.edu/mhap/raw/ecoli_p6_25x.filtered.fastq curl -L -o oxford.fasta http://nanopore.s3.climb.ac.uk/MAP006-PCR-1_2D_pass.fasta   一步执行 1 2 3 4  canu \\  -p ecoli -d ecoli-pacbio \\  genomeSize=4.8m \\  -pacbio-raw pacbio.fastq   canu可分步骤执行 Correct 1 2 3 4  canu -correct \\  -p ecoli -d ecoli \\  genomeSize=4.8m \\  -pacbio-raw pacbio.fastq   Trim 1 2 3 4  canu -trim \\  -p ecoli -d ecoli \\  genomeSize=4.8m \\  -pacbio-corrected ecoli/ecoli.correctedReads.fasta.gz   Assemble 1 2 3 4 5 6 7 8 9 10 11  canu -assemble \\  -p ecoli -d ecoli-erate-0.039 \\  genomeSize=4.8m \\  correctedErrorRate=0.039 \\  -pacbio-corrected ecoli/ecoli.trimmedReads.fasta.gz canu -assemble \\  -p ecoli -d ecoli-erate-0.075 \\  genomeSize=4.8m \\  correctedErrorRate=0.075 \\  -pacbio-corrected ecoli/ecoli.trimmedReads.fasta.gz   应补足的参数 1 2 3 4 5 6 7 8 9 10 11  useGrid=false \\  gnuplotImageFormat=canvas \\ or gnuplotTested=true ##跳过gnuplot检测，在运行过程中不再使用gnuplot生成图片 ## 如不添加上方gnuplot参数，则容易在程序启动前自检中出现如下报错 ##（此问题似乎在1.7.1版本修复） ##ERROR: Failed to detect a suitable output format for gnuplot. ##ERROR: Looked for png, svg and gif, found none of them. ## java SE 8   Consensus Accuracy Canu consensus sequences are typically well above 99% identity for PacBio datasets. Nanopore accuracy varies depending on pore and basecaller version, but is typically above 98% for recent data. Accuracy can be improved by polishing the contigs with tools developed specifically for that task. We recommend Quiver for PacBio and Nanopolish for Oxford Nanpore data. When Illumina reads are available, Pilon can be used to polish either PacBio or Oxford Nanopore assemblies.\nFor high coverage:\n  For more than 60X coverage, decrease the allowed difference in overlaps (from 4.5% to 4.0% with correctedErrorRate=0.040 for PacBio, from 14.4% to 12% with correctedErrorRate=0.12 for Nanopore), so that only the better corrected reads are used. This is primarily an optimization for speed and generally does not change assembly continuity.   参考来源 http://canu.readthedocs.io/en/latest/faq.html#what-parameters-should-i-use-for-my-reads\nhttps://github.com/marbl/canu/issues/939\n","permalink":"https://sr-c.github.io/2018/07/04/canu/","tags":["canu"],"title":"canu的使用"},{"categories":null,"contents":"编译安装时报错 zlib.h:no such file or directory\n网络搜索出的解决方案大多是安装zlib\n1  yum install zlib-devel   但实际上zlib已经安装\n1  rpm search zlib | grep zlib    zlib library files are placed into /usr/local/lib and zlib header files are placed into /usr/local/include, by default.\n 解决方案 实际是本用户的conda环境影响了编译。zlib这样系统底层的库，系统并没有缺失。在~/.bashrc中屏蔽了conda的字段后，注销重新登陆，重新编译就pass了。\n参考来源 https://blog.csdn.net/u012724150/article/details/54836441\n","permalink":"https://sr-c.github.io/2018/07/02/debug-zlib-h/","tags":["debug"],"title":"解决zlib.h:no such file or directory"},{"categories":null,"contents":"语法 1 2  awk \u0026#39;program\u0026#39; input files awk -f progfile optional list of files   简单的单行awk命令格式如下，其命令由一个单独的 模式–动作 语句 (pattern-action statement) 组成\n1  awk \u0026#39;[pattern] {action}\u0026#39; {filenames} # 行匹配语句 awk \u0026#39;\u0026#39; 只能用单引号   AWK程序的结构 每一个awk程序都是由一个或多个模式–动作 语句组成的序列:\n1 2 3  pattern { action } pattern { action } ...   awk的基本操作是在由输入行组成的序列中, 陆续地扫描每一行, 搜索可以被模式 匹配 (match) 的行。 “匹配” 的精确含义依赖于问题中的模式, 比如, 对于$3 \u0026gt; 0, 意味着 “条件为真”.\n每一个输入行轮流被每一个模式测试。 每匹配一个模式, 对应的动作 (可能包含多个步骤) 就会执行。然后下一行被读取, 匹配重新开始。这个过程会一起持续到所有的输入被读取完毕为止。 上面的程序是模式与动作的典型例子. 程序\n $3 == 0 { print $1 }\n 由一条单独的模式–动作语句组成: 如果某行的第 3 个字段为 0, 那么它的第 1 个字段就会被打印出来.\n内建变量 awk从它的输入中每次读取一行，将行分解为一个个的字段 (默认将字段看作是非空白字符组成的序列)。当前输入行的第一个字段叫作$1, 第二个是$2, 依次类推，一整行记为$0. 每行的字段数有可能不一样。\nNF, 当前输入行的字段数量\nNR, 到目前为止，读取到的行的数量\n标准化输出 printf语句具有形式\n printf(format, value 1 , value 2 , \u0026hellip; , value n )\n{ printf(\u0026ldquo;total pay for %s is $%.2f\\n\u0026rdquo;, $1, $2 * $3) }\n format是一个字符串, 它包含按字面打印的文本, 中间散布着格式说明符, 格式说明符用于说明如何打印值. 一个格式说明符是一个 %, 后面跟着几个字符, 这些字符控制一个value的输出格式. 第一个格式说明符说明 value 1的输出格式, 第二个格式说明符说明 value 2的输出格式, 依次类推。 于是, 格式说明符的数量应该和被打印的 value 一样多。\n格式说明符 %s, 以字符串的形式打印\n%.2f , 按照数值格式打印，且带有两位小数\n%-8s , 将_value_以左对齐字符形式输出，占用8个字符的宽度\n%6.2f , 将_value_以带有两位小数的数值形式输出，至少占用6个字符的宽度\n参考来源 The AWK Programming Language (Aho, Kernighan, Weinberger 著, 中文名: AWK 程序设计语言)\nhttps://github.com/wuzhouhui/awk\n","permalink":"https://sr-c.github.io/2018/06/30/awk/","tags":["awk"],"title":"使用awk提取文件内容"},{"categories":null,"contents":"安装 1 2  install.package(\u0026#34;pheatmap\u0026#34;) library(pheatmap)   读入数据 1 2  setwd(\u0026#34;path/to/my/workdir/\u0026#34;) data \u0026lt;- read.delim(\u0026#34;mydata.txt\u0026#34;, header = T, sep = \u0026#34;\\t\u0026#34;, row.names = 1)   开始绘制 1  pheatmap(as.matrix(data), color=colorRampPalette(c(\u0026#34;green\u0026#34;,\u0026#34;black\u0026#34;,\u0026#34;red\u0026#34;))(100), scale=\u0026#39;row\u0026#39;, border_color=NA, width=9, height=18, fontsize=9, fontsize_row=6, filename=\u0026#34;heatmap.pdf\u0026#34; )   参数设置 1 2 3 4 5 6  scale=\u0026#39;row\u0026#39; #对矩阵进行标准化，以row或column方向进行标准化，默认参数为none cluster_row, cluster_col #默认对行列方向均进行聚类，设置为FALSE可不进行相应方向的聚类 treeheight_row=0, treeheight_col=0 #不显示dendrogram border_color=NA #热图的每个小块间默认以灰色隔开，设置NA去掉border legend=FALSE #不显示legend color=colorRampPalette(rev(c(\u0026#34;red\u0026#34;,\u0026#34;black\u0026#34;,\u0026#34;green\u0026#34;)))(102) #设定cell的颜色   将数值显示在热图的格子中 number_format设置数值的格式，较常用的有 \u0026ldquo;%.2f\u0026rdquo;（保留小数点后两位），\u0026quot;%.1e\u0026rdquo;（科学计数法显示，保留小数点后一位）\nnumber_color设置显示内容的颜色\n1 2 3 4 5  pheatmap(test, display_numbers = TRUE, number_format = \u0026#34;%.2f\u0026#34;, number_color=\u0026#34;purple\u0026#34;) #\u0026#34;%.2f\u0026#34;表示保留小数点后两位 # 在热图格子中展示文本 pheatmap(test, display_numbers = TRUE) pheatmap(test, display_numbers = TRUE, number_format = \u0026#34;\\%.1e\u0026#34;) pheatmap(test, display_numbers = matrix(ifelse(test \u0026gt; 5, \u0026#34;*\u0026#34;, \u0026#34;\u0026#34;), nrow(test)))#还可以自己设定要显示的内容   改变每个格子的大小 pheatmap(test, cellwidth = 15, cellheight = 12, main = \u0026ldquo;Example heatmap\u0026rdquo;, fontsize = 8, filename = \u0026ldquo;test.pdf\u0026rdquo;)\nmain可设置热图的标题，fontsize设置字体大小，filename可直接将热图存出，支持格式png, pdf, tiff, bmp, jpeg，并且可以通过width, height设置图片的大小\n1 2 3 4 5 6 7 8  #pheatmap还可以显示行或列的分组信息，支持多种分组； annotation_col = data.frame(CellType = factor(rep(c(\u0026#34;CT1\u0026#34;, \u0026#34;CT2\u0026#34;), 5)), Time = 1:5) rownames(annotation_col) = paste(\u0026#34;Test\u0026#34;, 1:10, sep = \u0026#34;\u0026#34;) annotation_row = data.frame(GeneClass = factor(rep(c(\u0026#34;Path1\u0026#34;, \u0026#34;Path2\u0026#34;, \u0026#34;Path3\u0026#34;), c(10, 4, 6)))) rownames(annotation_row) = paste(\u0026#34;Gene\u0026#34;, 1:20, sep = \u0026#34;\u0026#34;) pheatmap(test, annotation_col = annotation_col, annotation_row = annotation_row)   1 2 3 4 5 6  #设定各个分组的颜色 ann_colors = list(Time = c(\u0026#34;white\u0026#34;, \u0026#34;firebrick\u0026#34;), #连续数值型分组可设置成渐变 CellType = c(CT1 = \u0026#34;#1B9E77\u0026#34;, CT2 = \u0026#34;#D95F02\u0026#34;), GeneClass = c(Path1 = \u0026#34;#7570B3\u0026#34;, Path2 = \u0026#34;#E7298A\u0026#34;, Path3 = \u0026#34;#66A61E\u0026#34;)) pheatmap(test, annotation_col = annotation_col, annotation_row = annotation_row, annotation_colors = ann_colors)   根据特定条件将热图分隔开 cutree_rows, cutree_cols 根据行列的聚类数将热图分隔开 pheatmap(test, cutree_rows=3, cutree_cols=2)\n1 2 3 4 5 6 7  #还可以自己设定各个分组的颜色 ann_colors = list(Time = c(\u0026#34;white\u0026#34;, \u0026#34;firebrick\u0026#34;), #连续数值型分组可设置成渐变 CellType = c(CT1 = \u0026#34;#1B9E77\u0026#34;, CT2 = \u0026#34;#D95F02\u0026#34;), GeneClass = c(Path1 = \u0026#34;#7570B3\u0026#34;, Path2 = \u0026#34;#E7298A\u0026#34;, Path3 = \u0026#34;#66A61E\u0026#34;)) pheatmap(test, annotation_col = annotation_col, annotation_row = annotation_row, annotation_colors = ann_colors) #还可以利用gaps_row, gaps_col自己设定要分隔开的位置 pheatmap(test, annotation_col = annotation_col, cluster_rows = FALSE, gaps_row = c(10, 14), cutree_col = 2)   指定取色范围 默认的取色范围就是数据的取值范围range(data)，而常常我们会需求特定的取值范围，或者指定中间特定值（如0）的颜色。那么就需要使用breaks参数指定取色范围，并根据break范围设定颜色范围。\n1 2 3 4 5 6 7 8  #breaks bk \u0026lt;- c(seq(-9,-0.1,by=0.01),seq(0,9,by=0.01)) #pheatmap pheatmap(data1, scale = \u0026#34;none\u0026#34;, color = c(colorRampPalette(colors = c(\u0026#34;blue\u0026#34;,\u0026#34;white\u0026#34;))(length(bk)/2),colorRampPalette(colors = c(\u0026#34;white\u0026#34;,\u0026#34;red\u0026#34;))(length(bk)/2)), legend_breaks=seq(-8,8,2), breaks=bk)   设置legend的大小 pheatmap关于legend仅提供了3个参数，而如何设定legend的大小并不方便。\n legend 是否显示legend legend_breaks 以向量方式设定breakpoint legend_labels 以向量方式提供breakpoint的标签  一个讨巧的方法是通过设定字体的方式来调整，增大字体可以改变legend的大小。而文本的字体可以使用fontsize_row和fontsize_col来分别限定。\n参考来源 https://blog.csdn.net/sinat_38163598/article/details/72770404\nhttps://cran.r-project.org/web/packages/pheatmap/pheatmap.pdf\nhttps://www.jianshu.com/p/6b765e83d723\n","permalink":"https://sr-c.github.io/2018/06/30/pheatmap/","tags":["heatmap"],"title":"使用pheatmap绘制热图"},{"categories":null,"contents":"安装 从ncbi下载已编译好的版本，解压缩后即可使用。\n使用 Building a BLAST database with local sequences 1 2 3 4 5 6 7 8 9 10 11  $ makeblastdb -in mydb.fsa -parse_seqids -dbtype nucl Building a new DB, current time: 01/28/2011 13:39:37 New DB name: mydb.fsa New DB title: mydb.fsa Sequence type: Nucleotide Keep Linkouts: T Keep MBits: T Maximum file size: 1073741824B Adding sequences from FASTA; added 3 sequences in 0.00206995 seconds. $   参数说明：\n1 2 3 4  -in 需要建库的fasta序列 -dbtype 如果是蛋白库则用prot，核酸库用nucl -out 所建数据库的名称 -parse_seqids =\u0026gt; Parse Seq-ids in FASTA input   -parse_seqids 和-out uniprot 一般都加上，使用-parse_seqids需求数据库中每条序列都有唯一的标识，使得blast能够根据序列标识来提取序列。\nUse Windowmasker to filter the query sequence(s) in a BLAST search 1 2 3 4 5 6  1. Run BLAST search using Windowmasker for sequence filtering based upon taxid (9606 is the taxid for human). $ blastn -query input -db database -window_masker_taxid 9606 -out results.txt 2. Run BLAST search using Windowmasker for sequence filtering based upon the windowmasker file name. $ blastn –query input –db database –window_masker_db 9606/wmasker.obinary   Quick start A BLAST search against a database requires at least a –query and –db option. The command:\n1  blastn –db nt –query nt.fsa –out results.out   参数说明\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  -query 输入文件名，也就是需要比对的序列文件 -db 格式化后的数据库名称 -evalue 设定输出结果中的e-value阈值 -out 输出文件名 -num_alignments 输出比对上的序列的最大值条目数 -num_threads 线程数 此外还有： -num_descriptions 对比对上序列的描述信息，一般跟tabular格式连用 -outfmt 0 = pairwise, 1 = query-anchored showing identities, 2 = query-anchored no identities, 3 = flat query-anchored, show identities, 4 = flat query-anchored, no identities, 5 = XML Blast output, 6 = tabular, 7 = tabular with comment lines, 8 = Text ASN.1, 9 = Binary ASN.1 10 = Comma-separated values 格式5，输出的内容最丰富，以标记语言写入，方便根据需要提取信息； 格式6，以表格形式输出，易于阅读。   参考来源 https://www.ncbi.nlm.nih.gov/books/NBK279688/\nhttp://www.bioinfo-scrounger.com/archives/77\n","permalink":"https://sr-c.github.io/2018/06/23/local-blast/","tags":["blast"],"title":"本地blast"},{"categories":null,"contents":"","permalink":"https://sr-c.github.io/2018/06/19/libstdc-so-6-GLIBCXX-not-found/","tags":["debug"],"title":"libstdc++.so.6 GLIBCXX_not_found问题的解决方案"},{"categories":null,"contents":"安装miniconda 在软件主页选择需要安装的版本，在此我们选择Python3.6 64位的版本， 下载后运行安装脚本即可完成安装\n1  sh Miniconda3-latest-Linux-x86_64.sh   配置conda国内源 1 2 3  conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes   配置conda第三方源 Conda Forge 1  conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/   bioconda 1  conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/   安装软件 首次安装软件可能会在配置环境时等待较长时间，请耐心等待。。。\n1  conda install busco   环境配置 1 2 3 4 5 6 7 8 9 10  #新建一个名称为your_env_name的python2.7环境 conda create -n your_env_name python=2.7 #激活虚拟环境 source activate your_env_name #关闭虚拟环境 source deactivate #删除虚拟环境中的某个包 conda remove --name your_env_name package_name #删除虚拟环境 conda remove -n your_env_name --all   参考来源 https://mirror.tuna.tsinghua.edu.cn/help/anaconda/\n","permalink":"https://sr-c.github.io/2018/06/12/conda-install/","tags":["conda"],"title":"使用conda安装软件"},{"categories":null,"contents":"软件安装 BUSCO的手动安装并不困难，但是BUSCO依赖tblastn与augustus，而augustus的安装比较复杂，因此选择使用conda自动化安装应当是首选。conda的安装参照后文。\n1  conda install busco   下载数据库 1 2 3  cd ~/database/busco wget http://busco.ezlab.org/v2/datasets/euarchontoglires_odb9.tar.gz tar -zxvf eukaryota_odb9.tar.gz   软件使用 1  run_BUSCO.py -i yourassemble.fa -o yourassemble_busco -m genome -l ~/database/busco_database/hymenoptera_odb9/ -c 8   参数说明：\n1 2 3 4 5  -i|--in: 输入文件 -o|--out: 输出文件、文件夹前缀 -l|--lineage: 数据库的路径 -m|--mode: 运行模式，geno|tran|prot -c|--cpu: 线程数，默认1.   结果解读 BUSCO 的结果一看就懂，主要看统计文件run_busco/short_summary_busco.txt 。我们的测试数据数据量太少，下面放一个真实项目的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13  $ cat run_busco/short_summary_busco.txt # 省略若干行 # Summarized benchmarking in BUSCO notation for file longest_isoform.fasta # BUSCO was run in mode: tran C:89.0%[S:49.9%,D:39.2%],F:4.4%,M:6.5%,n:429 382 Complete BUSCOs (C) 214 Complete and single-copy BUSCOs (S) 168 Complete and duplicated BUSCOs (D) 19 Fragmented BUSCOs (F) 28 Missing BUSCOs (M) 429 Total BUSCO groups searched   真实项目中，Complete BUSCOs (C) 的比例通畅都能达到 80% 以上。不过如果低于这个值，也未必有问题，还是要根据实际项目情况判断。除了跟经验值比较。更有意义的是使用不同软件、参数多组装几个版本，挑选最优版。\n可视化 如果有多个版本，就可以画这样一张图，分析哪个版本更优。\n1 2 3 4 5 6 7 8  mkdir my_summaries #拷贝多个版本的结果到同一个目录中，注意重命名 cp run_SPEC1/short_summary_SPEC1.txt my_summaries/ cp run_SPEC2/short_summary_SPEC2.txt my_summaries/ cp run_SPEC3/short_summary_SPEC3.txt my_summaries/ cp run_SPEC4/short_summary_SPEC4.txt my_summaries/ cp run_SPEC5/short_summary_SPEC5.txt my_summaries/ python BUSCO_plot.py –wd my_summaries评价质量可以使用BUSCO自带的脚本用R作图，反映short_report中的内容，按照预测出基因的比例进行比较。   参考来源： http://genek.tv/article/29\n","permalink":"https://sr-c.github.io/2018/06/04/BUSCO/","tags":["assembly"],"title":"使用BUSCO评价基因组组装质量"},{"categories":null,"contents":"install 1 2 3  wget https://github.com/mahulchak/quickmerge/archive/v0.2.tar.gz tar zxf v0.2.tar.gz bash make_merger.sh   Running The simplest way to run \u0026lsquo;merger\u0026rsquo; is to use the python wrapper \u0026lsquo;merge_wrapper_v2.py\u0026rsquo;:\n1  merge_wrapper.py hybrid_assembly.fasta self_assembly.fasta   Which of my assemblies should I use as query and reference? There is no golden rule for this. Quickmerge was developed with the idea that every genome is different and that you know best about the genome you are assembling. All you have to do is to keep in mind that quickmerge joins contigs in one assembly (query assembly) using the contigs from the reference assembly. So the merged assembly receives the most sequences from the query assembly, and the reference assembly provides only the sequences that bridge gaps in the query assembly. As a result, the merged assembly size and completeness (as evaluated by BUSCO or CEGMA metric) would be more like the query genome. If you believe that completeness and size of the assembly 1 is truer representation of the actual genome, then you may use that assembly as the query. Please see below for strategies on one and two rounds of quickmerge.\nUSAGE 1: One round of quickmerge Use this when your PacBio assembly (PBcR/canu/Falcon) is much more fragmented than the hybrid/DBG2OLC assembly. Use your DBG2OLC assembly as the reference. This is more likely to happen when coverage is low (\u0026lt;50X). In this case, it is expected that you will run quickmerge only once. E.g.\n Choose your query and reference assembly. You can try one quickmerge run with assembly1 as the reference and another quickmerge run with assembly1 as the query. Choose all the parameters. Run quickmerge. Use the merged assembly with the better N50.  USAGE 2: One round merged assembly does not have both best N50 and correct assembly size This is our preferred method, especially when the PacBio only assembly is much more contiguous than the DBG2OLC or hybrid assembly (more likely to happen when coverage is \u0026gt;50X).\n Choose assembly1 as query and assembly2 as reference. Run quickmerge and obtain N50, assembly size, etc. Set assembly2 as query and assembly1 as query. Run quickmerge and obtain N50, assembly size, etc. Does either of your merged assemblies (from 2 or 4) give you the best N50 and the correct genome size at the same time? If not, choose the merge assembly that is more contiguous and lets call it assembly3. Lets say that size and other metrics (e.g. BUSCO or CEGMA metric) of assembly1 (or assembly2) is more correct. Now run quickmerge with assembly1 (or assembly2) as the query and assembly3 as the reference. If you are not using the wrapper python script, you should rename the contigs in assembly 3 before running quickmerge.Simply dosed -i 's/^\u0026gt;/\u0026gt;N_/g' assembly3.fastabefore running quickmerge. Now your merged assembly has the best N50 without sacrificing on the other metrics.  Reference\nhttps://github.com/mahulchak/quickmerge/wiki\n","permalink":"https://sr-c.github.io/2018/06/04/quickmerge/","tags":["PacBio"],"title":"quickmerge的使用"},{"categories":["Assembly","Genomics"],"contents":"In the README of quickmerge, I got a advice.\n Assembly polishing with Quiver and pilon before and after assembly merging is strongly recommended.\n As for Sequal data, Pacbio updated the quiver to arrow.\n Quiver is supported for PacBio RS data. Arrow is supported for PacBio Sequel data and RS data with the P6-C4 chemistry.\n So, let\u0026rsquo;s do it !\nStep 0 Install SMRT Tools 1 2  SMRT_ROOT=/opt/pacbio/smrtlink ./smrtlink_5.1.0.26412.run --rootdir $SMRT_ROOT --smrttools-only   SMRT tools will be installed in /opt/pacbio/smrtlink/smrtcmds/bin , add this directory into your PATH bash variable.\nStep 0.5 Convert row bax file into scraps.bam (RS data) 1  bax2bam -f $FOFN -o $BAM_DIR\u0026#34;/SMRT-cell-\u0026#34;$COMPT --subread --pulsefeatures=DeletionQV,DeletionTag,InsertionQV,IPD,MergeQV,SubstitutionQV,PulseWidth,SubstitutionTag   Step 0.8 Convert scraps.bam to subreads.bam Step 1 Align subreads.bam 1  pbalign --nproc 16 xxx.bam reference.fasta align_xxx.bam   Step 2 Sort bam files 1 2  ## Sort alignments by leftmost coordinates, or by read name when -n is used. samtools sort [-@ threads] [-m max memory per threads] align_xxx.bam -o align_xxx.sort.bam   Step 3 Merge all the produced bam files 1  samtools merge \u0026lt;align_xxx.merge.sort.bam\u0026gt; \u0026lt;in1.bam\u0026gt; \u0026lt;in2.bam\u0026gt; [...]   Step 4 Index the input files 1 2 3 4 5  ## Index a coordinate-sorted BAM or CRAM file for fast random access samtools index align_xxx.merge.sort.bam pbindex align_xxx.merge.sort.bam ## Index reference sequence in the FASTA format samtools faidx reference.fasta   Step 5 VariantCaller: arrow 1 2 3 4  arrow -j 16 --maskRadius 3 \\ aligned_xxx.merge.sort.bam \\ -r reference.fasta \\ -o variants.gff -o myConsesus.fasta -o myConsesus.fastq    Reference\nhttps://www.biostars.org/p/273447\nhttps://www.pacb.com/wp-content/uploads/SMRT-Tools-Reference-Guide-v4.0.0.pdf\n ","permalink":"https://sr-c.github.io/2018/05/29/polish-pacbio-assembly/","tags":["PacBio"],"title":"Polish Pacbio Assembly"},{"categories":null,"contents":"安装Perlberw 1  \\curl -L https://install.perlbrew.pl | bash   安装完成后即可不需sudo安装模块，尝试不同版本的perl\n使用cpanminus管理模块 使用perlberw安装cpanm\n1  perlbrew install-cpanm   使用cpanm安装模块，不需要管理员权限\n1  $ cpanm Moose   使用 cpanm 安装模块，会将模块安装到目前使用版本 Perl 函数库~/perl5/perlbrew/perls中\n1 2 3 4 5 6 7 8 9 10 11  $ tree ~/perl5/perlbrew/perls/perl-5.14.1/lib | head /Users/c9s/perl5/perlbrew/perls/perl-5.14.1/lib ├── 5.14.1 │ ├── AnyDBM_File.pm │ ├── App │ │ ├── Cpan.pm │ │ ├── Prove │ │ │ ├── State │ │ │ │ ├── Result │ │ │ │ │ └── Test.pm │ │ │ │ └── Result.pm   使用local::lib 安装了perlbrew与cpanm之后，直接使用cpanm安装软件会提示\n1 2 3 4 5 6 7 8 9  ! ! Can\u0026#39;t write to /usr/local/share/perl5 and /usr/local/bin: Installing modules to /home/username/perl5 ! To turn off this warning, you have to do one of the following: ! - run me as a root or with --sudo option (to install to /usr/local/share/perl5 and /usr/local/bin) ! - Configure local::lib in your existing shell to set PERL_MM_OPT etc. ! - Install local::lib by running the following commands ! ! cpanm --local-lib=~/perl5 local::lib \u0026amp;\u0026amp; eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib) !   若忽略此条，则使得已通过cpanm安装的模块不能被正确调用。以App::pmuninstall为例，安装后，pm-unistall不出现在环境变量中。local::lib安装后即可正确调用。\n1  cpanm --local-lib=~/perl5 local::lib \u0026amp;\u0026amp; eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib)    p.s. 在PBS集群中，perl的本地配置不能够直接传递到计算节点，使得PASA运行在需调用perl模块时仍然报错。\n perl模块的卸载 使用模块App::pmuninstall ，可卸载其他模块\n1  cpanm App::pmuninstall   手动确认卸载完成 perl -e 'print join \u0026quot;\\n\u0026quot;,@INC' 会print出所有模块和库的安装目录，到这些目录下找到对应的pm删除掉就可以了。\n参考来源 https://perlbrew.pl/Perlbrew-%E4%B8%AD%E6%96%87%E7%B0%A1%E4%BB%8B.html\nhttps://blog.csdn.net/u011450367/article/details/41522729\nhttps://www.howtoing.com/how-to-install-perlbrew-and-manage-multiple-versions-of-perl-5-on-centos-7\n","permalink":"https://sr-c.github.io/2018/05/27/perl-modules/","tags":["perl"],"title":"配置perl模块"},{"categories":null,"contents":"建立数据库 1  ${RepeatModelerPath}/BuildDatabase -name ${database_name} ${fasta}   但是对于ABBlast来说，BuildDatabase建立的索引还缺少nt_db.xni文件，出现提示\n1 2 3 4  FATAL: xdf_db_fopen failed code 22 (identifier index does not exist) on database \u0026#34;nt_db\u0026#34;: the file \u0026#34;nt_db.xni\u0026#34; was not found. To create a sequence identifier index, execute xdformat with the -I option or re-index the existing database (faster) using the -X option.   需要手动调用ABBlast中的xdformat来建立xni索引\n1  xdformat -n -I nt_db   构建LIBRARY 运行RepeatModeler主程序\n1  ${RepeatModelerPath}/RepeatModeler -pa 30 -database ${database_name}\u0026gt;\u0026amp; run.out \u0026amp;   结果文件夹种的consensi.fa.classified可以作为library用于RepeatMasker进行重复序列的屏蔽。\n重复序列的计算与屏蔽 1 2 3  $RepeatMasker -pa 16 \\  -lib consensi.fa.classified \\  -dir Repeat_result -html -gff species.genome.fasta   注意，RepeatMasker的结果文件夹Repeat_result需要提前手动建立，否则程序运行完成后结果文件会丢失。\n无library直接使用RepeatMasker中的RepBase数据库来计算重复序列，若RepBase数据库对目标物种的覆盖不好，则很可能只找到较少的重复序列。此时，使用RepeatModeler构建library就很有必要。\n参考来源  https://logozy.me/an-zhuang-he-shi-yong-repeatmodler/ http://etutorials.org/Misc/blast/Part+V+BLAST+Reference/Chapter+14.+WU-BLAST+Reference/14.4+xdformat+Parameters/\n ","permalink":"https://sr-c.github.io/2018/05/27/RepeatModeler-engine/","tags":["software"],"title":"RepeatModeler与RepeatMasker 重复序列分析"},{"categories":null,"contents":"近来发现NexT主题升级到了6.2.0，主仓库由iissnan迁移到了theme-next，为了更新页面效果，合并站点与主题的配置文件，于是计划升级一波。\n 升级目标\n  NexT版本由5.1.3升级至6.2.0 Hexo版本由3.4.4升级至3.7.1  升级Hexo 1 2  $ npm -version 5.6.0   由于NexT6.2.0需求Hexo版本至少为3.5.0，因此首先升级Hexo，通过hexo version 查看当前版本（需在Hexo工作目录下进行）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ hexo version hexo: 3.4.4 hexo-cli: 1.1.0 os: Windows_NT 10.0.16299 win32 x64 http_parser: 2.7.0 node: 8.9.4 v8: 6.1.534.50 uv: 1.15.0 zlib: 1.2.11 ares: 1.10.1-DEV modules: 57 nghttp2: 1.25.0 openssl: 1.0.2n icu: 59.1 unicode: 9.0 cldr: 31.0.1 tz: 2017b   然后查看可升级的组件\n1 2 3 4 5 6  $ npm outdated Package Current Wanted Latest Location hexo 3.4.4 3.7.1 3.7.1 hexo-site hexo-generator-archive 0.1.4 0.1.5 0.1.5 hexo-site hexo-renderer-marked 0.3.0 0.3.2 0.3.2 hexo-site hexo-server 0.2.2 0.2.2 0.3.2 hexo-site   以此修改package.json，将hexo和相关待更新组件的版本号更新。修改完成后通过npm更新npm install --save\n1 2 3 4 5 6 7 8 9 10  $ npm install --save npm WARN deprecated titlecase@1.1.2: no longer maintained \u0026gt; nunjucks@3.1.2 postinstall C:\\Users\\Yaman\\sr-c.github.io\\node_modules\\nunjucks \u0026gt; node postinstall-build.js src npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.3 (node_modules\\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.3: wanted {\u0026#34;os\u0026#34;:\u0026#34;darwin\u0026#34;,\u0026#34;arch\u0026#34;:\u0026#34;any\u0026#34;} (current: {\u0026#34;os\u0026#34;:\u0026#34;win32\u0026#34;,\u0026#34;arch\u0026#34;:\u0026#34;x64\u0026#34;}) added 44 packages, removed 78 packages, updated 21 packages and moved 1 package in 12.845s   再次通过hexo version查看版本，更新完成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ hexo version hexo: 3.7.1 hexo-cli: 1.1.0 os: Windows_NT 10.0.16299 win32 x64 http_parser: 2.7.0 node: 8.9.4 v8: 6.1.534.50 uv: 1.15.0 zlib: 1.2.11 ares: 1.10.1-DEV modules: 57 nghttp2: 1.25.0 openssl: 1.0.2n icu: 59.1 unicode: 9.0 cldr: 31.0.1 tz: 2017b   升级NexT 参照官方文档，重新为新版本NexT配置一个目录，相当于重新安装一个新的主题。 同时，参照NexT的建议，将配置文件放置在站点目录的/source/_data/next.yml中。\n 注意——语言配置 NexT新版本中将zh-Hans改为了更为通用的zh-CN，注意在配置文件中更正，否则语言配置无效。\n 参考来源： https://tommy.net.cn/2018/02/26/upgrade-hexo-to-v3-5-0/ http://zerosoul.github.io/2016/06/15/upgrade-hexo-to-3-2/\n","permalink":"https://sr-c.github.io/2018/05/13/update_Hexo_and_NexT/","tags":["Hexo","Next"],"title":"Hexo与NexT升级"},{"categories":null,"contents":"MECAT依赖HDF5和dextract，README中推荐的安装方法，并不十分友好，记录如下。\n安装HDF5 HDF是比较成熟的一种库文件格式，目前流行的版本是HDF5，MECAT的README中为从源码编译安装，多次尝试，编译均告失败。好在发现HDF5官网提供CentOS6的已编译版本，直接解压即可食使用😀。\n安装dextract 从PacificBiosciences的库中git下载源码，dextract官方并未给出安装说明，按照MECAT提供的dextract_makefile进行编译，出现了如下undefined reference错误。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  gcc -O3 -Wall -Wextra -Wno-unused-result -fno-strict-aliasing -I/Storage/data002/shurh/software/hdf5/include -L/Storage/data002/shurh/software/hdf5/lib -o dextract dextract.c DB.c QV.c -lhdf5 -lsz /tmp/ccXFKK1s.o: In function `main\u0026#39;: dextract.c:(.text.startup+0x135): undefined reference to `parse_filter\u0026#39; dextract.c:(.text.startup+0x27c): undefined reference to `initBaxData\u0026#39; dextract.c:(.text.startup+0x33a): undefined reference to `getBaxData\u0026#39; dextract.c:(.text.startup+0x35a): undefined reference to `nextSubread\u0026#39; dextract.c:(.text.startup+0x36b): undefined reference to `nextSubread\u0026#39; dextract.c:(.text.startup+0x38c): undefined reference to `evaluate_bax_filter\u0026#39; dextract.c:(.text.startup+0x538): undefined reference to `sam_open\u0026#39; dextract.c:(.text.startup+0x54e): undefined reference to `sam_header_process\u0026#39; dextract.c:(.text.startup+0x58e): undefined reference to `sam_record_extract\u0026#39; dextract.c:(.text.startup+0x5a1): undefined reference to `SAM_EOF\u0026#39; dextract.c:(.text.startup+0x5b4): undefined reference to `evaluate_bam_filter\u0026#39; dextract.c:(.text.startup+0xc35): undefined reference to `sam_close\u0026#39; dextract.c:(.text.startup+0xfb0): undefined reference to `getBaxData\u0026#39; dextract.c:(.text.startup+0x1096): undefined reference to `sam_open\u0026#39; dextract.c:(.text.startup+0x129f): undefined reference to `parse_filter\u0026#39; dextract.c:(.text.startup+0x1430): undefined reference to `printBaxError\u0026#39; collect2: error: ld returned 1 exit status make: *** [dextract] Error 1   按照其他同志的踩坑，发觉dextract_makefile并不完善， 缺少几个源文件和库文件链接，修改第7行: ${CC} $(CFLAGS) -I$(HDF5_INCLUDE) -L$(HDF5_LIB) -o dextract dextract.c sam.c bax.c expr.c DB.c QV.c -lhdf5 -lz -lsz\n参考来源： https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.15-patch1/bin/linux-centos6-x86_64/hdf5-1.8.15-patch1-linux-centos6-x86_64-shared.tar.gz https://github.com/xiaochuanle/MECAT/issues/19\n","permalink":"https://sr-c.github.io/2018/05/12/MECAT_install/","tags":["debug"],"title":"MECAT安装"},{"categories":null,"contents":"TransDecoder是一款用于预测转录本序列中编码区域的软件。其使用比较简单，下载后其中的perl脚本已预编译完成，可直接使用。 参考官网介绍，分为3步进行。其中第2步可选，将序列比对到uniprot或Pfam数据库，补充最后的筛选过程。 程序的主要限速步骤在于第2步使用blast比对到uniprot，可考虑使用diamond替代blast提升比对速度。\n根据官网的介绍，其算法标准如下:\n 找到转录本序列中最小长度的开放读码框(ORF) 以类似于GeneID的方法对序列进行一次打分，得分大于0的序列被保留 若一条ORF的得分高于另外2种正向的ORF，其上述的得分会最高 若一个候选ORF完全被另一个ORF包括，则较长的ORF最终被采纳。需要注意的是，一条转录本序列，能够产生多个ORF（由于操纵子，嵌合体等因素的存在） 位置得分矩阵(PSSM)被用于优化起始密码子的预测 （可选）预测的氨基酸序列若与Pfam数据库中功能域的匹配高于其他噪音区域  参考链接： TransDecoder: https://github.com/TransDecoder/TransDecoder/wiki PSSM: https://www.ncbi.nlm.nih.gov/Class/Structure/pssm/pssm_viewer.cgi https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-6-33\n","permalink":"https://sr-c.github.io/2018/04/30/TransDecoder-log/","tags":null,"title":"使用TransDecoder预测转录本的编码区域"},{"categories":null,"contents":"想要多设备配置hexo，中间遇到了几个坑，记录下来，也帮助大家。\n背景介绍  hexo会将个人写的文章经过编译在public/.deploy_git文件夹中，然后将编译好的静态站点文件上传到github。因此，github中默认只存在已编译好的静态站点文件，不存在hexo编译环境，使得作者不能够直接在异地发布文章。\n 中心思想  在github库中另建立一个分支用于存储hexo的编译环境。作者在异地先通过git同步hexo编译环境，再通过hexo发布文章即可。\n 具体步骤  前提：笔者已参照官网说明在github上配置好了个人站点。（设置好了主题，写了几篇文章）\n 1. 在github上新建分支hexo用于存储hexo的配置环境，并将其设置为默认分支。 2. 在本地博客根目录下使用git命令将hexo配置环境上传到github上新建的hexo分支 1 2 3 4 5 6  git init // git初始化 git remote add origin https://github.com/用户名/仓库名.git // 添加仓库地址 git checkout -b hexo // 新建分支hexo并切换到新建的分支hexo git add . // 添加所有本地文件到git git commit -m \u0026#34;更新说明\u0026#34; // git提交 git push origin hexo // 文件推送到hexo分支   3. 在另一设备上使用git命令下载hexo分支中的文件 1 2  // 克隆文件到本地 git clone -b 分支名 https://github.com/用户名/仓库名.git   4. 在另一设备上（已安装好git与hexo）即可开始对站点的管理 1 2 3  cd \u0026lt;folder\u0026gt; npm install hexo s   注意事项   在不同设备上配置github注意要先配置好ssh密钥 themes文件夹中的主题如果是直接git clone下来的，在上传前先删除相应主题目录下的.git文件夹，但仍不能取得相应主题文件夹的权限。解决方案是将相应文件夹剪切到别处，执行一遍add commit push(在github端删除了该文件夹)，再在本地粘帖回来，执行add commit push三连，重新上传，即可。 新配置的设备上，安装好sshkey后仍然提示Host key verification failed. fatal: Could not read from remote repository.，则需要手动连接一次ssh git@github.com 即可。   参考来源 https://www.zhihu.com/question/21193762 http://blog.csdn.net/Monkey_LZL/article/details/60870891 http://blog.csdn.net/sinat_27088253/article/details/54314742\n","permalink":"https://sr-c.github.io/2018/01/08/hexo-remote-configuration/","tags":["Hexo","github"],"title":"hexo的多端配置"},{"categories":null,"contents":"BLAST+ 2.7.1 在运行的时候出现如下类似问题：\n1  $ Critical: (110.6) CNcbiRegistry: Syntax error in system-wide configuration file: NCBI C++ Exception: \u0026#34;..........\\src\\corelib\\ncbireg.cpp\u0026#34;, line 660: Error: ncbi::IRWRegistry ::x_Read() - Badly placed \u0026#39;\\\u0026#39; in the registry value: \u0026#39;ROOT=J:\\nASNLOAD=J:\\BioEdi t\\tables\\nDATA=J:\\BioEdit\\tables\\\u0026#39; (m_Pos = 4)   这是由于ncbi.ini文件被bioedit软件损坏所造成的。在C盘windows目录下，找到NCBI.ini，删除。\nfind ncbi.ini file (C:\\windows) and delete it.\n再次运行BLAST+，完美解决。\nrun BLAST+ and the error resolved.\n参考来源：http://blog.sina.com.cn/s/blog_14552795a0102whc1.html\n","permalink":"https://sr-c.github.io/2018/01/04/blast-error-ncbireg/","tags":["debug"],"title":"blast_error_ncbireg"},{"categories":null,"contents":"","permalink":"https://sr-c.github.io/search/","tags":null,"title":"Search"}]